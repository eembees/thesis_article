
@article{melis_mogrifier_2020,
	title = {Mogrifier {LSTM}},
	url = {http://arxiv.org/abs/1909.01792},
	abstract = {Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.},
	urldate = {2022-01-13},
	journal = {arXiv:1909.01792 [cs]},
	author = {Melis, Gábor and Kočiský, Tomáš and Blunsom, Phil},
	month = jan,
	year = {2020},
	note = {arXiv: 1909.01792
version: 2},
	keywords = {Computer Science - Computation and Language},
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2022-01-13},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.01271
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@incollection{lecun_convolutional_1998,
	address = {Cambridge, MA, USA},
	title = {Convolutional networks for images, speech, and time series},
	isbn = {978-0-262-51102-5},
	urldate = {2022-01-12},
	booktitle = {The handbook of brain theory and neural networks},
	publisher = {MIT Press},
	author = {LeCun, Yann and Bengio, Yoshua},
	month = oct,
	year = {1998},
	pages = {255--258},
}

@article{townsend_practical_2019,
	title = {Practical {Lossless} {Compression} with {Latent} {Variables} using {Bits} {Back} {Coding}},
	url = {http://arxiv.org/abs/1901.04866},
	abstract = {Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present `Bits Back with ANS' (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational auto-encoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back .},
	urldate = {2022-01-12},
	journal = {arXiv:1901.04866 [cs, math, stat]},
	author = {Townsend, James and Bird, Tom and Barber, David},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.04866},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@incollection{williams_gradient-based_1995,
	address = {Hillsdale, NJ, US},
	series = {Developments in connectionist theory},
	title = {Gradient-based learning algorithms for recurrent networks and their computational complexity},
	isbn = {978-0-8058-1258-9 978-0-8058-1259-6},
	abstract = {describe several gradient-based approaches to training a recurrent network to perform a desired sequential behavior in response to input / in characterizing these approaches as "gradient-based" we mean that at least part of the learning algorithm involves computing the gradient of some form of performance measure for the network in weight space, either exactly or approximately, with this result then used in some appropriate fashion to determine the weight changes / for the type of task investigated here, the performance measure is a simple measure of error between actual and desired output / primary focus will be on techniques for computing this exact or approximate gradient information  discuss several approaches to performing the desired gradient computation, some based on the familiar back propagation algorithm and some involving other ideas / discuss the relationship betweeen these various alternative approaches / provide an analysis of their computational requirements / note some special architectures which readily lend themselves to specific hybrid strategies giving rise to conceptually and/or computationally simpler algorithms for exact gradient computation / additional topics discussed are teacher forcing, a useful adjunct to all of the techniques discussed, and some experimental comparisons of the performance of some of the algorithms (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	booktitle = {Backpropagation:  {Theory}, architectures, and applications},
	publisher = {Lawrence Erlbaum Associates, Inc},
	author = {Williams, Ronald J. and Zipser, David},
	year = {1995},
	keywords = {Algorithms, Errors, Learning, Measurement, Neural Networks, Performance},
	pages = {433--486},
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2022-01-12},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{shin_protein_2021,
	title = {Protein design and variant prediction using autoregressive generative models},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-22732-w},
	doi = {10.1038/s41467-021-22732-w},
	abstract = {The ability to design functional sequences and predict effects of variation is central to protein engineering and biotherapeutics. State-of-art computational methods rely on models that leverage evolutionary information but are inadequate for important applications where multiple sequence alignments are not robust. Such applications include the prediction of variant effects of indels, disordered proteins, and the design of proteins such as antibodies due to the highly variable complementarity determining regions. We introduce a deep generative model adapted from natural language processing for prediction and design of diverse functional sequences without the need for alignments. The model performs state-of-art prediction of missense and indel effects and we successfully design and test a diverse 105-nanobody library that shows better expression than a 1000-fold larger synthetic library. Our results demonstrate the power of the alignment-free autoregressive model in generalizing to regions of sequence space traditionally considered beyond the reach of prediction and design.},
	language = {en},
	number = {1},
	urldate = {2022-01-11},
	journal = {Nature Communications},
	author = {Shin, Jung-Eun and Riesselman, Adam J. and Kollasch, Aaron W. and McMahon, Conor and Simon, Elana and Sander, Chris and Manglik, Aashish and Kruse, Andrew C. and Marks, Debora S.},
	month = apr,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational models;Machine learning;Protein design;Protein engineering;Protein function predictions
Subject\_term\_id: computational-models;machine-learning;protein-design;protein-engineering;protein-function-predictions},
	keywords = {Computational models, Machine learning, Protein design, Protein engineering, Protein function predictions},
	pages = {2403},
}

@misc{noauthor_wavenet_nodate,
	title = {{WaveNet} {ASR} {Librilight} [10h, 1h, 10m] [{WaveNet} trained on {Libri} 100h]] {\textbar} wavenet\_asr {Table} – {Weights} \& {Biases}},
	url = {https://wandb.ai/vseq/wavenet_asr/sweeps/h75kwccp/table?workspace=user-eembees},
	urldate = {2021-12-23},
}

@article{frazer_disease_2021,
	title = {Disease variant prediction with deep generative models of evolutionary data},
	volume = {599},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04043-8},
	doi = {10.1038/s41586-021-04043-8},
	abstract = {Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98\%) of these variants still have unknown consequences1–3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4–10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification12–16. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.},
	language = {en},
	number = {7883},
	urldate = {2021-12-19},
	journal = {Nature},
	author = {Frazer, Jonathan and Notin, Pascal and Dias, Mafalda and Gomez, Aidan and Min, Joseph K. and Brock, Kelly and Gal, Yarin and Marks, Debora S.},
	month = nov,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7883
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational models;Disease genetics;Genetic variation;Genetics research;Machine learning
Subject\_term\_id: computational-models;disease-genetics;genetic-variation;genetics-research;machine-learning},
	keywords = {Computational models, Disease genetics, Genetic variation, Genetics research, Machine learning},
	pages = {91--95},
}

@article{shin_protein_2021-1,
	title = {Protein design and variant prediction using autoregressive generative models},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-22732-w},
	doi = {10.1038/s41467-021-22732-w},
	abstract = {The ability to design functional sequences and predict effects of variation is central to protein engineering and biotherapeutics. State-of-art computational methods rely on models that leverage evolutionary information but are inadequate for important applications where multiple sequence alignments are not robust. Such applications include the prediction of variant effects of indels, disordered proteins, and the design of proteins such as antibodies due to the highly variable complementarity determining regions. We introduce a deep generative model adapted from natural language processing for prediction and design of diverse functional sequences without the need for alignments. The model performs state-of-art prediction of missense and indel effects and we successfully design and test a diverse 105-nanobody library that shows better expression than a 1000-fold larger synthetic library. Our results demonstrate the power of the alignment-free autoregressive model in generalizing to regions of sequence space traditionally considered beyond the reach of prediction and design.},
	language = {en},
	number = {1},
	urldate = {2021-12-19},
	journal = {Nature Communications},
	author = {Shin, Jung-Eun and Riesselman, Adam J. and Kollasch, Aaron W. and McMahon, Conor and Simon, Elana and Sander, Chris and Manglik, Aashish and Kruse, Andrew C. and Marks, Debora S.},
	month = apr,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational models;Machine learning;Protein design;Protein engineering;Protein function predictions
Subject\_term\_id: computational-models;machine-learning;protein-design;protein-engineering;protein-function-predictions},
	keywords = {Computational models, Machine learning, Protein design, Protein engineering, Protein function predictions},
	pages = {2403},
}

@article{riesselman_deep_2017,
	title = {Deep generative models of genetic variation capture mutation effects},
	url = {http://arxiv.org/abs/1712.06527},
	abstract = {The functions of proteins and RNAs are determined by a myriad of interactions between their constituent residues, but most quantitative models of how molecular phenotype depends on genotype must approximate this by simple additive effects. While recent models have relaxed this constraint to also account for pairwise interactions, these approaches do not provide a tractable path towards modeling higher-order dependencies. Here, we show how latent variable models with nonlinear dependencies can be applied to capture beyond-pairwise constraints in biomolecules. We present a new probabilistic model for sequence families, DeepSequence, that can predict the effects of mutations across a variety of deep mutational scanning experiments significantly better than site independent or pairwise models that are based on the same evolutionary data. The model, learned in an unsupervised manner solely from sequence information, is grounded with biologically motivated priors, reveals latent organization of sequence families, and can be used to extrapolate to new parts of sequence space},
	urldate = {2021-12-19},
	journal = {arXiv:1712.06527 [cond-mat, physics:physics, q-bio, stat]},
	author = {Riesselman, Adam J. and Ingraham, John B. and Marks, Debora S.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06527},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Physics - Biological Physics, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}

@article{mcgee_generative_2021,
	title = {Generative {Capacity} of {Probabilistic} {Protein} {Sequence} {Models}},
	url = {http://arxiv.org/abs/2012.02296},
	abstract = {Potts models and variational autoencoders (VAEs) have recently gained popularity as generative protein sequence models (GPSMs) to explore fitness landscapes and predict the effect of mutations. Despite encouraging results, quantitative characterization and comparison of GPSM-generated probability distributions is still lacking. It is currently unclear whether GPSMs can faithfully reproduce the complex multi-residue mutation patterns observed in natural sequences arising due to epistasis. We develop a set of sequence statistics to assess the "generative capacity" of three GPSMs of recent interest: the pairwise Potts Hamiltonian, the VAE, and the site-independent model, using natural and synthetic datasets. We show that the generative capacity of the Potts Hamiltonian model is the largest, in that the higher order mutational statistics generated by the model agree with those observed for natural sequences. In contrast, we show that the VAE's generative capacity lies between the pairwise Potts and site-independent models. Importantly, our work measures GPSM generative capacity in terms of higher-order sequence covariation statistics which we have developed, and provides a new framework for evaluating and interpreting GPSM accuracy that emphasizes the role of epistasis.},
	urldate = {2021-12-17},
	journal = {arXiv:2012.02296 [physics, q-bio]},
	author = {McGee, Francisco and Novinger, Quentin and Levy, Ronald M. and Carnevale, Vincenzo and Haldane, Allan},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.02296},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Quantitative Methods},
}

@article{hawkins-hooker_generating_2021,
	title = {Generating functional protein variants with variational autoencoders},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008736},
	doi = {10.1371/journal.pcbi.1008736},
	abstract = {The vast expansion of protein sequence databases provides an opportunity for new protein design approaches which seek to learn the sequence-function relationship directly from natural sequence variation. Deep generative models trained on protein sequence data have been shown to learn biologically meaningful representations helpful for a variety of downstream tasks, but their potential for direct use in the design of novel proteins remains largely unexplored. Here we show that variational autoencoders trained on a dataset of almost 70000 luciferase-like oxidoreductases can be used to generate novel, functional variants of the luxA bacterial luciferase. We propose separate VAE models to work with aligned sequence input (MSA VAE) and raw sequence input (AR-VAE), and offer evidence that while both are able to reproduce patterns of amino acid usage characteristic of the family, the MSA VAE is better able to capture long-distance dependencies reflecting the influence of 3D structure. To confirm the practical utility of the models, we used them to generate variants of luxA whose luminescence activity was validated experimentally. We further showed that conditional variants of both models could be used to increase the solubility of luxA without disrupting function. Altogether 6/12 of the variants generated using the unconditional AR-VAE and 9/11 generated using the unconditional MSA VAE retained measurable luminescence, together with all 23 of the less distant variants generated by conditional versions of the models; the most distant functional variant contained 35 differences relative to the nearest training set sequence. These results demonstrate the feasibility of using deep generative models to explore the space of possible protein sequences and generate useful variants, providing a method complementary to rational design and directed evolution approaches.},
	language = {en},
	number = {2},
	urldate = {2021-12-17},
	journal = {PLOS Computational Biology},
	author = {Hawkins-Hooker, Alex and Depardieu, Florence and Baur, Sebastien and Couairon, Guillaume and Chen, Arthur and Bikard, David},
	month = feb,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Hidden Markov models, Luciferase, Luminescence, Multiple alignment calculation, Protein domains, Protein sequencing, Sequence alignment, Solubility},
	pages = {e1008736},
}

@article{sinai_variational_2018,
	title = {Variational auto-encoding of protein sequences},
	url = {http://arxiv.org/abs/1712.03346},
	abstract = {Proteins are responsible for the most diverse set of functions in biology. The ability to extract information from protein sequences and to predict the effects of mutations is extremely valuable in many domains of biology and medicine. However the mapping between protein sequence and function is complex and poorly understood. Here we present an embedding of natural protein sequences using a Variational Auto-Encoder and use it to predict how mutations affect protein function. We use this unsupervised approach to cluster natural variants and learn interactions between sets of positions within a protein. This approach generally performs better than baseline methods that consider no interactions within sequences, and in some cases better than the state-of-the-art approaches that use the inverse-Potts model. This generative model can be used to computationally guide exploration of protein sequence space and to better inform rational and automatic protein design.},
	urldate = {2021-12-13},
	journal = {arXiv:1712.03346 [cs, q-bio]},
	author = {Sinai, Sam and Kelsic, Eric and Church, George M. and Nowak, Martin A.},
	month = jan,
	year = {2018},
	note = {arXiv: 1712.03346},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
}

@article{mcgee_generative_2021-1,
	title = {Generative {Capacity} of {Probabilistic} {Protein} {Sequence} {Models}},
	url = {http://arxiv.org/abs/2012.02296},
	abstract = {Potts models and variational autoencoders (VAEs) have recently gained popularity as generative protein sequence models (GPSMs) to explore fitness landscapes and predict the effect of mutations. Despite encouraging results, quantitative characterization and comparison of GPSM-generated probability distributions is still lacking. It is currently unclear whether GPSMs can faithfully reproduce the complex multi-residue mutation patterns observed in natural sequences arising due to epistasis. We develop a set of sequence statistics to assess the "generative capacity" of three GPSMs of recent interest: the pairwise Potts Hamiltonian, the VAE, and the site-independent model, using natural and synthetic datasets. We show that the generative capacity of the Potts Hamiltonian model is the largest, in that the higher order mutational statistics generated by the model agree with those observed for natural sequences. In contrast, we show that the VAE's generative capacity lies between the pairwise Potts and site-independent models. Importantly, our work measures GPSM generative capacity in terms of higher-order sequence covariation statistics which we have developed, and provides a new framework for evaluating and interpreting GPSM accuracy that emphasizes the role of epistasis.},
	urldate = {2021-12-13},
	journal = {arXiv:2012.02296 [physics, q-bio]},
	author = {McGee, Francisco and Novinger, Quentin and Levy, Ronald M. and Carnevale, Vincenzo and Haldane, Allan},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.02296},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Quantitative Methods},
}

@article{riesselman_deep_2018,
	title = {Deep generative models of genetic variation capture the effects of mutations},
	volume = {15},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0138-4},
	doi = {10.1038/s41592-018-0138-4},
	abstract = {The functions of proteins and RNAs are defined by the collective interactions of many residues, and yet most statistical models of biological sequences consider sites nearly independently. Recent approaches have demonstrated benefits of including interactions to capture pairwise covariation, but leave higher-order dependencies out of reach. Here we show how it is possible to capture higher-order, context-dependent constraints in biological sequences via latent variable models with nonlinear dependencies. We found that DeepSequence (https://github.com/debbiemarkslab/DeepSequence), a probabilistic model for sequence families, predicted the effects of mutations across a variety of deep mutational scanning experiments substantially better than existing methods based on the same evolutionary data. The model, learned in an unsupervised manner solely on the basis of sequence information, is grounded with biologically motivated priors, reveals the latent organization of sequence families, and can be used to explore new parts of sequence space.},
	language = {en},
	number = {10},
	urldate = {2021-12-13},
	journal = {Nature Methods},
	author = {Riesselman, Adam J. and Ingraham, John B. and Marks, Debora S.},
	month = oct,
	year = {2018},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 10
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational models;Machine learning
Subject\_term\_id: computational-models;machine-learning},
	keywords = {Computational models, Machine learning},
	pages = {816--822},
}

@article{trinquier_efficient_2021,
	title = {Efficient generative modeling of protein sequences using simple autoregressive models},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25756-4},
	doi = {10.1038/s41467-021-25756-4},
	abstract = {Generative models emerge as promising candidates for novel sequence-data driven approaches to protein design, and for the extraction of structural and functional information about proteins deeply hidden in rapidly growing sequence databases. Here we propose simple autoregressive models as highly accurate but computationally efficient generative sequence models. We show that they perform similarly to existing approaches based on Boltzmann machines or deep generative models, but at a substantially lower computational cost (by a factor between 102 and 103). Furthermore, the simple structure of our models has distinctive mathematical advantages, which translate into an improved applicability in sequence generation and evaluation. Within these models, we can easily estimate both the probability of a given sequence, and, using the model’s entropy, the size of the functional sequence space related to a specific protein family. In the example of response regulators, we find a huge number of ca. 1068 possible sequences, which nevertheless constitute only the astronomically small fraction 10−80 of all amino-acid sequences of the same length. These findings illustrate the potential and the difficulty in exploring sequence space via generative sequence models.},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {Nature Communications},
	author = {Trinquier, Jeanne and Uguzzoni, Guido and Pagnani, Andrea and Zamponi, Francesco and Weigt, Martin},
	month = oct,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Biological physics;Machine learning;Molecular evolution;Protein design;Statistical methods
Subject\_term\_id: biological-physics;machine-learning;molecular-evolution;protein-design;statistical-methods},
	keywords = {Biological physics, Machine learning, Molecular evolution, Protein design, Statistical methods},
	pages = {5800},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	urldate = {2021-12-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@inproceedings{pham_convolutional_2016,
	address = {Austin, Texas},
	title = {Convolutional {Neural} {Network} {Language} {Models}},
	url = {https://aclanthology.org/D16-1123},
	doi = {10.18653/v1/D16-1123},
	urldate = {2021-12-08},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pham, Ngoc-Quan and Kruszewski, German and Boleda, Gemma},
	month = nov,
	year = {2016},
	pages = {1153--1162},
}

@article{trinquier_efficient_2021-1,
	title = {Efficient generative modeling of protein sequences using simple autoregressive models},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25756-4},
	doi = {10.1038/s41467-021-25756-4},
	abstract = {Abstract
            
              Generative models emerge as promising candidates for novel sequence-data driven approaches to protein design, and for the extraction of structural and functional information about proteins deeply hidden in rapidly growing sequence databases. Here we propose simple autoregressive models as highly accurate but computationally efficient generative sequence models. We show that they perform similarly to existing approaches based on Boltzmann machines or deep generative models, but at a substantially lower computational cost (by a factor between 10
              2
              and 10
              3
              ). Furthermore, the simple structure of our models has distinctive mathematical advantages, which translate into an improved applicability in sequence generation and evaluation. Within these models, we can easily estimate both the probability of a given sequence, and, using the model’s entropy, the size of the functional sequence space related to a specific protein family. In the example of response regulators, we find a huge number of ca. 10
              68
              possible sequences, which nevertheless constitute only the astronomically small fraction 10
              −80
              of all amino-acid sequences of the same length. These findings illustrate the potential and the difficulty in exploring sequence space via generative sequence models.},
	language = {en},
	number = {1},
	urldate = {2021-12-08},
	journal = {Nature Communications},
	author = {Trinquier, Jeanne and Uguzzoni, Guido and Pagnani, Andrea and Zamponi, Francesco and Weigt, Martin},
	month = dec,
	year = {2021},
	pages = {5800},
}

@article{wu_protein_2021,
	series = {Mechanistic {Biology} * {Machine} {Learning} in {Chemical} {Biology}},
	title = {Protein sequence design with deep generative models},
	volume = {65},
	issn = {1367-5931},
	url = {https://www.sciencedirect.com/science/article/pii/S136759312100051X},
	doi = {10.1016/j.cbpa.2021.04.004},
	abstract = {Protein engineering seeks to identify protein sequences with optimized properties. When guided by machine learning, protein sequence generation methods can draw on prior knowledge and experimental efforts to improve this process. In this review, we highlight recent applications of machine learning to generate protein sequences, focusing on the emerging field of deep generative methods.},
	language = {en},
	urldate = {2021-12-08},
	journal = {Current Opinion in Chemical Biology},
	author = {Wu, Zachary and Johnston, Kadina E. and Arnold, Frances H. and Yang, Kevin K.},
	month = dec,
	year = {2021},
	keywords = {Deep learning, Generative models, Protein engineering},
	pages = {18--27},
}

@article{wu_protein_2021-1,
	series = {Mechanistic {Biology} * {Machine} {Learning} in {Chemical} {Biology}},
	title = {Protein sequence design with deep generative models},
	volume = {65},
	issn = {1367-5931},
	url = {https://www.sciencedirect.com/science/article/pii/S136759312100051X},
	doi = {10.1016/j.cbpa.2021.04.004},
	abstract = {Protein engineering seeks to identify protein sequences with optimized properties. When guided by machine learning, protein sequence generation methods can draw on prior knowledge and experimental efforts to improve this process. In this review, we highlight recent applications of machine learning to generate protein sequences, focusing on the emerging field of deep generative methods.},
	language = {en},
	urldate = {2021-12-08},
	journal = {Current Opinion in Chemical Biology},
	author = {Wu, Zachary and Johnston, Kadina E. and Arnold, Frances H. and Yang, Kevin K.},
	month = dec,
	year = {2021},
	keywords = {Deep learning, Generative models, Protein engineering},
	pages = {18--27},
}

@article{trinquier_efficient_2021-2,
	title = {Efficient generative modeling of protein sequences using simple autoregressive models},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25756-4},
	doi = {10.1038/s41467-021-25756-4},
	abstract = {Generative models emerge as promising candidates for novel sequence-data driven approaches to protein design, and for the extraction of structural and functional information about proteins deeply hidden in rapidly growing sequence databases. Here we propose simple autoregressive models as highly accurate but computationally efficient generative sequence models. We show that they perform similarly to existing approaches based on Boltzmann machines or deep generative models, but at a substantially lower computational cost (by a factor between 102 and 103). Furthermore, the simple structure of our models has distinctive mathematical advantages, which translate into an improved applicability in sequence generation and evaluation. Within these models, we can easily estimate both the probability of a given sequence, and, using the model’s entropy, the size of the functional sequence space related to a specific protein family. In the example of response regulators, we find a huge number of ca. 1068 possible sequences, which nevertheless constitute only the astronomically small fraction 10−80 of all amino-acid sequences of the same length. These findings illustrate the potential and the difficulty in exploring sequence space via generative sequence models.},
	language = {en},
	number = {1},
	urldate = {2021-12-08},
	journal = {Nature Communications},
	author = {Trinquier, Jeanne and Uguzzoni, Guido and Pagnani, Andrea and Zamponi, Francesco and Weigt, Martin},
	month = oct,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Biological physics;Machine learning;Molecular evolution;Protein design;Statistical methods
Subject\_term\_id: biological-physics;machine-learning;molecular-evolution;protein-design;statistical-methods},
	keywords = {Biological physics, Machine learning, Molecular evolution, Protein design, Statistical methods},
	pages = {5800},
}

@misc{thomsen_deepfret_2020,
	title = {{DeepFRET}, a software for rapid and automated single-molecule {FRET} data classification using deep learning {\textbar} {eLife}},
	url = {https://elifesciences.org/articles/60404},
	urldate = {2021-12-08},
	author = {Thomsen, Johannes},
	year = {2020},
}

@inproceedings{dutilleux_implementation_1990,
	address = {Berlin, Heidelberg},
	series = {inverse problems and theoretical imaging},
	title = {An {Implementation} of the “algorithme à trous” to {Compute} the {Wavelet} {Transform}},
	isbn = {978-3-642-75988-8},
	doi = {10.1007/978-3-642-75988-8_29},
	abstract = {The computation of the wavelet transform involves the computation of the convolution product of the signal to be analysed by the analysing wavelet. It will be shown that the computation load grows with the scale factor of the analysis. We are interested in musical sounds lasting a few seconds. Using a straightforward algorithm leads to a prohibitive computation time, so we need a more effective computation procedure.},
	language = {en},
	booktitle = {Wavelets},
	publisher = {Springer},
	author = {Dutilleux, P.},
	editor = {Combes, Jean-Michel and Grossmann, Alexander and Tchamitchian, Philippe},
	year = {1990},
	keywords = {Analyse Wavelet, Digital Signal Processor, Finite Impulse Response, Finite Impulse Response Filter, Side Lobe},
	pages = {298--304},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2021-02-26},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@article{astolfi_duration_2015,
	title = {Duration of voicing and silence periods of continuous speech in different acoustic environments},
	volume = {137},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/1.4906259},
	doi = {10.1121/1.4906259},
	number = {2},
	urldate = {2021-12-07},
	journal = {The Journal of the Acoustical Society of America},
	author = {Astolfi, Arianna and Carullo, Alessio and Pavese, Lorenzo and Puglisi, Giuseppina Emma},
	month = feb,
	year = {2015},
	note = {Publisher: Acoustical Society of America},
	pages = {565--579},
}

@article{atal_pattern_1976,
	title = {A pattern recognition approach to voiced-unvoiced-silence classification with applications to speech recognition},
	volume = {24},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1976.1162800},
	abstract = {In speech analysis, the voiced-unvoiced decision is usually performed in conjunction with pitch analysis. The linking of voiced-unvoiced (V-UV) decision to pitch analysis not only results in unnecessary complexity, but makes it difficult to classify short speech segments which are less than a few pitch periods in duration. In this paper, we describe a pattern recognition approach for deciding whether a given segment of a speech signal should be classified as voiced speech, unvoiced speech, or silence, based on measurements made on the signal. In this method, five different measurements are made on the speech segment to be classified. The measured parameters are the zero-crossing rate, the speech energy, the correlation between adjacent speech samples, the first predictor coefficient from a 12-pole linear predictive coding (LPC) analysis, and the energy in the prediction error. The speech segment is assigned to a particular class based on a minimum-distance rule obtained under the assumption that the measured parameters are distributed according to the multidimensional Gaussian probability density function. The means and covariances for the Gaussian distribution are determined from manually classified speech data included in a training set. The method has been found to provide reliable classification with speech segments as short as 10 ms and has been used for both speech analysis-synthesis and recognition applications. A simple nonlinear smoothing algorithm is described to provide a smooth 3-level contour of an utterance for use in speech recognition applications. Quantitative results and several examples illustrating the performance of the method are included in the paper.},
	number = {3},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Atal, B. and Rabiner, L.},
	month = jun,
	year = {1976},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Density measurement, Energy measurement, Joining processes, Linear predictive coding, Particle measurements, Pattern recognition, Performance analysis, Speech analysis, Speech coding, Speech recognition},
	pages = {201--212},
}

@inproceedings{kahn_libri-light_2020,
	title = {Libri-{Light}: {A} {Benchmark} for {ASR} with {Limited} or {No} {Supervision}},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Kahn, J. and Rivière, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazaré, P. E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and Likhomanenko, T. and Synnaeve, G. and Joulin, A. and Mohamed, A. and Dupoux, E.},
	year = {2020},
	pages = {7669--7673},
}

@misc{noauthor_-law_2008,
	type = {web page},
	title = {µ-{Law} {Compressed} {Sound} {Format}},
	copyright = {Text is U.S. government work},
	url = {https://www.loc.gov/preservation/digital/formats/fdd/fdd000039.shtml},
	abstract = {Format Description for µ-Law -- Standard companding algorithm used in digital communications systems in North America and Japan (telephones, for the most part) to optimize the dynamic range of an analog signal (generally a voice) for digitizing, i.e., to compress 16 bit LPCM (Linear Pulse Code Modulated) data down to 8 bits of logarithmic data.},
	language = {eng},
	urldate = {2021-12-07},
	month = sep,
	year = {2008},
}

@misc{garofolo_john_s_timit_1993,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}ul{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}/ul{\textgreater}{\textless}/br{\textgreater} 
Portions © 1993 Trustees of the University of Pennsylvania},
	urldate = {2021-12-07},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	year = {1993},
	doi = {10.35111/17GK-BN40},
	note = {Artwork Size: 715776 KB
Pages: 715776 KB
Type: dataset},
}

@article{bond-taylor_deep_2021,
	title = {Deep {Generative} {Modelling}: {A} {Comparative} {Review} of {VAEs}, {GANs}, {Normalizing} {Flows}, {Energy}-{Based} and {Autoregressive} {Models}},
	shorttitle = {Deep {Generative} {Modelling}},
	url = {http://arxiv.org/abs/2103.04922},
	abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
	urldate = {2021-12-06},
	journal = {arXiv:2103.04922 [cs, stat]},
	author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.04922},
	keywords = {68T01 (Primary), 68T07 (Secondary), Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, G.3, I.4.0, I.5.0, Statistics - Machine Learning},
}

@article{dieleman_variable-rate_2021,
	title = {Variable-rate discrete representation learning},
	url = {http://arxiv.org/abs/2103.06089},
	abstract = {Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations.},
	urldate = {2021-12-06},
	journal = {arXiv:2103.06089 [cs, eess]},
	author = {Dieleman, Sander and Nash, Charlie and Engel, Jesse and Simonyan, Karen},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.06089},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{dauphin_language_2017,
	title = {Language {Modeling} with {Gated} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1612.08083},
	abstract = {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.},
	urldate = {2021-12-05},
	journal = {arXiv:1612.08083 [cs]},
	author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
	month = sep,
	year = {2017},
	note = {arXiv: 1612.08083},
	keywords = {Computer Science - Computation and Language},
}

@article{miller_stable_2019,
	title = {Stable {Recurrent} {Models}},
	url = {http://arxiv.org/abs/1805.10369},
	abstract = {Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.},
	urldate = {2021-12-05},
	journal = {arXiv:1805.10369 [cs, stat]},
	author = {Miller, John and Hardt, Moritz},
	month = mar,
	year = {2019},
	note = {arXiv: 1805.10369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{oord_conditional_2016,
	title = {Conditional {Image} {Generation} with {PixelCNN} {Decoders}},
	url = {http://arxiv.org/abs/1606.05328},
	abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
	urldate = {2021-12-05},
	journal = {arXiv:1606.05328 [cs]},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05328},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{khurana_convolutional_2020,
	title = {A {Convolutional} {Deep} {Markov} {Model} for {Unsupervised} {Speech} {Representation} {Learning}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/khurana20_interspeech.html},
	doi = {10.21437/Interspeech.2020-3084},
	abstract = {Probabilistic Latent Variable Models (LVMs) provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders (VAEs), their use for speech representation learning remains largely unexplored. In this work, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset (LibriSpeech), ConvDMM produces features that signiﬁcantly outperform multiple self-supervised feature extracting methods on linear phone classiﬁcation and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we ﬁnd that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labelled training examples.},
	language = {en},
	urldate = {2021-12-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Khurana, Sameer and Laurent, Antoine and Hsu, Wei-Ning and Chorowski, Jan and Lancucki, Adrian and Marxer, Ricard and Glass, James},
	month = oct,
	year = {2020},
	pages = {3790--3794},
}

@article{bengio_neural_2003,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {https://www.jmlr.org/papers/v3/bengio03a},
	abstract = {A goal of statistical language modeling is to learn the joint
probability function of sequences of words in a language. This is
intrinsically difficult because of the curse of dimensionality:
a word sequence on which the model will be tested is likely to be
different from all the word sequences seen during training.
Traditional but very successful approaches based on n-grams obtain
generalization by concatenating very short overlapping
sequences seen in the training
set.  We propose to fight the curse of dimensionality by
learning a distributed representation for words which allows each
training sentence to inform the model about an exponential number of
semantically neighboring sentences.  The model learns simultaneously
(1) a distributed representation for each word along with (2) the
probability function for word sequences, expressed in terms of these
representations. Generalization is obtained because a sequence of
words that has never been seen before gets high probability if it is
made of words that are similar (in the sense of having a nearby
representation) to words forming an already seen sentence. Training
such large models (with millions of parameters) within a reasonable
time is itself a significant challenge.  We report on experiments
using neural networks for the probability function, showing on two
text corpora that the proposed approach significantly improves on
state-of-the-art n-gram models, and that the proposed approach
allows to take advantage of longer contexts.},
	number = {Feb},
	urldate = {2021-12-05},
	journal = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	pages = {1137--1155},
}

@article{kalchbrenner_neural_2017,
	title = {Neural {Machine} {Translation} in {Linear} {Time}},
	url = {http://arxiv.org/abs/1610.10099},
	abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
	urldate = {2021-12-05},
	journal = {arXiv:1610.10099 [cs]},
	author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.10099},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{kalchbrenner_neural_2017-1,
	title = {Neural {Machine} {Translation} in {Linear} {Time}},
	url = {http://arxiv.org/abs/1610.10099},
	abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
	urldate = {2021-12-05},
	journal = {arXiv:1610.10099 [cs]},
	author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.10099},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-term {Memory}},
	volume = {9},
	doi = {10.1162/neco.1997.9.8.1735},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = dec,
	year = {1997},
	pages = {1735--80},
}

@article{noauthor_notitle_nodate,
}

@misc{noauthor_librispeech_nodate,
	title = {Librispeech: {An} {ASR} corpus based on public domain audio books {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7178964},
	urldate = {2021-12-04},
}

@article{micikevicius_mixed_2018,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	urldate = {2021-12-04},
	journal = {arXiv:1710.03740 [cs, stat]},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = feb,
	year = {2018},
	note = {arXiv: 1710.03740},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kurzinger_lightweight_2020,
	title = {Lightweight {End}-to-{End} {Speech} {Recognition} from {Raw} {Audio} {Data} {Using} {Sinc}-{Convolutions}},
	url = {http://arxiv.org/abs/2010.07597},
	abstract = {Many end-to-end Automatic Speech Recognition (ASR) systems still rely on pre-processed frequency-domain features that are handcrafted to emulate the human hearing. Our work is motivated by recent advances in integrated learnable feature extraction. For this, we propose Lightweight Sinc-Convolutions (LSC) that integrate Sinc-convolutions with depthwise convolutions as a low-parameter machine-learnable feature extraction for end-to-end ASR systems. We integrated LSC into the hybrid CTC/attention architecture for evaluation. The resulting end-to-end model shows smooth convergence behaviour that is further improved by applying SpecAugment in time-domain. We also discuss filter-level improvements, such as using log-compression as activation function. Our model achieves a word error rate of 10.7\% on the TEDlium v2 test dataset, surpassing the corresponding architecture with log-mel filterbank features by an absolute 1.9\%, but only has 21\% of its model size.},
	urldate = {2021-11-30},
	journal = {arXiv:2010.07597 [cs, eess]},
	author = {Kürzinger, Ludwig and Lindae, Nicolas and Klewitz, Palle and Rigoll, Gerhard},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.07597},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@article{baevski_unsupervised_2021,
	title = {Unsupervised {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2105.11084},
	abstract = {Despite rapid progress in the recent past, current speech recognition systems still require labeled training data which limits this technology to a small fraction of the languages spoken around the globe. This paper describes wav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition models without any labeled data. We leverage self-supervised speech representations to segment unlabeled audio and learn a mapping from these representations to phonemes via adversarial training. The right representations are key to the success of our method. Compared to the best previous unsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT benchmark from 26.1 to 11.3. On the larger English Librispeech benchmark, wav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the best published systems trained on 960 hours of labeled data from only two years ago. We also experiment on nine other languages, including low-resource languages such as Kyrgyz, Swahili and Tatar.},
	urldate = {2021-11-26},
	journal = {arXiv:2105.11084 [cs, eess]},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
	month = oct,
	year = {2021},
	note = {arXiv: 2105.11084},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{graves_associative_2018,
	title = {Associative {Compression} {Networks} for {Representation} {Learning}},
	url = {http://arxiv.org/abs/1804.02476},
	abstract = {This paper introduces Associative Compression Networks (ACNs), a new framework for variational autoencoding with neural networks. The system differs from existing variational autoencoders (VAEs) in that the prior distribution used to model each code is conditioned on a similar code from the dataset. In compression terms this equates to sequentially transmitting the dataset using an ordering determined by proximity in latent space. Since the prior need only account for local, rather than global variations in the latent space, the coding cost is greatly reduced, leading to rich, informative codes. Crucially, the codes remain informative when powerful, autoregressive decoders are used, which we argue is fundamentally difficult with normal VAEs. Experimental results on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover high-level latent features such as object class, writing style, pose and facial expression, which can be used to cluster and classify the data, as well as to generate diverse and convincing samples. We conclude that ACNs are a promising new direction for representation learning: one that steps away from IID modelling, and towards learning a structured description of the dataset as a whole.},
	urldate = {2021-11-22},
	journal = {arXiv:1804.02476 [cs, stat]},
	author = {Graves, Alex and Menick, Jacob and Oord, Aaron van den},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02476},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{dai_re-examination_2019,
	title = {Re-examination of the {Role} of {Latent} {Variables} in {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1902.01388},
	abstract = {With latent variables, stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence. However, opposite results are also observed in other domains, where standard recurrent networks often outperform stochastic models. To better understand this discrepancy, we re-examine the roles of latent variables in stochastic recurrent models for speech density estimation. Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations, the stochastic models were implicitly leveraging intra-step correlation but the standard recurrent baselines were prohibited to do so, resulting in an unfair comparison. To correct the unfairness, we remove such restriction in our re-examination, where all the models can explicitly leverage intra-step correlation with an auto-regressive structure. Over a diverse set of sequential data, including human speech, MIDI music, handwriting trajectory and frame-permuted speech, our results show that stochastic recurrent models fail to exhibit any practical advantage despite the claimed theoretical superiority. In contrast, standard recurrent models equipped with an auto-regressive output distribution consistently perform better, significantly advancing the state-of-the-art results on three speech datasets.},
	urldate = {2021-11-22},
	journal = {arXiv:1902.01388 [cs, stat]},
	author = {Dai, Zihang and Lai, Guokun and Yang, Yiming and Yoo, Shinjae},
	month = sep,
	year = {2019},
	note = {arXiv: 1902.01388},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{garbacea_low_2019,
	title = {Low {Bit}-{Rate} {Speech} {Coding} with {VQ}-{VAE} and a {WaveNet} {Decoder}},
	url = {http://arxiv.org/abs/1910.06464},
	doi = {10.1109/ICASSP.2019.8683277},
	abstract = {In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.},
	urldate = {2021-11-10},
	journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	author = {Gârbacea, Cristina and Oord, Aäron van den and Li, Yazhe and Lim, Felicia S. C. and Luebs, Alejandro and Vinyals, Oriol and Walters, Thomas C.},
	month = may,
	year = {2019},
	note = {arXiv: 1910.06464},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	pages = {735--739},
}

@article{chelba_n-gram_2017,
	title = {N-gram {Language} {Modeling} using {Recurrent} {Neural} {Network} {Estimation}},
	url = {https://arxiv.org/abs/1703.10724v2},
	abstract = {We investigate the effective memory depth of RNN models by using them for \$n\$-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the \$n\$-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM \$n\$-gram matches the LSTM LM performance for \$n=9\$ and slightly outperforms it for \$n=13\$. When allowing dependencies across sentence boundaries, the LSTM \$13\$-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM \$n\$-gram smoothing also has the desirable property of improving with increasing \$n\$-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low \$n\$-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short \$n\$-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts (\$n {\textgreater} 5\$); depending on the task and amount of data it can match fully recurrent LSTM models at about \$n=13\$. This may have implications when modeling short-format text, e.g. voice search/query LMs. Building LSTM \$n\$-gram LMs may be appealing for some practical situations: the state in a \$n\$-gram LM can be succinctly represented with \$(n-1)*4\$ bytes storing the identity of the words in the context and batches of \$n\$-gram contexts can be processed in parallel. On the downside, the \$n\$-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.},
	language = {en},
	urldate = {2021-11-10},
	author = {Chelba, Ciprian and Norouzi, Mohammad and Bengio, Samy},
	month = mar,
	year = {2017},
}

@article{miller_stable_2019-1,
	title = {Stable {Recurrent} {Models}},
	url = {http://arxiv.org/abs/1805.10369},
	abstract = {Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.},
	urldate = {2021-11-10},
	journal = {arXiv:1805.10369 [cs, stat]},
	author = {Miller, John and Hardt, Moritz},
	month = mar,
	year = {2019},
	note = {arXiv: 1805.10369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cuellar_application_2006,
	address = {Dordrecht},
	title = {{AN} {APPLICATION} {OF} {NON}-{LINEAR} {PROGRAMMING} {TO} {TRAIN} {RECURRENT} {NEURAL} {NETWORKS} {IN} {TIME} {SERIES} {PREDICTION} {PROBLEMS}},
	isbn = {978-1-4020-5347-4},
	doi = {10.1007/978-1-4020-5347-4_11},
	abstract = {Artificial Neural Networks are bioinspired mathematical models that have been widely used to solve many complex problems. However, the training of a Neural Network is a difficult task since the traditional training algorithms may get trapped into local solutions easily. This problem is greater in Recurrent Neural Networks, where the traditional training algorithms sometimes provide unsuitable solutions. Some evolutionary techniques have also been used to improve the training stage, and to overcome such local solutions, but they have the disadvantage that the time taken to train the network is high. The objective of this work is to show that the use of some non-linear programming techniques is a good choice to train a Neural Network, since they may provide suitable solutions quickly. In the experimental section, we apply the models proposed to train an Elman Recurrent Neural Network in real-life Time Series Prediction problems.},
	language = {en},
	booktitle = {Enterprise {Information} {Systems} {VII}},
	publisher = {Springer Netherlands},
	author = {Cuéllar, M.P. and Delgado, M. and Pegalajar, M.C.},
	editor = {Chen, Chin-Sheng and Filipe, Joaquim and Seruca, Isabel and Cordeiro, José},
	year = {2006},
	keywords = {Hide Layer, Hide Neuron, Mean Square Error, Neuron Layer, Recurrent Neural Network},
	pages = {95--102},
}

@misc{hardt_when_nodate,
	title = {When {Recurrent} {Models} {Don}'t {Need} to be {Recurrent}},
	url = {http://offconvex.github.io/2018/07/27/approximating-recurrent/},
	abstract = {Algorithms off the convex path.},
	urldate = {2021-11-09},
	journal = {Off the convex path},
	author = {Hardt, Moritz},
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Representation {Learning}},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Abstracts, Boltzmann machine, Deep learning, Feature extraction, Learning systems, Machine learning, Manifolds, Neural networks, Speech recognition, autoencoder, feature learning, neural nets, representation learning, unsupervised learning},
	pages = {1798--1828},
}

@article{papamakarios_masked_2018,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	url = {http://arxiv.org/abs/1705.07057},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2021-11-09},
	journal = {arXiv:1705.07057 [cs, stat]},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	month = jun,
	year = {2018},
	note = {arXiv: 1705.07057},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{oord_pixel_2016,
	title = {Pixel {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1601.06759},
	abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
	urldate = {2021-11-09},
	journal = {arXiv:1601.06759 [cs]},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	month = aug,
	year = {2016},
	note = {arXiv: 1601.06759},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{yan_videogpt_2021,
	title = {{VideoGPT}: {Video} {Generation} using {VQ}-{VAE} and {Transformers}},
	shorttitle = {{VideoGPT}},
	url = {http://arxiv.org/abs/2104.10157},
	abstract = {We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html},
	urldate = {2021-10-27},
	journal = {arXiv:2104.10157 [cs]},
	author = {Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
	month = sep,
	year = {2021},
	note = {arXiv: 2104.10157},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{lancucki_robust_2020,
	title = {Robust {Training} of {Vector} {Quantized} {Bottleneck} {Models}},
	doi = {10.1109/IJCNN48605.2020.9207145},
	abstract = {In this paper we demonstrate methods for reliable and efficient training of discrete representation using Vector-Quantized Variational Auto-Encoder models (VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial representations of speech, applicable to unsupervised voice conversion and reaching state-of-the-art performance on unit discovery tasks. For unsupervised representation learning, they became viable alternatives to continuous latent variable models such as the Variational Auto-Encoder (VAE). However, training deep discrete variable models is challenging, due to the inherent non-differentiability of the discretization operation. In this paper we focus on VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par with its continuous counterparts. It quantizes encoder outputs with on-line k-means clustering. We show that the codebook learning can suffer from poor initialization and non-stationarity of clustered encoder outputs. We demonstrate that these can be successfully overcome by increasing the learning rate for the codebook and periodic date-dependent codeword re-initialization. As a result, we achieve more robust training across different tasks, and significantly increase the usage of latent codewords even for large codebooks. This has practical benefit, for instance, in unsupervised representation learning, where large codebooks may lead to disentanglement of latent representations.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Łańcucki, Adrian and Chorowski, Jan and Sanchez, Guillaume and Marxer, Ricard and Chen, Nanxin and Dolfing, Hans J.G.A. and Khurana, Sameer and Alumäe, Tanel and Laurent, Antoine},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Feature extraction, Nickel, Quantization (signal), Reservoirs, Robustness, Task analysis, Training, VQ-VAE, discrete information bottle-neck, k-means},
	pages = {1--7},
}

@article{sonderby_continuous_nodate,
	title = {Continuous {Relaxation} {Training} of {Discrete} {Latent} {Variable} {Image} {Models}},
	abstract = {Despite recent improvements in training methodology, discrete latent variable models have failed to achieve the performance and popularity of their continuous counterparts. Here, we evaluate several approaches to training large-scale image models on CIFAR-10 using a probabilistic variant of the recently proposed Vector Quantized VAE architecture. We ﬁnd that biased estimators such as continuous relaxations provide reliable methods for training these models while unbiased score-function-based estimators like VIMCO struggle in high-dimensional discrete spaces. Furthermore, we observe that the learned discrete codes lie on low-dimensional manifolds, indicating that discrete latent variables can learn to represent continuous latent quantities. Our ﬁndings show that continuous relaxation training of discrete latent variable models is a powerful method for learning representations that can ﬂexibly capture both continuous and discrete aspects of natural data.},
	language = {en},
	author = {Sønderby, Casper Kaae and Poole, Ben and Mnih, Andriy},
	pages = {4},
}

@article{hsu_learning_2017,
	title = {Learning {Latent} {Representations} for {Speech} {Generation} and {Transformation}},
	url = {http://arxiv.org/abs/1704.04222},
	abstract = {An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.},
	urldate = {2021-10-08},
	journal = {arXiv:1704.04222 [cs, stat]},
	author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
	month = sep,
	year = {2017},
	note = {arXiv: 1704.04222},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huang_neural_2018,
	title = {Neural {Autoregressive} {Flows}},
	url = {http://arxiv.org/abs/1804.00779},
	abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time, via Inverse Autoregressive Flows (IAF). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
	urldate = {2021-10-08},
	journal = {arXiv:1804.00779 [cs, stat]},
	author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.00779},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{spector_preventing_nodate,
	title = {Preventing {Index} {Collapse} in {Discrete} {VAEs} for {Sentences}},
	abstract = {We introduce a discrete latent variable model
for sentence generation based on the VectorQuantized Variational Autoencoder (VQ-VAE)
introduced in Oord et al. (2017). To prevent the
problem of index collapse, where usage of the
discrete latents is limited to only a small number
of values, we propose a modification to the VQVAE training scheme based on K-Means clustering. Empirically, we achieve superior performance in both index collapse, reconstruction perplexity, and language modeling perplexity compared to the training methods used in Oord et al.
(2017) and Kaiser et al. (2018). Though we
demonstrate the viability of discrete latent variables for text, we are unable to fully replicate the
performance of continuous VAE’s on sentences,
which we hypothesize relates to the degeneracy
of the variational family used by VQ-VAE.},
	author = {Spector, Asher and Ren, Jason and Yang, Tristan},
}

@article{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2021-10-04},
	journal = {arXiv:1704.01279 [cs]},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.01279},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
}

@article{salimans_pixelcnn_2017,
	title = {{PixelCNN}++: {Improving} the {PixelCNN} with {Discretized} {Logistic} {Mixture} {Likelihood} and {Other} {Modifications}},
	shorttitle = {{PixelCNN}++},
	url = {http://arxiv.org/abs/1701.05517},
	abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
	urldate = {2021-08-30},
	journal = {arXiv:1701.05517 [cs, stat]},
	author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.05517},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kingma_semi-supervised_2014,
	title = {Semi-{Supervised} {Learning} with {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1406.5298},
	abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
	urldate = {2021-06-17},
	journal = {arXiv:1406.5298 [cs, stat]},
	author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
	month = oct,
	year = {2014},
	note = {arXiv: 1406.5298},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maaloe_auxiliary_nodate,
	title = {Auxiliary {Deep} {Generative} {Models}},
	abstract = {Deep generative models parameterized by neural networks have recently achieved state-ofthe-art performance in unsupervised and semisupervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our ﬁndings suggest that more expressive and properly speciﬁed deep generative models converge faster with better results. We show state-of-theart performance within semi-supervised learning on MNIST, SVHN and NORB datasets.},
	language = {en},
	author = {Maaløe, Lars and Sønderby, Casper Kaae and Sønderby, Søren Kaae and Winther, Ole},
	pages = {9},
}

@article{larsen_autoencoding_nodate,
	title = {Autoencoding beyond pixels using a learned similarity metric},
	abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual ﬁdelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modiﬁed using simple arithmetic.},
	language = {en},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	pages = {9},
}

@misc{noauthor_home_nodate,
	title = {Home {Page} for 20 {Newsgroups} {Data} {Set}},
	url = {http://qwone.com/~jason/20Newsgroups/},
	urldate = {2021-06-08},
}

@article{jin_what_2020,
	title = {What {Disease} does this {Patient} {Have}? {A} {Large}-scale {Open} {Domain} {Question} {Answering} {Dataset} from {Medical} {Exams}},
	shorttitle = {What {Disease} does this {Patient} {Have}?},
	url = {http://arxiv.org/abs/2009.13081},
	abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
	urldate = {2021-06-08},
	journal = {arXiv:2009.13081 [cs]},
	author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.13081},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{jin_what_2020-1,
	title = {What {Disease} does this {Patient} {Have}? {A} {Large}-scale {Open} {Domain} {Question} {Answering} {Dataset} from {Medical} {Exams}},
	shorttitle = {What {Disease} does this {Patient} {Have}?},
	url = {http://arxiv.org/abs/2009.13081},
	abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
	urldate = {2021-06-08},
	journal = {arXiv:2009.13081 [cs]},
	author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.13081},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{khattab_relevance-guided_2020,
	title = {Relevance-guided {Supervision} for {OpenQA} with {ColBERT}},
	url = {http://arxiv.org/abs/2007.00814},
	abstract = {Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose a weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on both Natural Questions and TriviaQA, and the resulting end-to-end OpenQA system attains state-of-the-art performance on both of those datasets.},
	urldate = {2021-06-08},
	journal = {arXiv:2007.00814 [cs]},
	author = {Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.00814},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@article{annamoradnejad_colbert_2021,
	title = {{ColBERT}: {Using} {BERT} {Sentence} {Embedding} for {Humor} {Detection}},
	shorttitle = {{ColBERT}},
	url = {http://arxiv.org/abs/2004.12765},
	abstract = {Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. In this paper, we propose a novel approach for detecting humor in short texts based on the general linguistic structure of humor. Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs of parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms the baseline models with a large margin, showing the importance of utilizing linguistic structure of texts in machine learning models.},
	urldate = {2021-06-08},
	journal = {arXiv:2004.12765 [cs]},
	author = {Annamoradnejad, Issa and Zoghi, Gohar},
	month = apr,
	year = {2021},
	note = {arXiv: 2004.12765},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2, I.7},
}

@article{khattab_colbert_2020,
	title = {{ColBERT}: {Efficient} and {Effective} {Passage} {Search} via {Contextualized} {Late} {Interaction} over {BERT}},
	shorttitle = {{ColBERT}},
	url = {http://arxiv.org/abs/2004.12832},
	abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.},
	urldate = {2021-06-08},
	journal = {arXiv:2004.12832 [cs]},
	author = {Khattab, Omar and Zaharia, Matei},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.12832},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@article{baevski_unsupervised_nodate,
	title = {Unsupervised {Speech} {Recognition}},
	language = {en},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
	pages = {35},
}

@article{elias_parallel_2020,
	title = {Parallel {Tacotron}: {Non}-{Autoregressive} and {Controllable} {TTS}},
	shorttitle = {Parallel {Tacotron}},
	url = {http://arxiv.org/abs/2010.11439},
	abstract = {Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still room for improvements to its efficiency and naturalness. This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder. This model, called {\textbackslash}emph\{Parallel Tacotron\}, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware. The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem and improves naturalness. To further improve the naturalness, we use lightweight convolutions, which can efficiently capture local contexts, and introduce an iterative spectrogram loss inspired by iterative refinement. Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective evaluations with significantly decreased inference time.},
	urldate = {2021-05-25},
	journal = {arXiv:2010.11439 [cs, eess]},
	author = {Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Weiss, Ron and Wu, Yonghui},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11439},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{chorowski_unsupervised_2019,
	title = {Unsupervised speech representation learning using {WaveNet} autoencoders},
	volume = {27},
	issn = {2329-9290, 2329-9304},
	url = {http://arxiv.org/abs/1901.08810},
	doi = {10.1109/TASLP.2019.2938863},
	abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.{\textbackslash} phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
	number = {12},
	urldate = {2021-05-20},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and Oord, Aäron van den},
	month = dec,
	year = {2019},
	note = {arXiv: 1901.08810},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	pages = {2041--2053},
}

@article{schlesinger_vae_2021,
	title = {{VAE} {Approximation} {Error}: {ELBO} and {Conditional} {Independence}},
	shorttitle = {{VAE} {Approximation} {Error}},
	url = {http://arxiv.org/abs/2102.09310},
	abstract = {The importance of Variational Autoencoders reaches far beyond standalone generative models -- the approach is also used for learning latent representations and can be generalized to semi-supervised learning. This requires a thorough analysis of their commonly known shortcomings: posterior collapse and approximation errors. This paper analyzes VAE approximation errors caused by the combination of the ELBO objective with the choice of the encoder probability family, in particular under conditional independence assumptions. We identify the subclass of generative models consistent with the encoder family. We show that the ELBO optimizer is pulled from the likelihood optimizer towards this consistent subset. Furthermore, this subset can not be enlarged, and the respective error cannot be decreased, by only considering deeper encoder networks.},
	urldate = {2021-05-19},
	journal = {arXiv:2102.09310 [cs, stat]},
	author = {Schlesinger, Dmitrij and Shekhovtsov, Alexander and Flach, Boris},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09310},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{havasi_training_2020,
	title = {Training independent subnetworks for robust prediction},
	url = {http://arxiv.org/abs/2010.06610},
	abstract = {Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved `for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.},
	urldate = {2021-05-19},
	journal = {arXiv:2010.06610 [cs, stat]},
	author = {Havasi, Marton and Jenatton, Rodolphe and Fort, Stanislav and Liu, Jeremiah Zhe and Snoek, Jasper and Lakshminarayanan, Balaji and Dai, Andrew M. and Tran, Dustin},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.06610},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jang_categorical_2017,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	urldate = {2021-05-19},
	journal = {arXiv:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = aug,
	year = {2017},
	note = {arXiv: 1611.01144},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jozefowicz_exploring_2016,
	title = {Exploring the {Limits} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	urldate = {2021-05-18},
	journal = {arXiv:1602.02410 [cs]},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02410},
	keywords = {Computer Science - Computation and Language},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2021-05-18},
	journal = {arXiv:2006.11477 [cs, eess]},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.11477},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{shen_natural_2018,
	title = {Natural {TTS} {Synthesis} by {Conditioning} {WaveNet} on {Mel} {Spectrogram} {Predictions}},
	url = {http://arxiv.org/abs/1712.05884},
	abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of \$4.53\$ comparable to a MOS of \$4.58\$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and \$F\_0\$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.},
	urldate = {2021-05-17},
	journal = {arXiv:1712.05884 [cs]},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
	month = feb,
	year = {2018},
	note = {arXiv: 1712.05884},
	keywords = {Computer Science - Computation and Language},
}

@article{elias_parallel_2021,
	title = {Parallel {Tacotron} 2: {A} {Non}-{Autoregressive} {Neural} {TTS} {Model} with {Differentiable} {Duration} {Modeling}},
	shorttitle = {Parallel {Tacotron} 2},
	url = {http://arxiv.org/abs/2103.14574},
	abstract = {This paper introduces Parallel Tacotron 2, a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic Time Warping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations. Its duration control capability is also demonstrated.},
	urldate = {2021-05-17},
	journal = {arXiv:2103.14574 [cs, eess]},
	author = {Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Skerry-Ryan, R. J. and Wu, Yonghui},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.14574},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{paine_fast_2016,
	title = {Fast {Wavenet} {Generation} {Algorithm}},
	url = {http://arxiv.org/abs/1611.09482},
	abstract = {This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2{\textasciicircum}L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.},
	urldate = {2021-05-17},
	journal = {arXiv:1611.09482 [cs]},
	author = {Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A. and Huang, Thomas S.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.09482},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Sound},
}

@article{oord_parallel_2017,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	url = {http://arxiv.org/abs/1711.10433},
	abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
	urldate = {2021-05-17},
	journal = {arXiv:1711.10433 [cs]},
	author = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10433},
	keywords = {Computer Science - Machine Learning},
}

@article{yu_multi-scale_2016,
	title = {Multi-{Scale} {Context} {Aggregation} by {Dilated} {Convolutions}},
	url = {http://arxiv.org/abs/1511.07122},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	urldate = {2021-05-17},
	journal = {arXiv:1511.07122 [cs]},
	author = {Yu, Fisher and Koltun, Vladlen},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.07122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{elias_parallel_2021-1,
	title = {Parallel {Tacotron} 2: {A} {Non}-{Autoregressive} {Neural} {TTS} {Model} with {Differentiable} {Duration} {Modeling}},
	shorttitle = {Parallel {Tacotron} 2},
	url = {http://arxiv.org/abs/2103.14574},
	abstract = {This paper introduces Parallel Tacotron 2, a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic Time Warping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations. Its duration control capability is also demonstrated.},
	urldate = {2021-05-16},
	journal = {arXiv:2103.14574 [cs, eess]},
	author = {Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Skerry-Ryan, R. J. and Wu, Yonghui},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.14574},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{wang_tacotron_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Tacotron},
	url = {/paper/Tacotron%3A-Towards-End-to-End-Speech-Synthesis-Wang-Skerry-Ryan/a072c2a400f62f720b68dc54a662fb1ae115bf06},
	abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it\&\#39;s substantially faster than sample-level autoregressive methods.},
	language = {en},
	urldate = {2021-05-16},
	journal = {undefined},
	author = {Wang, Yuxuan and Skerry-Ryan, R. and Stanton, Daisy and Wu, Y. and Weiss, Ron J. and Jaitly, Navdeep and Yang, Z. and Xiao, Y. and Chen, Z. and Bengio, S. and Le, Quoc V. and Agiomyrgiannakis, Yannis and Clark, R. and Saurous, R.},
	year = {2017},
}

@article{hono_periodnet_2021,
	title = {{PeriodNet}: {A} non-autoregressive waveform generation model with a structure separating periodic and aperiodic components},
	shorttitle = {{PeriodNet}},
	url = {http://arxiv.org/abs/2102.07786},
	abstract = {We propose PeriodNet, a non-autoregressive (non-AR) waveform generation model with a new model structure for modeling periodic and aperiodic components in speech waveforms. The non-AR waveform generation models can generate speech waveforms parallelly and can be used as a speech vocoder by conditioning an acoustic feature. Since a speech waveform contains periodic and aperiodic components, both components should be appropriately modeled to generate a high-quality speech waveform. However, it is difficult to decompose the components from a natural speech waveform in advance. To address this issue, we propose a parallel model and a series model structure separating periodic and aperiodic components. The features of our proposed models are that explicit periodic and aperiodic signals are taken as input, and external periodic/aperiodic decomposition is not needed in training. Experiments using a singing voice corpus show that our proposed structure improves the naturalness of the generated waveform. We also show that the speech waveforms with a pitch outside of the training data range can be generated with more naturalness.},
	urldate = {2021-05-16},
	journal = {arXiv:2102.07786 [cs, eess]},
	author = {Hono, Yukiya and Takaki, Shinji and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.07786},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@article{stoller_seq-u-net_2019,
	title = {Seq-{U}-{Net}: {A} {One}-{Dimensional} {Causal} {U}-{Net} for {Efficient} {Sequence} {Modelling}},
	shorttitle = {Seq-{U}-{Net}},
	url = {http://arxiv.org/abs/1911.06393},
	abstract = {Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. However, efficiently modelling long-term dependencies in these sequences is still challenging. Although the receptive field of these models grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, prohibiting the use of longer receptive fields in practice. To increase efficiency, we make use of the "slow feature" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales and adapt it to our auto-regressive scenario by making convolutions causal. We apply our model ("Seq-U-Net") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computation time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance in all tasks.},
	urldate = {2021-05-16},
	journal = {arXiv:1911.06393 [cs, eess, stat]},
	author = {Stoller, Daniel and Tian, Mi and Ewert, Sebastian and Dixon, Simon},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06393},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{dai_transformer-xl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2021-05-12},
	journal = {arXiv:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.02860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{williams_hierarchical_2020,
	title = {Hierarchical {Quantized} {Autoencoders}},
	url = {http://arxiv.org/abs/2002.08111},
	abstract = {Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.},
	urldate = {2021-05-11},
	journal = {arXiv:2002.08111 [cs, stat]},
	author = {Williams, Will and Ringer, Sam and Ash, Tom and Hughes, John and MacLeod, David and Dougherty, Jamie},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.08111},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{sundermeyer_lstm_2012,
	title = {{LSTM} neural networks for language modeling},
	abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a ﬁxed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difﬁcult to train and therefore are unlikely to show the full potential of recurrent models.},
	language = {en},
	booktitle = {Thirteenth annual conference of the international speech communication association},
	author = {Sundermeyer, Martin and Schlüter, Ralf and Ney, Hermann},
	year = {2012},
	pages = {4},
}

@article{alquraishi_proteinnet_2019,
	title = {{ProteinNet}: a standardized data set for machine learning of protein structure},
	volume = {20},
	issn = {1471-2105},
	shorttitle = {{ProteinNet}},
	url = {https://doi.org/10.1186/s12859-019-2932-0},
	doi = {10.1186/s12859-019-2932-0},
	abstract = {Rapid progress in deep learning has spurred its application to bioinformatics problems including protein structure prediction and design. In classic machine learning problems like computer vision, progress has been driven by standardized data sets that facilitate fair assessment of new methods and lower the barrier to entry for non-domain experts. While data sets of protein sequence and structure exist, they lack certain components critical for machine learning, including high-quality multiple sequence alignments and insulated training/validation splits that account for deep but only weakly detectable homology across protein space.},
	number = {1},
	urldate = {2021-05-05},
	journal = {BMC Bioinformatics},
	author = {AlQuraishi, Mohammed},
	month = jun,
	year = {2019},
	keywords = {CASP, Co-evolution, Database, Deep learning, Machine learning, PSSM, Protein sequence, Protein structure, Protein structure prediction, Proteins},
	pages = {311},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2021-05-05},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2021-05-03},
	journal = {arXiv:2006.11239 [cs, stat]},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.11239},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{auger-methe_state-space_2016,
	title = {State-space models’ dirty little secrets: even simple linear {Gaussian} models can have estimation problems},
	volume = {6},
	copyright = {2016 The Author(s)},
	issn = {2045-2322},
	shorttitle = {State-space models’ dirty little secrets},
	url = {https://www.nature.com/articles/srep26677},
	doi = {10.1038/srep26677},
	abstract = {State-space models (SSMs) are increasingly used in ecology to model time-series such as animal movement paths and population dynamics. This type of hierarchical model is often structured to account for two levels of variability: biological stochasticity and measurement error. SSMs are flexible. They can model linear and nonlinear processes using a variety of statistical distributions. Recent ecological SSMs are often complex, with a large number of parameters to estimate. Through a simulation study, we show that even simple linear Gaussian SSMs can suffer from parameter- and state-estimation problems. We demonstrate that these problems occur primarily when measurement error is larger than biological stochasticity, the condition that often drives ecologists to use SSMs. Using an animal movement example, we show how these estimation problems can affect ecological inference. Biased parameter estimates of a SSM describing the movement of polar bears (Ursus maritimus) result in overestimating their energy expenditure. We suggest potential solutions, but show that it often remains difficult to estimate parameters. While SSMs are powerful tools, they can give misleading results and we urge ecologists to assess whether the parameters can be estimated accurately before drawing ecological conclusions from their results.},
	language = {en},
	number = {1},
	urldate = {2021-05-03},
	journal = {Scientific Reports},
	author = {Auger-Méthé, Marie and Field, Chris and Albertsen, Christoffer M. and Derocher, Andrew E. and Lewis, Mark A. and Jonsen, Ian D. and Mills Flemming, Joanna},
	month = may,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {26677},
}

@article{hafner_learning_2019,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	url = {http://arxiv.org/abs/1811.04551},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
	urldate = {2021-05-03},
	journal = {arXiv:1811.04551 [cs, stat]},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.04551},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chung_hierarchical_2017,
	title = {Hierarchical {Multiscale} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1609.01704},
	abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
	urldate = {2021-05-03},
	journal = {arXiv:1609.01704 [cs]},
	author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	month = mar,
	year = {2017},
	note = {arXiv: 1609.01704},
	keywords = {Computer Science - Machine Learning},
}

@article{koutnik_clockwork_2014,
	title = {A {Clockwork} {RNN}},
	url = {http://arxiv.org/abs/1402.3511},
	abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
	urldate = {2021-05-03},
	journal = {arXiv:1402.3511 [cs]},
	author = {Koutník, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Jürgen},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.3511},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2021-04-28},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{kim_variational_2019,
	title = {Variational {Temporal} {Abstraction}},
	url = {http://arxiv.org/abs/1910.00775},
	abstract = {We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpy-imagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.},
	urldate = {2021-04-28},
	journal = {arXiv:1910.00775 [cs, stat]},
	author = {Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00775},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2021-04-19},
	journal = {arXiv:1711.00937 [cs]},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv: 1711.00937},
	keywords = {Computer Science - Machine Learning},
}

@article{saxena_clockwork_2021,
	title = {Clockwork {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2102.09532},
	abstract = {Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding long-term dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CW-VAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.},
	urldate = {2021-04-19},
	journal = {arXiv:2102.09532 [cs]},
	author = {Saxena, Vaibhav and Ba, Jimmy and Hafner, Danijar},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09532},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{sutskever_towards_2015,
	title = {Towards {Principled} {Unsupervised} {Learning}},
	url = {http://arxiv.org/abs/1511.06440},
	abstract = {General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks. In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. We demonstrate that the ODM cost works well on number of small and semi-artificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.},
	urldate = {2021-04-16},
	journal = {arXiv:1511.06440 [cs]},
	author = {Sutskever, Ilya and Jozefowicz, Rafal and Gregor, Karol and Rezende, Danilo and Lillicrap, Tim and Vinyals, Oriol},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.06440},
	keywords = {Computer Science - Machine Learning},
}

@article{dieleman_challenge_2018,
	title = {The challenge of realistic music generation: modelling raw audio at scale},
	shorttitle = {The challenge of realistic music generation},
	url = {http://arxiv.org/abs/1806.10474},
	abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
	urldate = {2021-04-08},
	journal = {arXiv:1806.10474 [cs, eess, stat]},
	author = {Dieleman, Sander and Oord, Aäron van den and Simonyan, Karen},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.10474},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{nalisnick_deep_2019,
	title = {Do {Deep} {Generative} {Models} {Know} {What} {They} {Don}'t {Know}?},
	url = {http://arxiv.org/abs/1810.09136},
	abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
	urldate = {2021-04-08},
	journal = {arXiv:1810.09136 [cs, stat]},
	author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.09136},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{havtorn_hierarchical_2021,
	title = {Hierarchical {VAEs} {Know} {What} {They} {Don}'t {Know}},
	url = {http://arxiv.org/abs/2102.08248},
	abstract = {Deep generative models have shown themselves to be state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.},
	urldate = {2021-04-08},
	journal = {arXiv:2102.08248 [cs, stat]},
	author = {Havtorn, Jakob D. and Frellsen, Jes and Hauberg, Søren and Maaløe, Lars},
	month = mar,
	year = {2021},
	note = {arXiv: 2102.08248},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kalatzis_variational_2020,
	title = {Variational {Autoencoders} with {Riemannian} {Brownian} {Motion} {Priors}},
	url = {http://arxiv.org/abs/2002.05227},
	abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
	urldate = {2021-03-24},
	journal = {arXiv:2002.05227 [cs, stat]},
	author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, Søren},
	month = aug,
	year = {2020},
	note = {arXiv: 2002.05227},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tran_unsupervised_2016,
	title = {Unsupervised {Neural} {Hidden} {Markov} {Models}},
	url = {http://arxiv.org/abs/1609.09007},
	abstract = {In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.},
	urldate = {2021-03-23},
	journal = {arXiv:1609.09007 [cs]},
	author = {Tran, Ke and Bisk, Yonatan and Vaswani, Ashish and Marcu, Daniel and Knight, Kevin},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.09007},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{schmid_single-molecule_2016,
	title = {Single-{Molecule} {Analysis} beyond {Dwell} {Times}: {Demonstration} and {Assessment} in and out of {Equilibrium}},
	volume = {111},
	issn = {0006-3495},
	shorttitle = {Single-{Molecule} {Analysis} beyond {Dwell} {Times}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5052450/},
	doi = {10.1016/j.bpj.2016.08.023},
	abstract = {We present a simple and robust technique for extracting kinetic rate models and thermodynamic quantities from single-molecule time traces. Single-molecule analysis of complex kinetic sequences (SMACKS) is a maximum-likelihood approach that resolves all statistically relevant rates and also their uncertainties. This is achieved by optimizing one global kinetic model based on the complete data set while allowing for experimental variations between individual trajectories. In contrast to dwell-time analysis, which is the current standard method, SMACKS includes every experimental data point, not only dwell times. As a result, it works as well for long trajectories as for an equivalent set of short ones. In addition, the previous systematic overestimation of fast over slow rates is solved. We demonstrate the power of SMACKS on the kinetics of the multidomain protein Hsp90 measured by single-molecule Förster resonance energy transfer. Experiments in and out of equilibrium are analyzed and compared to simulations, shedding new light on the role of Hsp90’s ATPase function. SMACKS resolves accurate rate models even if states cause indistinguishable signals. Thereby, it pushes the boundaries of single-molecule kinetics beyond those of current methods.},
	number = {7},
	urldate = {2021-03-23},
	journal = {Biophysical Journal},
	author = {Schmid, Sonja and Götz, Markus and Hugel, Thorsten},
	month = oct,
	year = {2016},
	pmid = {27705761},
	pmcid = {PMC5052450},
	pages = {1375--1384},
}

@article{schmid_efficient_2018,
	title = {Efficient use of single molecule time traces to resolve kinetic rates, models and uncertainties},
	volume = {148},
	issn = {0021-9606},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6520253/},
	doi = {10.1063/1.5006604},
	abstract = {Single molecule time traces reveal the time evolution of unsynchronized kinetic systems. Especially single molecule Förster resonance energy transfer (smFRET) provides access to enzymatically important time scales, combined with molecular distance resolution and minimal interference with the sample. Yet the kinetic analysis of smFRET time traces is complicated by experimental shortcomings—such as photo-bleaching and noise. Here we recapitulate the fundamental limits of single molecule fluorescence that render the classic, dwell-time based kinetic analysis unsuitable. In contrast, our Single Molecule Analysis of Complex Kinetic Sequences (SMACKS) considers every data point and combines the information of many short traces in one global kinetic rate model. We demonstrate the potential of SMACKS by resolving the small kinetic effects caused by different ionic strengths in the chaperone protein Hsp90. These results show an unexpected interrelation between conformational dynamics and ATPase activity in Hsp90.},
	number = {12},
	urldate = {2021-03-23},
	journal = {The Journal of chemical physics},
	author = {Schmid, Sonja and Hugel, Thorsten},
	month = mar,
	year = {2018},
	pmid = {29604821},
	pmcid = {PMC6520253},
	pages = {123312},
}

@article{mckinney_analysis_2006,
	title = {Analysis of single-molecule {FRET} trajectories using hidden {Markov} modeling},
	volume = {91},
	issn = {0006-3495},
	doi = {10.1529/biophysj.106.082487},
	abstract = {The analysis of single-molecule fluorescence resonance energy transfer (FRET) trajectories has become one of significant biophysical interest. In deducing the transition rates between various states of a system for time-binned data, researchers have relied on simple, but often arbitrary methods of extracting rates from FRET trajectories. Although these methods have proven satisfactory in cases of well-separated, low-noise, two- or three-state systems, they become less reliable when applied to a system of greater complexity. We have developed an analysis scheme that casts single-molecule time-binned FRET trajectories as hidden Markov processes, allowing one to determine, based on probability alone, the most likely FRET-value distributions of states and their interconversion rates while simultaneously determining the most likely time sequence of underlying states for each trajectory. Together with a transition density plot and Bayesian information criterion we can also determine the number of different states present in a system in addition to the state-to-state transition probabilities. Here we present the algorithm and test its limitations with various simulated data and previously reported Holliday junction data. The algorithm is then applied to the analysis of the binding and dissociation of three RecA monomers on a DNA construct.},
	language = {eng},
	number = {5},
	journal = {Biophysical Journal},
	author = {McKinney, Sean A. and Joo, Chirlmin and Ha, Taekjip},
	month = sep,
	year = {2006},
	pmid = {16766620},
	pmcid = {PMC1544307},
	keywords = {Algorithms, Biopolymers, Computer Simulation, Fluorescence Resonance Energy Transfer, Markov Chains, Models, Chemical, Models, Statistical, Motion},
	pages = {1941--1951},
}

@article{gm_comprehensive_2020,
	title = {A comprehensive survey and analysis of generative models in machine learning},
	volume = {38},
	issn = {1574-0137},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013720303853},
	doi = {10.1016/j.cosrev.2020.100285},
	abstract = {Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.},
	language = {en},
	urldate = {2021-03-18},
	journal = {Computer Science Review},
	author = {Gm, Harshvardhan and Gourisaria, Mahendra Kumar and Pandey, Manjusha and Rautaray, Siddharth Swarup},
	month = nov,
	year = {2020},
	keywords = {Bayesian inference, Deep learning, Generative models, Machine learning, Neural networks},
	pages = {100285},
}

@misc{chaochao_agnostic_2020,
	title = {The {Agnostic} {Hypothesis}: {A} {Unifying} {View} of {Machine} {Learning}},
	shorttitle = {The {Agnostic} {Hypothesis}},
	url = {https://causallu.com/2020/10/24/the-agnostic-hypothesis-a-unifying-view-of-machine-learning/},
	abstract = {1. Introduction In the past decade, machine learning has been playing an increasingly important role in the revolution of artificial intelligence (AI). As the products and services driven by machin…},
	language = {en},
	urldate = {2021-03-10},
	journal = {Chaochao Lu},
	author = {{Chaochao}},
	month = oct,
	year = {2020},
}

@article{pearl_introduction_2010,
	title = {An {Introduction} to {Causal} {Inference}},
	volume = {6},
	issn = {1557-4679},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2836213/},
	doi = {10.2202/1557-4679.1203},
	abstract = {This paper summarizes recent advances in causal inference and underscores the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underlie all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in , which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: those about (1) the effects of potential interventions, (2) probabilities of counterfactuals, and (3) direct and indirect effects (also known as "mediation"). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both. The tools are demonstrated in the analyses of mediation, causes of effects, and probabilities of causation.},
	number = {2},
	urldate = {2021-03-10},
	journal = {The International Journal of Biostatistics},
	author = {Pearl, Judea},
	month = feb,
	year = {2010},
	pmid = {20305706},
	pmcid = {PMC2836213},
}

@techreport{kilic_generalizing_2020,
	type = {preprint},
	title = {Generalizing {HMMs} to {Continuous} {Time} for {Fast} {Kinetics}: {Hidden} {Markov} {Jump} {Processes}},
	shorttitle = {Generalizing {HMMs} to {Continuous} {Time} for {Fast} {Kinetics}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.07.28.225052},
	abstract = {The hidden Markov model (HMM) is a framework for time series analysis widely applied to single molecule experiments. It has traditionally been used to interpret signals generated by systems, such as single molecules, evolving in a discrete state space observed at discrete time levels dictated by the data acquisition rate. Within the HMM framework, originally developed for applications outside the Natural Sciences, such as speech recognition, transitions between states, such as molecular conformational states, are modeled as occurring at the end of each data acquisition period and are described using transition probabilities. Yet, while measurements are often performed at discrete time levels in the Natural Sciences, physical systems evolve in continuous time according to transition rates. It then follows that the modeling assumptions underlying the HMM are justiﬁed if the transition rates of a physical process from state to state are small as compared to the data acquisition rate. In other words, HMMs apply to slow kinetics. The problem is, as the transition rates are unknown in principle, it is unclear, a priori, whether the HMM applies to a particular system. For this reason, we must generalize HMMs for physical systems, such as single molecules, as these switch between discrete states in continuous time. We do so by exploiting recent mathematical tools developed in the context of inferring Markov jump processes and propose the hidden Markov jump process (HMJP). We explicitly show in what limit the HMJP reduces to the HMM. Resolving the discrete time discrepancy of the HMM has clear implications: we no longer need to assume that processes, such as molecular events, must occur on timescales slower than data acquisition and can learn transition rates even if these are on the same timescale or otherwise exceed data acquisition rates.},
	language = {en},
	urldate = {2021-03-10},
	institution = {Biophysics},
	author = {Kilic, Zeliha and Sgouralis, Ioannis and Pressé, Steve},
	month = jul,
	year = {2020},
	doi = {10.1101/2020.07.28.225052},
}

@article{kilic_generalizing_2020-1,
	title = {Generalizing {HMMs} to {Continuous} {Time} for {Fast} {Kinetics}: {Hidden} {Markov} {Jump} {Processes}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Generalizing {HMMs} to {Continuous} {Time} for {Fast} {Kinetics}},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.28.225052v2},
	doi = {10.1101/2020.07.28.225052},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}The hidden Markov model (HMM) is a framework for time series analysis widely applied to single molecule experiments. It has traditionally been used to interpret signals generated by systems, such as single molecules, evolving in a discrete state space observed at discrete time levels dictated by the data acquisition rate. Within the HMM framework, originally developed for applications outside the Natural Sciences, such as speech recognition, transitions between states, such as molecular conformational states, are modeled as occurring at the end of each data acquisition period and are described using transition probabilities. Yet, while measurements are often performed at discrete time levels in the Natural Sciences, physical systems evolve in continuous time according to transition rates. It then follows that the modeling assumptions underlying the HMM are justified if the transition rates of a physical process from state to state are small as compared to the data acquisition rate. In other words, HMMs apply to slow kinetics. The problem is, as the transition rates are unknown in principle, it is unclear, \textit{a priori}, whether the HMM applies to a particular system. For this reason, we must generalize HMMs for physical systems, such as single molecules, as these switch between discrete states in \textit{continuous time}. We do so by exploiting recent mathematical tools developed in the context of inferring Markov jump processes and propose the hidden Markov jump process (HMJP). We explicitly show in what limit the HMJP reduces to the HMM. Resolving the discrete time discrepancy of the HMM has clear implications: we no longer need to assume that processes, such as molecular events, must occur on timescales slower than data acquisition and can learn transition rates even if these are on the same timescale or otherwise exceed data acquisition rates.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-03-10},
	journal = {bioRxiv},
	author = {Kilic, Zeliha and Sgouralis, Ioannis and Pressé, Steve},
	month = jul,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.07.28.225052},
}

@article{kilic_continuous_2020,
	title = {A {Continuous} {Time} {Representation} of {smFRET} for the {Extraction} of {Rapid} {Kinetics}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.28.267468v1},
	doi = {10.1101/2020.08.28.267468},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Our goal is to learn kinetic rates from single molecule FRET (smFRET) data even if these exceed the data acquisition rate. To achieve this, we develop a variant of our recently proposed \textit{hidden Markov jump process} (HMJP) with which we learn transition kinetics from parallel measurements in donor and acceptor channels. Our HMJP generalizes the hidden Markov model (HMM) paradigm in two critical ways: (1) it deals with physical smFRET systems as they switch between conformational states in \textit{continuous time}; (2) it estimates the transition rates between conformational states directly without having recourse to transition probabilities or assuming slow dynamics (as is necessary of the HMM). Our continuous time treatment learns transition kinetics and photon emission rates for dynamical regimes inaccessible to the HMM which treats system kinetics in discrete time. We validate the robustness of our framework on simulated data and demonstrate its performance on experimental data from FRET labeled Holliday junctions.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2021-03-10},
	journal = {bioRxiv},
	author = {Kilic, Zeliha and Sgouralis, Ioannis and Heo, Wooseok and Ishii, Kunihiko and Tahara, Tahei and Pressé, Steve},
	month = aug,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.08.28.267468},
}

@article{segal_high-throughput_2019,
	series = {Single molecule approaches of nucleic acids conformational changes},
	title = {High-throughput {smFRET} analysis of freely diffusing nucleic acid molecules and associated proteins},
	volume = {169},
	issn = {1046-2023},
	url = {https://www.sciencedirect.com/science/article/pii/S1046202318304432},
	doi = {10.1016/j.ymeth.2019.07.021},
	abstract = {Single-molecule Förster resonance energy transfer (smFRET) is a powerful technique for nanometer-scale studies of single molecules. Solution-based smFRET, in particular, can be used to study equilibrium intra- and intermolecular conformations, binding/unbinding events and conformational changes under biologically relevant conditions without ensemble averaging. However, single-spot smFRET measurements in solution are slow. Here, we detail a high-throughput smFRET approach that extends the traditional single-spot confocal geometry to a multispot one. The excitation spots are optically conjugated to two custom silicon single photon avalanche diode (SPAD) arrays. Two-color excitation is implemented using a periodic acceptor excitation (PAX), allowing distinguishing between singly- and doubly-labeled molecules. We demonstrate the ability of this setup to rapidly and accurately determine FRET efficiencies and population stoichiometries by pooling the data collected independently from the multiple spots. We also show how the high throughput of this approach can be used o increase the temporal resolution of single-molecule FRET population characterization from minutes to seconds. Combined with microfluidics, this high-throughput approach will enable simple real-time kinetic studies as well as powerful molecular screening applications.},
	language = {en},
	urldate = {2021-03-10},
	journal = {Methods},
	author = {Segal, Maya and Ingargiola, Antonino and Lerner, Eitan and Chung, SangYoon and White, Jonathan A. and Streets, Aaron and Weiss, S. and Michalet, X.},
	month = oct,
	year = {2019},
	keywords = {Freely-diffusing, High-throughput, SPAD array, Single-molecule FRET},
	pages = {21--45},
}

@article{li_automatic_2020,
	title = {Automatic classification and segmentation of single-molecule fluorescence time traces with deep learning},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-19673-1},
	doi = {10.1038/s41467-020-19673-1},
	abstract = {Traces from single-molecule fluorescence microscopy (SMFM) experiments exhibit photophysical artifacts that typically necessitate human expert screening, which is time-consuming and introduces potential for user-dependent expectation bias. Here, we use deep learning to develop a rapid, automatic SMFM trace selector, termed AutoSiM, that improves the sensitivity and specificity of an assay for a DNA point mutation based on single-molecule recognition through equilibrium Poisson sampling (SiMREPS). The improved performance of AutoSiM is based on accepting both more true positives and fewer false positives than the conventional approach of hidden Markov modeling (HMM) followed by hard thresholding. As a second application, the selector is used for automated screening of single-molecule Förster resonance energy transfer (smFRET) data to identify high-quality traces for further analysis, and achieves {\textasciitilde}90\% concordance with manual selection while requiring less processing time. Finally, we show that AutoSiM can be adapted readily to novel datasets, requiring only modest Transfer Learning.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Nature Communications},
	author = {Li, Jieming and Zhang, Leyou and Johnson-Buck, Alexander and Walter, Nils G.},
	month = nov,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {5833},
}

@article{atan_deep-treat_nodate,
	title = {Deep-{Treat}: {Learning} {Optimal} {Personalized} {Treatments} from {Observational} {Data} using {Neural} {Networks}},
	abstract = {We propose a novel approach for constructing effective treatment policies when the observed data is biased and lacks counterfactual information. Learning in settings where the observed data does not contain all possible outcomes for all treatments is difﬁcult since the observed data is typically biased due to existing clinical guidelines. This is an important problem in the medical domain as collecting unbiased data is expensive and so learning from the wealth of existing biased data is a worthwhile task. Our approach separates the problem into two stages: ﬁrst we reduce the bias by learning a representation map using a novel auto-encoder network – this allows us to control the trade-off between the bias-reduction and the information loss – and then we construct effective treatment policies on the transformed data using a novel feedforward network. Separation of the problem into these two stages creates an algorithm that can be adapted to the problem at hand – the bias-reduction step can be performed as a preprocessing step for other algorithms. We compare our algorithm against state-of-art algorithms on two semi-synthetic datasets and demonstrate that our algorithm achieves a significant improvement in performance.},
	language = {en},
	author = {Atan, Onur and Jordon, James},
	pages = {8},
}

@article{lopez-paz_discovering_2017,
	title = {Discovering {Causal} {Signals} in {Images}},
	url = {http://arxiv.org/abs/1605.08179},
	abstract = {This paper establishes the existence of observable footprints that reveal the "causal dispositions" of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.},
	urldate = {2021-03-08},
	journal = {arXiv:1605.08179 [cs, stat]},
	author = {Lopez-Paz, David and Nishihara, Robert and Chintala, Soumith and Schölkopf, Bernhard and Bottou, Léon},
	month = oct,
	year = {2017},
	note = {arXiv: 1605.08179},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@article{goudet_causal_2018,
	title = {Causal {Generative} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.08936},
	abstract = {We present Causal Generative Neural Networks (CGNNs) to learn functional causal models from observational data. CGNNs leverage conditional independencies and distributional asymmetries to discover bivariate and multivariate causal structures. CGNNs make no assumption regarding the lack of confounders, and learn a differentiable generative model of the data by using backpropagation. Extensive experiments show their good performances comparatively to the state of the art in observational causal discovery on both simulated and real data, with respect to cause-effect inference, v-structure identification, and multivariate causal discovery.},
	urldate = {2021-03-08},
	journal = {arXiv:1711.08936 [stat]},
	author = {Goudet, Olivier and Kalainathan, Diviyan and Caillou, Philippe and Guyon, Isabelle and Lopez-Paz, David and Sebag, Michèle},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.08936},
	keywords = {Statistics - Machine Learning},
}

@article{kallus_deepmatch_2018,
	title = {{DeepMatch}: {Balancing} {Deep} {Covariate} {Representations} for {Causal} {Inference} {Using} {Adversarial} {Training}},
	shorttitle = {{DeepMatch}},
	url = {http://arxiv.org/abs/1802.05664},
	abstract = {We study optimal covariate balance for causal inferences from observational data when rich covariates and complex relationships necessitate flexible modeling with neural networks. Standard approaches such as propensity weighting and matching/balancing fail in such settings due to miscalibrated propensity nets and inappropriate covariate representations, respectively. We propose a new method based on adversarial training of a weighting and a discriminator network that effectively addresses this methodological gap. This is demonstrated through new theoretical characterizations of the method as well as empirical results using both fully connected architectures to learn complex relationships and convolutional architectures to handle image confounders, showing how this new method can enable strong causal analyses in these challenging settings.},
	urldate = {2021-03-08},
	journal = {arXiv:1802.05664 [stat]},
	author = {Kallus, Nathan},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05664},
	keywords = {Statistics - Machine Learning},
}

@article{johansson_learning_2018,
	title = {Learning {Weighted} {Representations} for {Generalization} {Across} {Designs}},
	url = {http://arxiv.org/abs/1802.08598},
	abstract = {Predictive models that generalize well under distributional shift are often desirable and sometimes crucial to building robust and reliable machine learning applications. We focus on distributional shift that arises in causal inference from observational data and in unsupervised domain adaptation. We pose both of these problems as prediction under a shift in design. Popular methods for overcoming distributional shift make unrealistic assumptions such as having a well-specified model or knowing the policy that gave rise to the observed data. Other methods are hindered by their need for a pre-specified metric for comparing observations, or by poor asymptotic properties. We devise a bound on the generalization error under design shift, incorporating both representation learning and sample re-weighting. Based on the bound, we propose an algorithmic framework that does not require any of the above assumptions and which is asymptotically consistent. We empirically study the new framework using two synthetic datasets, and demonstrate its effectiveness compared to previous methods.},
	urldate = {2021-03-08},
	journal = {arXiv:1802.08598 [stat]},
	author = {Johansson, Fredrik D. and Kallus, Nathan and Shalit, Uri and Sontag, David},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.08598},
	keywords = {Statistics - Machine Learning},
}

@article{li_matching_nodate,
	title = {Matching on {Balanced} {Nonlinear} {Representations} for {Treatment} {Effects} {Estimation}},
	abstract = {Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classiﬁcation problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be signiﬁcantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.},
	language = {en},
	author = {Li, Sheng and Fu, Yun},
	pages = {11},
}

@article{shalit_estimating_2017,
	title = {Estimating individual treatment effect: generalization bounds and algorithms},
	shorttitle = {Estimating individual treatment effect},
	url = {http://arxiv.org/abs/1606.03976},
	abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
	urldate = {2021-03-08},
	journal = {arXiv:1606.03976 [cs, stat]},
	author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
	month = may,
	year = {2017},
	note = {arXiv: 1606.03976},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{louizos_causal_nodate,
	title = {Causal {Effect} {Inference} with {Deep} {Latent}-{Variable} {Models}},
	abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a speciﬁc patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is signiﬁcantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
	language = {en},
	author = {Louizos, Christos and Shalit, Uri and Mooij, Joris M and Sontag, David and Zemel, Richard and Welling, Max},
	pages = {11},
}

@article{wang_blessings_2019,
	title = {The {Blessings} of {Multiple} {Causes}},
	url = {http://arxiv.org/abs/1805.06826},
	abstract = {Causal inference from observational data often assumes "ignorability," that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.},
	urldate = {2021-03-08},
	journal = {arXiv:1805.06826 [cs, stat]},
	author = {Wang, Yixin and Blei, David M.},
	month = apr,
	year = {2019},
	note = {arXiv: 1805.06826},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{ranganath_multiple_2019,
	title = {Multiple {Causal} {Inference} with {Latent} {Confounding}},
	url = {http://arxiv.org/abs/1805.08273},
	abstract = {Causal inference from observational data requires assumptions. These assumptions range from measuring confounders to identifying instruments. Traditionally, causal inference assumptions have focused on estimation of effects for a single treatment. In this work, we construct techniques for estimation with multiple treatments in the presence of unobserved confounding. We develop two assumptions based on shared confounding between treatments and independence of treatments given the confounder. Together, these assumptions lead to a confounder estimator regularized by mutual information. For this estimator, we develop a tractable lower bound. To recover treatment effects, we use the residual information in the treatments independent of the confounder. We validate on simulations and an example from clinical medicine.},
	urldate = {2021-03-08},
	journal = {arXiv:1805.08273 [cs, stat]},
	author = {Ranganath, Rajesh and Perotte, Adler},
	month = mar,
	year = {2019},
	note = {arXiv: 1805.08273},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hartford_counterfactual_2016,
	title = {Counterfactual {Prediction} with {Deep} {Instrumental} {Variables} {Networks}},
	url = {http://arxiv.org/abs/1612.09596},
	abstract = {We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.},
	urldate = {2021-03-08},
	journal = {arXiv:1612.09596 [cs, stat]},
	author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.09596},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
}

@article{peters_causal_2015,
	title = {Causal inference using invariant prediction: identification and confidence intervals},
	shorttitle = {Causal inference using invariant prediction},
	url = {http://arxiv.org/abs/1501.01332},
	abstract = {What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
	urldate = {2021-03-08},
	journal = {arXiv:1501.01332 [stat]},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	month = nov,
	year = {2015},
	note = {arXiv: 1501.01332},
	keywords = {Statistics - Methodology},
}

@article{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2021-03-08},
	journal = {arXiv:2102.11107 [cs]},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.11107},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{hinton_how_2021,
	title = {How to represent part-whole hierarchies in a neural network},
	url = {http://arxiv.org/abs/2102.12627},
	abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
	urldate = {2021-02-27},
	journal = {arXiv:2102.12627 [cs]},
	author = {Hinton, Geoffrey},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12627},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.6, I.4.8},
}

@inproceedings{shen_towards_2019,
	address = {Florence, Italy},
	title = {Towards {Generating} {Long} and {Coherent} {Text} with {Multi}-{Level} {Latent} {Variable} {Models}},
	url = {https://www.aclweb.org/anthology/P19-1200},
	doi = {10.18653/v1/P19-1200},
	abstract = {Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-VAEs. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.},
	urldate = {2021-02-26},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Dinghan and Celikyilmaz, Asli and Zhang, Yizhe and Chen, Liqun and Wang, Xin and Gao, Jianfeng and Carin, Lawrence},
	month = jul,
	year = {2019},
	pages = {2079--2089},
}

@article{bowman_generating_2016,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	urldate = {2021-02-25},
	journal = {arXiv:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	month = may,
	year = {2016},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{khurana_convolutional_2020,
	title = {A {Convolutional} {Deep} {Markov} {Model} for {Unsupervised} {Speech} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2006.02547},
	abstract = {Probabilistic Latent Variable Models (LVMs) provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders (VAEs), their use for speech representation learning remains largely unexplored. In this work, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset (LibriSpeech), ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labeled training examples.},
	urldate = {2021-02-25},
	journal = {arXiv:2006.02547 [cs, eess]},
	author = {Khurana, Sameer and Laurent, Antoine and Hsu, Wei-Ning and Chorowski, Jan and Lancucki, Adrian and Marxer, Ricard and Glass, James},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.02547},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{szalai_super-resolution_2021,
	title = {Super-resolution {Imaging} of {Energy} {Transfer} by {Intensity}-{Based} {STED}-{FRET}},
	issn = {1530-6984},
	url = {https://doi.org/10.1021/acs.nanolett.1c00158},
	doi = {10.1021/acs.nanolett.1c00158},
	abstract = {Förster resonance energy transfer (FRET) imaging methods provide unique insight into the spatial distribution of energy transfer and (bio)molecular interaction events, though they deliver average information for an ensemble of events included in a diffraction-limited volume. Coupling super-resolution fluorescence microscopy and FRET has been a challenging and elusive task. Here, we present STED-FRET, a method of general applicability to obtain super-resolved energy transfer images. In addition to higher spatial resolution, STED-FRET provides a more accurate quantification of interaction and has the capacity of suppressing contributions of noninteracting partners, which are otherwise masked by averaging in conventional imaging. The method capabilities were first demonstrated on DNA-origami model systems, verified on uniformly double-labeled microtubules, and then utilized to image biomolecular interactions in the membrane-associated periodic skeleton (MPS) of neurons.},
	urldate = {2021-02-24},
	journal = {Nano Letters},
	author = {Szalai, Alan M. and Siarry, Bruno and Lukin, Jerónimo and Giusti, Sebastián and Unsain, Nicolás and Cáceres, Alfredo and Steiner, Florian and Tinnefeld, Philip and Refojo, Damián and Jovin, Thomas M. and Stefani, Fernando D.},
	month = feb,
	year = {2021},
	note = {Publisher: American Chemical Society},
}

@inproceedings{doerr_probabilistic_2018,
	title = {Probabilistic {Recurrent} {State}-{Space} {Models}},
	url = {http://proceedings.mlr.press/v80/doerr18a.html},
	abstract = {State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g., LSTMs) proved extremely ...},
	language = {en},
	urldate = {2021-02-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Doerr, Andreas and Daniel, Christian and Schiegg, Martin and Duy, Nguyen-Tuong and Schaal, Stefan and Toussaint, Marc and Sebastian, Trimpe},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1280--1289},
}

@article{rangapuram_deep_nodate,
	title = {Deep {State} {Space} {Models} for {Time} {Series} {Forecasting}},
	abstract = {We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efﬁciency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from large collection of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.},
	language = {en},
	author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
	pages = {10},
}

@article{chung_recurrent_2016,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2021-02-18},
	journal = {arXiv:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1506.02216},
	keywords = {Computer Science - Machine Learning},
}

@article{maaloe_biva_2019,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	shorttitle = {{BIVA}},
	url = {http://arxiv.org/abs/1902.02102},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	urldate = {2021-02-10},
	journal = {arXiv:1902.02102 [cs, stat]},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	month = nov,
	year = {2019},
	note = {arXiv: 1902.02102},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vahdat_nvae_2021,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	shorttitle = {{NVAE}},
	url = {http://arxiv.org/abs/2007.03898},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\${\textbackslash}times\$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
	urldate = {2021-02-10},
	journal = {arXiv:2007.03898 [cs, stat]},
	author = {Vahdat, Arash and Kautz, Jan},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.03898},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mardt_vampnets_2018,
	title = {{VAMPnets} for deep learning of molecular kinetics},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-02388-1},
	doi = {10.1038/s41467-017-02388-1},
	abstract = {There is an increasing demand for computing the relevant structures, equilibria, and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation of simulated coordinates into structural features, dimension reduction, clustering the dimension-reduced data, and estimation of a Markov state model or related model of the interconversion rates between molecular structures. This handcrafted approach demands a substantial amount of modeling expertise, as poor decisions at any step will lead to large modeling errors. Here we employ the variational approach for Markov processes (VAMP) to develop a deep learning framework for molecular kinetics using neural networks, dubbed VAMPnets. A VAMPnet encodes the entire mapping from molecular coordinates to Markov states, thus combining the whole data processing pipeline in a single end-to-end framework. Our method performs equally or better than state-of-the-art Markov modeling methods and provides easily interpretable few-state kinetic models.},
	language = {en},
	number = {1},
	urldate = {2021-02-08},
	journal = {Nature Communications},
	author = {Mardt, Andreas and Pasquali, Luca and Wu, Hao and Noé, Frank},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {5},
}

@article{krishnan_structured_2017,
	title = {Structured {Inference} {Networks} for {Nonlinear} {State} {Space} {Models}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10779},
	language = {en},
	number = {1},
	urldate = {2021-02-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Krishnan, Rahul and Shalit, Uri and Sontag, David},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {Hidden Markov Models},
}

@inproceedings{platt_networks_1992,
	title = {Networks for the {Separation} of {Sources} that are {Superimposed} and {Delayed}},
	volume = {4},
	url = {https://proceedings.neurips.cc/paper/1991/file/c410003ef13d451727aeff9082c29a5c-Paper.pdf},
	urldate = {2021-02-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Platt, John and Faggin, Federico},
	editor = {Moody, J. and Hanson, S. and Lippmann, R. P.},
	year = {1992},
	pages = {730--737},
}

@article{xue_deep_2019,
	title = {Deep {Physiological} {State} {Space} {Model} for {Clinical} {Forecasting}},
	url = {http://arxiv.org/abs/1912.01762},
	abstract = {Clinical forecasting based on electronic medical records (EMR) can uncover the temporal correlations between patients' conditions and outcomes from sequences of longitudinal clinical measurements. In this work, we propose an intervention-augmented deep state space generative model to capture the interactions among clinical measurements and interventions by explicitly modeling the dynamics of patients' latent states. Based on this model, we are able to make a joint prediction of the trajectories of future observations and interventions. Empirical evaluations show that our proposed model compares favorably to several state-of-the-art methods on real EMR data.},
	urldate = {2021-02-08},
	journal = {arXiv:1912.01762 [cs, stat]},
	author = {Xue, Yuan and Zhou, Denny and Du, Nan and Dai, Andrew and Xu, Zhen and Zhang, Kun and Cui, Claire},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.01762},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fraccaro_sequential_2016,
	title = {Sequential {Neural} {Models} with {Stochastic} {Layers}},
	url = {http://arxiv.org/abs/1605.07571},
	abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
	urldate = {2021-02-08},
	journal = {arXiv:1605.07571 [cs, stat]},
	author = {Fraccaro, Marco and Sønderby, Søren Kaae and Paquet, Ulrich and Winther, Ole},
	month = nov,
	year = {2016},
	note = {arXiv: 1605.07571},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{aguilar_bayesian_nodate,
	title = {Bayesian {Inference} on {Latent} {Structure} in {Time} {Series}},
	abstract = {A range of developments in Bayesian time series modelling in recent years has focussed on issues of identifying latent structure in time series. This has led to new uses and interpretations of existing theory for latent process decompositions of dynamic models, and to new models for univariate and multivariate time series. This article draws together concepts and modelling approaches that are central to applications of time series decomposition methods, and reviews recent modelling and applied developments. Several applications in time series analyses in geology, climatology, psychiatry and ﬁnance are discussed, as are related modelling directions and current research frontiers.},
	language = {en},
	author = {Aguilar, Omar and Huerta, Gabriel and Prado, Raquel and West, Mike},
	pages = {16},
}

@article{eichler_graphical_nodate,
	title = {Graphical {Modelling} of {Multivariate} {Time} {Series} with {Latent} {Variables}},
	abstract = {In time series analysis, inference about causeeﬀect relationships is commonly based on the concept of Granger-causality, which exploits temporal structure to achieve causal ordering of dependent variables. One major problem of the application of Granger-causality for the identiﬁcation of causal relationships is the possible presence of latent variables that affect the measured components and thus lead to so-called spurious causalities. In this paper, we describe a new graphical approach for modelling the dependence structure of multivariate stationary time series that are affected by latent variables. Is is based on mixed graphs in which dashed edges indicate associations that are induced by latent variables. For Gaussian processes, we present a new class of models that satisfy the Grangercausality restrictions imposed by such graphs and show that these models can be viewed as suitably constrained ARMA models. Furthermore, we discuss maximum likelihood estimation based on Whittle’s approximation of the log-likelihood and propose an iterative algorithm for computing the associated estimates.},
	language = {en},
	author = {Eichler, Michael},
	pages = {8},
}

@article{sahani_probabilistic_nodate,
	title = {Probabilistic \& {Unsupervised} {Learning}},
	language = {en},
	journal = {. . .},
	author = {Sahani, Maneesh},
	pages = {168},
}

@article{lian_modeling_2015,
	title = {Modeling {Time} {Series} and {Sequences}:  {Learning} {Representations} and {Making} {Predictions}},
	language = {en},
	author = {Lian, Wenzhao},
	year = {2015},
	pages = {117},
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2021-02-08},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cohen_how_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {How to {Train} {Deep} {Variational} {Autoencoders} and {Probabilistic} {Ladder} {Networks}},
	isbn = {978-1-59593-383-6},
	abstract = {Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the ﬂexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the ﬁrst time allowing to train deep models of up to ﬁve stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.},
	language = {en},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	collaborator = {Cohen, William and Moore, Andrew},
	year = {2006},
}

@article{sonderby_ladder_2016,
	title = {Ladder {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1602.02282},
	abstract = {Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
	urldate = {2021-02-07},
	journal = {arXiv:1602.02282 [cs, stat]},
	author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
	month = may,
	year = {2016},
	note = {arXiv: 1602.02282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sonderby_ladder_2016-1,
	title = {Ladder {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1602.02282},
	abstract = {Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
	urldate = {2021-02-04},
	journal = {arXiv:1602.02282 [cs, stat]},
	author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
	month = may,
	year = {2016},
	note = {arXiv: 1602.02282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gemici_generative_2017,
	title = {Generative {Temporal} {Models} with {Memory}},
	url = {http://arxiv.org/abs/1702.04649},
	abstract = {We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.},
	urldate = {2021-02-04},
	journal = {arXiv:1702.04649 [cs, stat]},
	author = {Gemici, Mevlana and Hung, Chia-Chun and Santoro, Adam and Wayne, Greg and Mohamed, Shakir and Rezende, Danilo J. and Amos, David and Lillicrap, Timothy},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04649},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{fraccaro_generative_2018,
	title = {Generative {Temporal} {Models} with {Spatial} {Memory} for {Partially} {Observed} {Environments}},
	url = {http://proceedings.mlr.press/v80/fraccaro18a.html},
	abstract = {In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent’s representations during training or vi...},
	language = {en},
	urldate = {2021-02-04},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fraccaro, Marco and Rezende, Danilo and Zwols, Yori and Pritzel, Alexander and Eslami, S. M. Ali and Viola, Fabio},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1549--1558},
}

@article{baevski_vq-wav2vec_2020,
	title = {vq-wav2vec: {Self}-{Supervised} {Learning} of {Discrete} {Speech} {Representations}},
	shorttitle = {vq-wav2vec},
	url = {http://arxiv.org/abs/1910.05453},
	abstract = {We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.},
	urldate = {2021-01-22},
	journal = {arXiv:1910.05453 [cs]},
	author = {Baevski, Alexei and Schneider, Steffen and Auli, Michael},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.05453},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{jegou_product_2011,
	title = {Product {Quantization} for {Nearest} {Neighbor} {Search}},
	volume = {33},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2010.57},
	abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jégou, H. and Douze, M. and Schmid, C.},
	month = jan,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithms, Artificial Intelligence, Cartesian product, Cluster Analysis, Electronic mail, Euclidean distance, File systems, GIST image descriptor, High-dimensional indexing, Image Interpretation, Computer-Assisted, Image Processing, Computer-Assisted, Image databases, Indexing, Information Storage and Retrieval, Models, Statistical, Nearest neighbor searches, Neural networks, Pattern Recognition, Automated, Permission, Quantization, SIFT image descriptor, Scalability, approximate nearest neighbor search, approximate search., file organisation, image indexing, image retrieval, indexing, inverted file system, low-dimensional subspace, product quantization, subspace quantization index, vector quantisation, very large database, very large databases},
	pages = {117--128},
}

@article{jiang_improving_2019,
	title = {Improving {Transformer}-based {Speech} {Recognition} {Using} {Unsupervised} {Pre}-training},
	url = {http://arxiv.org/abs/1910.09932},
	abstract = {Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3\%, exceeding the best end-to-end model by over 0.2\% absolute CER. With more pre-training data, we can further reduce the CER to 21.0\%, or a 11.8\% relative CER reduction over baseline.},
	urldate = {2021-01-21},
	journal = {arXiv:1910.09932 [cs, eess]},
	author = {Jiang, Dongwei and Lei, Xiaoning and Li, Wubo and Luo, Ne and Hu, Yuxuan and Zou, Wei and Li, Xiangang},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.09932},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{detlefsen_what_2020,
	title = {What is a meaningful representation of protein sequences?},
	url = {http://arxiv.org/abs/2012.02679},
	abstract = {How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.},
	urldate = {2021-01-19},
	journal = {arXiv:2012.02679 [cs, q-bio]},
	author = {Detlefsen, Nicki Skafte and Hauberg, Søren and Boomsma, Wouter},
	month = nov,
	year = {2020},
	note = {arXiv: 2012.02679},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Quantitative Biology - Quantitative Methods},
}

@article{arjovsky_invariant_2020,
	title = {Invariant {Risk} {Minimization}},
	url = {http://arxiv.org/abs/1907.02893},
	abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
	urldate = {2021-01-19},
	journal = {arXiv:1907.02893 [cs, stat]},
	author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.02893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{parthasarathi_lessons_2019,
	title = {Lessons from {Building} {Acoustic} {Models} with a {Million} {Hours} of {Speech}},
	url = {http://arxiv.org/abs/1904.01624},
	abstract = {This is a report of our lessons learned building acoustic models from 1 Million hours of unlabeled speech, while labeled speech is restricted to 7,000 hours. We employ student/teacher training on unlabeled data, helping scale out target generation in comparison to confidence model based methods, which require a decoder and a confidence model. To optimize storage and to parallelize target generation, we store high valued logits from the teacher model. Introducing the notion of scheduled learning, we interleave learning on unlabeled and labeled data. To scale distributed training across a large number of GPUs, we use BMUF with 64 GPUs, while performing sequence training only on labeled data with gradient threshold compression SGD using 16 GPUs. Our experiments show that extremely large amounts of data are indeed useful; with little hyper-parameter tuning, we obtain relative WER improvements in the 10 to 20\% range, with higher gains in noisier conditions.},
	urldate = {2021-01-19},
	journal = {arXiv:1904.01624 [cs, eess, stat]},
	author = {Parthasarathi, Sree Hari Krishnan and Strom, Nikko},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01624},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{xu_self-training_2020,
	title = {Self-training and {Pre}-training are {Complementary} for {Speech} {Recognition}},
	url = {http://arxiv.org/abs/2010.11430},
	abstract = {Self-training and unsupervised pre-training have emerged as effective approaches to improve speech recognition systems using unlabeled data. However, it is not clear whether they learn similar patterns or if they can be effectively combined. In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0 are complementary in a variety of labeled data setups. Using just 10 minutes of labeled data from Libri-light as well as 53k hours of unlabeled data from LibriVox achieves WERs of 3.0\%/5.2\% on the clean and other test sets of Librispeech - rivaling the best published systems trained on 960 hours of labeled data only a year ago. Training on all labeled data of Librispeech achieves WERs of 1.5\%/3.1\%.},
	urldate = {2021-01-19},
	journal = {arXiv:2010.11430 [cs, eess]},
	author = {Xu, Qiantong and Baevski, Alexei and Likhomanenko, Tatiana and Tomasello, Paden and Conneau, Alexis and Collobert, Ronan and Synnaeve, Gabriel and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11430},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{wang_linformer_2020,
	title = {Linformer: {Self}-{Attention} with {Linear} {Complexity}},
	shorttitle = {Linformer},
	url = {http://arxiv.org/abs/2006.04768},
	abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
	urldate = {2021-01-19},
	journal = {arXiv:2006.04768 [cs, stat]},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04768},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2021-01-14},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{likhomanenko_who_2019,
	title = {Who {Needs} {Words}? {Lexicon}-{Free} {Speech} {Recognition}},
	shorttitle = {Who {Needs} {Words}?},
	url = {http://arxiv.org/abs/1904.04479},
	doi = {10.21437/Interspeech.2019-3107},
	abstract = {Lexicon-free speech recognition naturally deals with the problem of out-of-vocabulary (OOV) words. In this paper, we show that character-based language models (LM) can perform as well as word-based LMs for speech recognition, in word error rates (WER), even without restricting the decoding to a lexicon. We study character-based LMs and show that convolutional LMs can effectively leverage large (character) contexts, which is key for good speech recognition performance downstream. We specifically show that the lexicon-free decoding performance (WER) on utterances with OOV words using character-based LMs is better than lexicon-based decoding, both with character or word-based LMs.},
	urldate = {2021-01-13},
	journal = {Interspeech 2019},
	author = {Likhomanenko, Tatiana and Synnaeve, Gabriel and Collobert, Ronan},
	month = sep,
	year = {2019},
	note = {arXiv: 1904.04479},
	keywords = {Computer Science - Computation and Language},
	pages = {3915--3919},
}

@article{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2021-01-06},
	journal = {arXiv:2006.11477 [cs, eess]},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.11477},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{schneider_wav2vec_2019,
	title = {wav2vec: {Unsupervised} {Pre}-training for {Speech} {Recognition}},
	shorttitle = {wav2vec},
	url = {http://arxiv.org/abs/1904.05862},
	abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36\% when only a few hours of transcribed data is available. Our approach achieves 2.43\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.},
	urldate = {2021-01-06},
	journal = {arXiv:1904.05862 [cs]},
	author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
	month = sep,
	year = {2019},
	note = {arXiv: 1904.05862},
	keywords = {Computer Science - Computation and Language},
}

@article{schneider_wav2vec_2019-1,
	title = {wav2vec: {Unsupervised} {Pre}-training for {Speech} {Recognition}},
	shorttitle = {wav2vec},
	url = {http://arxiv.org/abs/1904.05862},
	abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36\% when only a few hours of transcribed data is available. Our approach achieves 2.43\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.},
	urldate = {2021-01-06},
	journal = {arXiv:1904.05862 [cs]},
	author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
	month = sep,
	year = {2019},
	note = {arXiv: 1904.05862},
	keywords = {Computer Science - Computation and Language},
}

@article{van_staden_comparison_2020,
	title = {A comparison of self-supervised speech representations as input features for unsupervised acoustic word embeddings},
	url = {http://arxiv.org/abs/2012.07387},
	abstract = {Many speech processing tasks involve measuring the acoustic similarity between speech segments. Acoustic word embeddings (AWE) allow for efficient comparisons by mapping speech segments of arbitrary duration to fixed-dimensional vectors. For zero-resource speech processing, where unlabelled speech is the only available resource, some of the best AWE approaches rely on weak top-down constraints in the form of automatically discovered word-like segments. Rather than learning embeddings at the segment level, another line of zero-resource research has looked at representation learning at the short-time frame level. Recent approaches include self-supervised predictive coding and correspondence autoencoder (CAE) models. In this paper we consider whether these frame-level features are beneficial when used as inputs for training to an unsupervised AWE model. We compare frame-level features from contrastive predictive coding (CPC), autoregressive predictive coding and a CAE to conventional MFCCs. These are used as inputs to a recurrent CAE-based AWE model. In a word discrimination task on English and Xitsonga data, all three representation learning approaches outperform MFCCs, with CPC consistently showing the biggest improvement. In cross-lingual experiments we find that CPC features trained on English can also be transferred to Xitsonga.},
	urldate = {2021-01-06},
	journal = {arXiv:2012.07387 [cs, eess]},
	author = {van Staden, Lisa and Kamper, Herman},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.07387},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{doersch_tutorial_2016,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2020-12-03},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = aug,
	year = {2016},
	note = {arXiv: 1606.05908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cohen_no_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {[{No} title found]},
	isbn = {978-1-59593-383-6},
	abstract = {Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the ﬂexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the ﬁrst time allowing to train deep models of up to ﬁve stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.},
	language = {en},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	collaborator = {Cohen, William and Moore, Andrew},
	year = {2006},
}

@article{maaloe_biva_nodate,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, ﬂow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classiﬁcation tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	language = {en},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	pages = {12},
}

@article{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2020-11-17},
	journal = {arXiv:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv: 1609.02907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_novo_nodate,
	title = {De novo protein design by deep network hallucination {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.22.211482v1},
	urldate = {2020-10-31},
}

@article{angenent-mari_deep_2020,
	title = {A deep learning approach to programmable {RNA} switches},
	volume = {11},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-020-18677-1},
	doi = {10.1038/s41467-020-18677-1},
	abstract = {Abstract
            
              Engineered RNA elements are programmable tools capable of detecting small molecules, proteins, and nucleic acids. Predicting the behavior of these synthetic biology components remains a challenge, a situation that could be addressed through enhanced pattern recognition from deep learning. Here, we investigate Deep Neural Networks (DNN) to predict toehold switch function as a canonical riboswitch model in synthetic biology. To facilitate DNN training, we synthesize and characterize in vivo a dataset of 91,534 toehold switches spanning 23 viral genomes and 906 human transcription factors. DNNs trained on nucleotide sequences outperform (R
              2
               = 0.43–0.70) previous state-of-the-art thermodynamic and kinetic models (R
              2
               = 0.04–0.15) and allow for human-understandable attention-visualizations (VIS4Map) to identify success and failure modes. This work shows that deep learning approaches can be used for functionality predictions and insight generation in RNA synthetic biology.},
	language = {en},
	number = {1},
	urldate = {2020-10-31},
	journal = {Nature Communications},
	author = {Angenent-Mari, Nicolaas M. and Garruss, Alexander S. and Soenksen, Luis R. and Church, George and Collins, James J.},
	month = dec,
	year = {2020},
	pages = {5057},
}

@inproceedings{gao_leveraging_2017,
	address = {Okinawa, Japan},
	title = {Leveraging side information for speaker identification with the {Enron} conversational telephone speech collection},
	isbn = {978-1-5090-4788-8},
	url = {http://ieeexplore.ieee.org/document/8268988/},
	doi = {10.1109/ASRU.2017.8268988},
	abstract = {Speaker identiﬁcation experiments typically focus on acoustic signals, but conversational speech often occurs in settings where additional useful side information may be available. This paper introduces a new distributable speaker identiﬁcation test collection based on recorded telephone calls of Enron energy traders. Experiments with these recordings demonstrate that social network features and recording channel metadata can be used to reduce error rates in speaker identiﬁcation below that achieved using acoustic evidence alone. Social network features from the parallel Enron email collection (37 of the 41 speakers in the telephone recordings sent or received emails in the collection) improve speaker identiﬁcation, as do social network features computed using lightly supervised techniques to estimate a social network from more than one thousand unlabeled recordings.},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	publisher = {IEEE},
	author = {Gao, Ning and Sell, Gregory and Oard, Douglas W. and Dredze, Mark},
	month = dec,
	year = {2017},
	pages = {577--583},
}

@article{shajkofci_spatially-variant_2020,
	title = {Spatially-{Variant} {CNN}-{Based} {Point} {Spread} {Function} {Estimation} for {Blind} {Deconvolution} and {Depth} {Estimation} in {Optical} {Microscopy}},
	volume = {29},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.2986880},
	abstract = {Optical microscopy is an essential tool in biology and medicine. Imaging thin, yet non-flat objects in a single shot (without relying on more sophisticated sectioning setups) remains challenging as the shallow depth of field that comes with high-resolution microscopes leads to unsharp image regions and makes depth localization and quantitative image interpretation difficult. Here, we present a method that improves the resolution of light microscopy images of such objects by locally estimating image distortion while jointly estimating object distance to the focal plane. Specifically, we estimate the parameters of a spatially-variant Point Spread Function (PSF) model using a Convolutional Neural Network (CNN), which does not require instrumentor object-specific calibration. Our method recovers PSF parameters from the image itself with up to a squared Pearson correlation coefficient of 0.99 in ideal conditions, while remaining robust to object rotation, illumination variations, or photon noise. When the recovered PSFs are used with a spatially-variant and regularized Richardson-Lucy (RL) deconvolution algorithm, we observed up to 2.1 dB better Signal-to-Noise Ratio (SNR) compared to other Blind Deconvolution (BD) techniques. Following microscope-specific calibration, we further demonstrate that the recovered PSF model parameters permit estimating surface depth with a precision of 2 μm and over an extended range when using engineered PSFs. Our method opens up multiple possibilities for enhancing images of non-flat objects with minimal need for a priori knowledge about the optical setup.},
	journal = {IEEE Transactions on Image Processing},
	author = {Shajkofci, Adrian and Liebling, Michael},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Calibration, Deconvolution, Estimation, Microscopy, Optical diffraction, Optical imaging, Optical microscopy, biomedical optical imaging, blind deconvolution, blind deconvolution techniques, calibration, convolutional neural nets, convolutional neural network, convolutional neural networks, deconvolution, depth from focus, depth localization, high-resolution microscopy, image distortion, image enhancement, image reconstruction, image resolution, image restoration, instrumentor object-specific calibration, light microscopy images, medical image processing, microscope-specific calibration, nonflat objects, object distance, optical microscopy, optical transfer function, point spread function estimation, quantitative image interpretation, recovered PSF model parameters, regularized Richardson-Lucy deconvolution algorithm, signal-to-noise ratio, size 2.0 mum, spatially-variant CNN-based point spread function estimation, spatially-variant point spread function model, squared Pearson correlation coefficient, surface depth, unsharp image regions},
	pages = {5848--5861},
}

@article{hidalgo_adapting_2019,
	title = {Adapting the scrum framework for agile project management in science: case study of a distributed research initiative},
	volume = {5},
	issn = {2405-8440},
	shorttitle = {Adapting the scrum framework for agile project management in science},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6441834/},
	doi = {10.1016/j.heliyon.2019.e01447},
	abstract = {This article explores the adoption of agile methods for the management of projects in collaborative research initiatives. The use of the scrum framework, a specific set of agile principles and practices for self-organizing cross-functional teams in software development projects, is currently being expanded to other types of organizations and knowledge management processes. The study addresses the extent to which key principles and tools usually used in scrum, due to their potentially positive influence on team dynamics and efficiency, can contribute to the collaborative management and coordination of tasks in research processes. The responses from interviews with 17 researchers, as well as participant observation and analysis of online activity, are examined and presented as a case study on the adoption of scrum practices in a distributed research centre dedicated to the evaluation of public policies. Results indicate that integrating agile methods and principles for interdisciplinary collaboration requires a high degree of flexibility and a “learn by doing” approach.},
	number = {3},
	urldate = {2020-06-21},
	journal = {Heliyon},
	author = {Hidalgo, Enric Senabre},
	month = mar,
	year = {2019},
	pmid = {30976706},
	pmcid = {PMC6441834},
}

@article{doersch_tutorial_2016-1,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2020-06-05},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = aug,
	year = {2016},
	note = {arXiv: 1606.05908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{schreiber_pomegranate_nodate,
	title = {pomegranate: {Fast} and {Flexible} {Probabilistic} {Modeling} in {Python}},
	language = {en},
	author = {Schreiber, Jacob},
	pages = {6},
}

@article{carlin_autocorrelations_2019,
	title = {Autocorrelations in pulsar glitch waiting times and sizes},
	volume = {488},
	issn = {0035-8711, 1365-2966},
	url = {http://arxiv.org/abs/1907.09143},
	doi = {10.1093/mnras/stz2014},
	abstract = {Among the five pulsars with the most recorded rotational glitches, only PSR J0534\$+\$2200 is found to have an autocorrelation between consecutive glitch sizes which differs significantly from zero (Spearman correlation coefficient \${\textbackslash}rho=-0.46\$, p-value \$=0.046\$). No statistically compelling autocorrelations between consecutive waiting times are found. The autocorrelation observations are interpreted within the framework of a predictive meta-model describing stress-release in terms of a state-dependent Poisson process. Specific combinations of size and waiting time autocorrelations are identified, alongside combinations of cross-correlations and size and waiting time distributions, that are allowed or excluded within the meta-model. For example, future observations of any "quasiperiodic" glitching pulsar, such as PSR J0537\$-\$6910, should not reveal a positive waiting time autocorrelation. The implications for microphysical models of the stress-release process driving pulsar glitches are discussed briefly.},
	number = {4},
	urldate = {2020-05-15},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Carlin, Julian B. and Melatos, Andrew},
	month = oct,
	year = {2019},
	note = {arXiv: 1907.09143},
	keywords = {Astrophysics - High Energy Astrophysical Phenomena},
	pages = {4890--4896},
}

@article{olshen_circular_2004,
	title = {Circular binary segmentation for the analysis of array-based {DNA} copy number data},
	volume = {5},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxh008},
	doi = {10.1093/biostatistics/kxh008},
	abstract = {DNA sequence copy number is the number of copies of DNA at a region of a genome. Cancer progression often involves alterations in DNA copy number. Newly developed microarray technologies enable simultaneous measurement of copy number at thousands of sites in a genome. We have developed a modiﬁcation of binary segmentation, which we call circular binary segmentation, to translate noisy intensity measurements into regions of equal copy number. The method is evaluated by simulation and is demonstrated on cell line data with known copy number alterations and on a breast cancer cell line data set.},
	language = {en},
	number = {4},
	urldate = {2020-05-15},
	journal = {Biostatistics},
	author = {Olshen, A. B. and Venkatraman, E. S. and Lucito, R. and Wigler, M.},
	month = oct,
	year = {2004},
	pages = {557--572},
}

@article{fryzlewicz_wild_2014,
	title = {Wild binary segmentation for multiple change-point detection},
	volume = {42},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1411.0858},
	doi = {10.1214/14-AOS1245},
	abstract = {We propose a new technique, called wild binary segmentation (WBS), for consistent estimation of the number and locations of multiple change-points in data. We assume that the number of change-points can increase to infinity with the sample size. Due to a certain random localisation mechanism, WBS works even for very short spacings between the change-points and/or very small jump magnitudes, unlike standard binary segmentation. On the other hand, despite its use of localisation, WBS does not require the choice of a window or span parameter, and does not lead to a significant increase in computational complexity. WBS is also easy to code. We propose two stopping criteria for WBS: one based on thresholding and the other based on what we term the `strengthened Schwarz information criterion'. We provide default recommended values of the parameters of the procedure and show that it offers very good practical performance in comparison with the state of the art. The WBS methodology is implemented in the R package wbs, available on CRAN. In addition, we provide a new proof of consistency of binary segmentation with improved rates of convergence, as well as a corresponding result for WBS.},
	number = {6},
	urldate = {2020-05-15},
	journal = {The Annals of Statistics},
	author = {Fryzlewicz, Piotr},
	month = dec,
	year = {2014},
	note = {arXiv: 1411.0858},
	keywords = {Mathematics - Statistics Theory},
	pages = {2243--2281},
}

@article{gregorio_single-molecule_2017,
	title = {Single-molecule analysis of ligand efficacy in β 2 {AR}–{G}-protein activation},
	volume = {547},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/nature22354},
	doi = {10.1038/nature22354},
	abstract = {Single-molecule FRET imaging provides insights into the allosteric link between the ligand-binding and G-protein nucleotide-binding pockets of the β2 adrenergic receptor (β2AR) and improved understanding of the G-protein activation mechanism.},
	language = {en},
	number = {7661},
	urldate = {2020-04-06},
	journal = {Nature},
	author = {Gregorio, G. Glenn and Masureel, Matthieu and Hilger, Daniel and Terry, Daniel S. and Juette, Manuel and Zhao, Hong and Zhou, Zhou and Perez-Aguilar, Jose Manuel and Hauge, Maria and Mathiasen, Signe and Javitch, Jonathan A. and Weinstein, Harel and Kobilka, Brian K. and Blanchard, Scott C.},
	month = jul,
	year = {2017},
	note = {Number: 7661
Publisher: Nature Publishing Group},
	pages = {68--73},
}

@article{gregorio_single-molecule_2017-1,
	title = {Single-molecule analysis of ligand efficacy in β 2 {AR}–{G}-protein activation},
	volume = {547},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/nature22354},
	doi = {10.1038/nature22354},
	abstract = {Single-molecule FRET imaging provides insights into the allosteric link between the ligand-binding and G-protein nucleotide-binding pockets of the β2 adrenergic receptor (β2AR) and improved understanding of the G-protein activation mechanism.},
	language = {en},
	number = {7661},
	urldate = {2020-05-07},
	journal = {Nature},
	author = {Gregorio, G. Glenn and Masureel, Matthieu and Hilger, Daniel and Terry, Daniel S. and Juette, Manuel and Zhao, Hong and Zhou, Zhou and Perez-Aguilar, Jose Manuel and Hauge, Maria and Mathiasen, Signe and Javitch, Jonathan A. and Weinstein, Harel and Kobilka, Brian K. and Blanchard, Scott C.},
	month = jul,
	year = {2017},
	note = {Number: 7661
Publisher: Nature Publishing Group},
	pages = {68--73},
}

@article{graves_connectionist_nodate,
	title = {Connectionist {Temporal} {Classiﬁcation}: {Labelling} {Unsegmented} {Sequence} {Data} with {Recurrent} {Neural} {Networks}},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
	language = {en},
	author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
	pages = {8},
}

@article{schreiber_pomegranate_nodate-1,
	title = {pomegranate: {Fast} and {Flexible} {Probabilistic} {Modeling} in {Python}},
	language = {en},
	author = {Schreiber, Jacob},
	pages = {6},
}

@misc{noauthor_prior-apprised_nodate,
	title = {Prior-{Apprised} {Unsupervised} {Learning} of {Sub}-{Pixel} {Curvilinear} {Features} in {Low} {Signal}-to-{Noise} {Images} - {ScienceDirect}},
	url = {https://www-sciencedirect-com.ep.fjernadgang.kb.dk/science/article/pii/S0006349520303271},
	urldate = {2020-04-20},
}

@article{yin_prior-apprised_2020,
	title = {Prior-{Apprised} {Unsupervised} {Learning} of {Sub}-{Pixel} {Curvilinear} {Features} in {Low} {Signal}-to-{Noise} {Images}},
	issn = {0006-3495},
	url = {http://www.sciencedirect.com/science/article/pii/S0006349520303271},
	doi = {10.1016/j.bpj.2020.04.009},
	abstract = {Many biophysical problems involve molecular and nanoscale targets moving next to a curvilinear track, e.g., a cytosolic cargo transported by motor proteins moving along a microtubule. For this type of problems, fluorescence imaging is usually the primary tool of choice. There is however a ∼20-fold mismatch between target-localization precision and track-imaging resolution, such that questions requiring high-fidelity definition of the target’s track remain inaccessible. On the other hand, if the contextual image of the tracks can be refined to a level comparable to that of the target, many intuitive yet mechanistically important issues can begin to be addressed. This work demonstrates that it is possible to statistically infer, to sub-pixel precision, curvilinear features in a low signal-to-noise image. This is achieved by a framework that consists of three stages where in each stage the descriptive prior information that the features come from curvilinear elements is explicitly taken into account: the Hessian-based feature enhancement, the sub-image feature sampling and registration, and the statistical learning of the underlying curvilinear structure using a new method developed here for inferring the principal curves. It is fully automated without user supervision, which is distinctly different from approaches that require user seeding or well-defined training data sets. Computer simulations of realistic images are used to investigate the performance of the framework and its implementation. The characterization results suggest that curvilinear features are refined to the same order of precision as that of the target, and that the bootstrap confidence intervals from the analysis allow an estimate for the statistical bounds of the simulated “true” curve. Also shown are analyses of experimental images from three different microscopy modalities: two-photon laser-scanning microscopy, epi-fluorescence microscopy, and total-internal-reflection fluorescence microscopy. The practical application of this prior-apprised unsupervised learning (PAUL) framework as well as its potential outlook are discussed.},
	language = {en},
	urldate = {2020-04-20},
	journal = {Biophysical Journal},
	author = {Yin, Shuhui and Tien, Ming and Yang, Haw},
	month = apr,
	year = {2020},
}

@misc{noauthor_prior-apprised_nodate-1,
	title = {Prior-{Apprised} {Unsupervised} {Learning} of {Sub}-{Pixel} {Curvilinear} {Features} in {Low} {Signal}-to-{Noise} {Images} - {ScienceDirect}},
	url = {https://www-sciencedirect-com.ep.fjernadgang.kb.dk/science/article/pii/S0006349520303271},
	urldate = {2020-04-20},
}

@article{kelly_new_2012,
	title = {A new method for inferring hidden markov models from noisy time sequences},
	volume = {7},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0029703},
	abstract = {We present a new method for inferring hidden Markov models from noisy time sequences without the necessity of assuming a model architecture, thus allowing for the detection of degenerate states. This is based on the statistical prediction techniques developed by Crutchfield et al. and generates so called causal state models, equivalent in structure to hidden Markov models. The new method is applicable to any continuous data which clusters around discrete values and exhibits multiple transitions between these values such as tethered particle motion data or Fluorescence Resonance Energy Transfer (FRET) spectra. The algorithms developed have been shown to perform well on simulated data, demonstrating the ability to recover the model used to generate the data under high noise, sparse data conditions and the ability to infer the existence of degenerate states. They have also been applied to new experimental FRET data of Holliday Junction dynamics, extracting the expected two state model and providing values for the transition rates in good agreement with previous results and with results obtained using existing maximum likelihood based methods. The method differs markedly from previous Markov-model reconstructions in being able to uncover truly hidden states.},
	language = {eng},
	number = {1},
	journal = {PloS One},
	author = {Kelly, David and Dillingham, Mark and Hudson, Andrew and Wiesner, Karoline},
	year = {2012},
	pmid = {22247783},
	pmcid = {PMC3256161},
	keywords = {Algorithms, Computer Simulation, DNA, Cruciform, Fluorescence Resonance Energy Transfer, Kinetics, Markov Chains, Models, Chemical, Models, Statistical},
	pages = {e29703},
}

@article{preus_isms_2015,
	title = {{iSMS}: single-molecule {FRET} microscopy software},
	volume = {12},
	issn = {1548-7105},
	shorttitle = {{iSMS}},
	url = {http://www.nature.com/articles/nmeth.3435},
	doi = {10.1038/nmeth.3435},
	language = {en},
	number = {7},
	urldate = {2020-04-17},
	journal = {Nature Methods},
	author = {Preus, Søren and Noer, Sofie L. and Hildebrandt, Lasse L. and Gudnason, Daniel and Birkedal, Victoria},
	month = jul,
	year = {2015},
	note = {Number: 7
Publisher: Nature Publishing Group},
	pages = {593--594},
}

@misc{noauthor_practical_nodate,
	title = {A practical guide to single-molecule {FRET} {\textbar} {Nature} {Methods}},
	url = {https://www-nature-com.ep.fjernadgang.kb.dk/articles/nmeth.1208},
	urldate = {2020-04-17},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@book{breiman_classification_nodate,
	address = {Belmont, Calif},
	series = {Wadsworth statistics/probability series.},
	title = {Classification and regression trees},
	isbn = {0-534-98053-8},
	language = {eng},
	publisher = {Wadsworth International Group},
	author = {Breiman, Leo},
}

@article{loh_classification_2011,
	title = {Classification and regression trees},
	volume = {1},
	issn = {1942-4787, 1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.8},
	doi = {10.1002/widm.8},
	language = {en},
	number = {1},
	urldate = {2020-03-16},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Loh, Wei‐Yin},
	month = jan,
	year = {2011},
	pages = {14--23},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@article{chen_xgboost_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2020-03-16},
	journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	note = {arXiv: 1603.02754},
	keywords = {Computer Science - Machine Learning},
	pages = {785--794},
}

@inproceedings{brodersen_balanced_2010,
	title = {The {Balanced} {Accuracy} and {Its} {Posterior} {Distribution}},
	doi = {10.1109/ICPR.2010.764},
	abstract = {Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual cross-validation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.},
	booktitle = {2010 20th {International} {Conference} on {Pattern} {Recognition}},
	author = {Brodersen, Kay Henning and Ong, Cheng Soon and Stephan, Klaas Enno and Buhmann, Joachim M.},
	month = aug,
	year = {2010},
	note = {ISSN: 1051-4651},
	keywords = {Accuracy, Approximation algorithms, Inference algorithms, Machine learning, Prediction algorithms, Probabilistic logic, Training, balanced accuracy, bias, class imbalance, classification algorithm, classification performance, generalisation (artificial intelligence), generalizability, pattern classification, performance evaluation, posterior distribution, statistical distributions},
	pages = {3121--3124},
}

@article{lakshmi_comparative_2018,
	title = {Comparative {Analysis} {On} {A} {Predictive} {Model} {Using} {Tree} {Based} {Machine} {Learning} {Techniques} {For} {Big} {Data} {Analytics}},
	volume = {7},
	copyright = {2018 i-manager publications. All rights reserved.},
	issn = {22775110},
	url = {http://search.proquest.com/docview/2093128857/abstract/12378DA9D6E64CB4PQ/1},
	doi = {http://dx.doi.org.ep.fjernadgang.kb.dk/10.26634/jit.7.2.14647},
	abstract = {Internet of Things (IoT), Big Data (BD), Artificial Intelligence (AI) and Machine Learning (ML) are the novel approaches where communication happens between man-made machines. Machines interact and acquire knowledge by implementing learning algorithms. Data analytics, prediction and classification methods are machine learning approaches applied on Big data for processing various unstructured data patterns. MapReduce is a widely used programming framework to parallelize these machine learning algorithms. To accomplish best outcomes, the algorithms are fine tuned using parallel practice. This technique uses MapReduce model for processing datasets multiple times by tuning the parameters as per the requirement. But this existing MapReduce model endures with high disk rates resulting in low throughput and inefficient time complexity. To achieve the minimal time consumption for tuning the jobs, Apache Spark framework replaces the MapReduce model. This is examined in this paper by evaluating the prediction on "Demand and Supply of India" dataset. A comparative analytical study is proposed in this paper to predict the demand for forecasting by training the existing data using tree based machine learning techniques. The prediction outcomes computed are compared on tree structured ML methods with respect to time and space utilization.},
	language = {English},
	number = {2},
	urldate = {2020-03-16},
	journal = {i-Manager's Journal on Information Technology; Nagercoil},
	author = {Lakshmi, J. V. N.},
	month = may,
	year = {2018},
	keywords = {Big Data, Data Analysis, Decision Tree, Gradient Boosting Tree, Machine Learning, Prediction Random Forest},
	pages = {8--15},
}

@article{hady_mechanical_2015,
	title = {Mechanical {Surface} {Waves} {Accompany} {Action} {Potential} {Propagation}},
	volume = {6},
	issn = {2041-1723},
	url = {http://arxiv.org/abs/1407.7600},
	doi = {10.1038/ncomms7697},
	abstract = {Many studies have shown that a mechanical displacement of the axonal membrane accompanies the electrical pulse defining the Action Potential (AP). Despite a large and diverse body of experimental evidence, there is no theoretical consensus either for the physical basis of this mechanical wave nor its interdependence with the electrical signal. In this manuscript we present a model for these mechanical displacements as arising from the driving of surface wave modes in which potential energy is stored in elastic properties of the neuronal membrane and cytoskeleton while kinetic energy is carried by the axoplasmic fluid. In our model these surface waves are driven by the traveling wave of electrical depolarization that characterizes the AP, altering the compressive electrostatic forces across the membrane as it passes. This driving leads to co-propagating mechanical displacements, which we term Action Waves (AWs). Our model for these AWs allows us to predict, in terms of elastic constants, axon radius and axoplasmic density and viscosity, the shape of the AW that should accompany any traveling wave of voltage, including the AP predicted by the Hodgkin and Huxley (HH) equations. We show that our model makes predictions that are in agreement with results in experimental systems including the garfish olfactory nerve and the squid giant axon. We expect our model to serve as a framework for understanding the physical origins and possible functional roles of these AWs in neurobiology.},
	language = {en},
	number = {1},
	urldate = {2020-02-13},
	journal = {Nature Communications},
	author = {Hady, Ahmed El and Machta, Benjamin B.},
	month = nov,
	year = {2015},
	note = {arXiv: 1407.7600},
	keywords = {Condensed Matter - Soft Condensed Matter, Physics - Biological Physics, Quantitative Biology - Neurons and Cognition},
	pages = {6697},
}

@article{jaynes_information_1957,
	title = {Information {Theory} and {Statistical} {Mechanics}},
	volume = {106},
	issn = {0031-899X},
	url = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
	doi = {10.1103/PhysRev.106.620},
	language = {en},
	number = {4},
	urldate = {2020-02-13},
	journal = {Physical Review},
	author = {Jaynes, E. T.},
	month = may,
	year = {1957},
	pages = {620--630},
}

@article{noauthor_notitle_nodate,
	doi = {10.1103/PhysRev.106.620},
}

@article{crooks_nonequilibrium_nodate,
	title = {Nonequilibrium {Measurements} of {Free} {Energy} {Differences} for {Microscopically} {Reversible} {Markovian} {Systems}},
	language = {en},
	author = {Crooks, Gavin E},
	pages = {7},
}

@article{shannon_mathematical_nodate,
	title = {A {Mathematical} {Theory} of {Communication}},
	language = {en},
	author = {Shannon, C E},
	pages = {55},
}

@article{brillouin_maxwells_1951,
	title = {Maxwell's {Demon} {Cannot} {Operate}: {Information} and {Entropy}. {I}},
	volume = {22},
	issn = {0021-8979, 1089-7550},
	shorttitle = {Maxwell's {Demon} {Cannot} {Operate}},
	url = {http://aip.scitation.org/doi/10.1063/1.1699951},
	doi = {10.1063/1.1699951},
	language = {en},
	number = {3},
	urldate = {2020-02-13},
	journal = {Journal of Applied Physics},
	author = {Brillouin, L.},
	month = mar,
	year = {1951},
	pages = {334--337},
}

@article{heimburg_soliton_2005,
	title = {On soliton propagation in biomembranes and nerves},
	volume = {102},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0503823102},
	doi = {10.1073/pnas.0503823102},
	language = {en},
	number = {28},
	urldate = {2020-02-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Heimburg, T. and Jackson, A. D.},
	month = jul,
	year = {2005},
	pages = {9790--9795},
}

@article{liphardt_equilibrium_2002,
	title = {Equilibrium {Information} from {Nonequilibrium} {Measurements} in an {Experimental} {Test} of {Jarzynski}'s {Equality}},
	volume = {296},
	issn = {00368075, 10959203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1071152},
	doi = {10.1126/science.1071152},
	language = {en},
	number = {5574},
	urldate = {2020-02-13},
	journal = {Science},
	author = {Liphardt, J.},
	month = jun,
	year = {2002},
	pages = {1832--1835},
}

@article{ounis_comparison_1990,
	title = {A {Comparison} of {Brownian} and {Turbulent} {Diffusion}},
	volume = {13},
	issn = {0278-6826, 1521-7388},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02786829008959423},
	doi = {10.1080/02786829008959423},
	language = {en},
	number = {1},
	urldate = {2020-02-10},
	journal = {Aerosol Science and Technology},
	author = {Ounis, Hadj and Ahmadi, Goodarz},
	month = jan,
	year = {1990},
	pages = {47--53},
}

@article{shivamoggi_generalized_2016,
	title = {A generalized {Brownian} motion model for turbulent relative particle dispersion},
	volume = {380},
	issn = {0375-9601},
	url = {http://www.sciencedirect.com/science/article/pii/S0375960116303887},
	doi = {10.1016/j.physleta.2016.06.051},
	abstract = {There is speculation that the difficulty in obtaining an extended range with Richardson–Obukhov scaling in both laboratory experiments and numerical simulations is due to the finiteness of the flow Reynolds number Re in these situations. In this paper, a generalized Brownian motion model has been applied to describe the relative particle dispersion problem in more realistic turbulent flows and to shed some light on this issue. The fluctuating pressure forces acting on a fluid particle are taken to be a colored noise and follow a stationary process and are described by the Uhlenbeck–Ornstein model while it appears plausible to take their correlation time to have a power-law dependence on Re, thus introducing a bridge between the Lagrangian quantities and the Eulerian parameters for this problem. This ansatz is in qualitative agreement with the possibility of a connection speculated earlier by Corrsin [26] between the white-noise representation for the fluctuating pressure forces and the large-Re assumption in the Kolmogorov [4] theory for the 3D fully developed turbulence (FDT) as well as a similar argument of Monin and Yaglom [23] and a similar result of Sawford [13] and Borgas and Sawford [24]. It also provides an insight into the result that the Richardson–Obukhov scaling holds only in the infinite-Re limit and disappears otherwise. This ansatz further provides a determination of the Richardson–Obukhov constant g as a function of Re, with an asymptotic constant value in the infinite-Re limit. It is shown to lead to full agreement, in the small-Re limit as well, with the Batchelor–Townsend [27] scaling for the rate of change of the mean square interparticle separation in 3D FDT, hence validating its soundness further.},
	language = {en},
	number = {36},
	urldate = {2020-02-10},
	journal = {Physics Letters A},
	author = {Shivamoggi, B. K.},
	month = aug,
	year = {2016},
	keywords = {Brownian motion, Turbulence theory},
	pages = {2809--2814},
}

@article{senior_improved_2020,
	title = {Improved protein structure prediction using potentials from deep learning},
	volume = {577},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1923-7},
	doi = {10.1038/s41586-019-1923-7},
	abstract = {Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7. AlphaFold predicts the distances between pairs of residues, is used to construct potentials of mean force that accurately describe the shape of a protein and can be optimized with gradient descent to predict protein structures.},
	language = {en},
	number = {7792},
	urldate = {2020-02-04},
	journal = {Nature},
	author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and Žídek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	month = jan,
	year = {2020},
	pages = {706--710},
}

@article{cameron_systematically_2020,
	title = {Systematically {Improving} {Espresso}: {Insights} from {Mathematical} {Modeling} and {Experiment}},
	volume = {0},
	issn = {2590-2393, 2590-2385},
	shorttitle = {Systematically {Improving} {Espresso}},
	url = {https://www.cell.com/matter/abstract/S2590-2385(19)30410-2},
	doi = {10.1016/j.matt.2019.12.019},
	language = {English},
	number = {0},
	urldate = {2020-01-29},
	journal = {Matter},
	author = {Cameron, Michael I. and Morisco, Dechen and Hofstetter, Daniel and Uman, Erol and Wilkinson, Justin and Kennedy, Zachary C. and Fontenot, Sean A. and Lee, William T. and Hendon, Christopher H. and Foster, Jamie M.},
	month = jan,
	year = {2020},
	keywords = {MAP5: Improvement, coffee, espresso, extraction, granular bed, kinetic model, mathematical model, transport},
}

@article{goyal_robust_2019,
	title = {Robust {Markov} {Decision} {Process}: {Beyond} {Rectangularity}},
	shorttitle = {Robust {Markov} {Decision} {Process}},
	url = {http://arxiv.org/abs/1811.00215},
	abstract = {Markov decision processes (MDPs) are a common approach used to model dynamic optimization problems. MDPs are specified by a set of states, actions, transition probability kernel and the rewards associated with transitions. The goal is to find a policy that maximizes the expected cumulated reward. However, in most real world problems, the model parameters are estimated from noisy observations and are uncertain. The optimal policy for the nominal parameters might be highly sensitive to even small perturbations in the parameters, leading to significantly suboptimal outcomes. To address this issue, we consider a robust approach where the uncertainty in probability transitions is modeled as an adversarial selection from an uncertainty set. Most prior works consider the case where uncertainty on transitions related to different states is uncoupled. However, the case of general uncertainty sets is known to be intractable. We consider a factor model where the transition probability is a linear function of a factor matrix that is uncertain and belongs to a factor matrix uncertainty set. It allows to model dependence between probability transitions across different states and it is significantly less conservative than prior approaches. We show that under a certain assumption, we can efficiently compute an optimal robust policy under the factor matrix uncertainty model. We show that an optimal robust policy can be chosen deterministic and in particular is an optimal policy for some transition kernel in the uncertainty set. This implies strong min-max duality. We introduce the robust counterpart of important structural results of classical MDPs and we provide a computational study to demonstrate the usefulness of our approach, where we present two examples where robustness improves the worst-case and the empirical performances while maintaining a reasonable performance on the nominal parameters.},
	urldate = {2020-01-27},
	journal = {arXiv:1811.00215 [math]},
	author = {Goyal, Vineet and Grand-Clement, Julien},
	month = may,
	year = {2019},
	note = {arXiv: 1811.00215},
	keywords = {Mathematics - Optimization and Control},
}

@article{wiesemann_robust_2012,
	title = {Robust {Markov} {Decision} {Processes}},
	copyright = {Copyright © 2013, INFORMS},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/moor.1120.0566},
	doi = {10.1287/moor.1120.0566},
	abstract = {Markov decision processes (MDPs) are powerful tools for decision making in uncertain dynamic environments. However, the solutions of MDPs are of limited practical use because of their sensitivity t...},
	language = {en},
	urldate = {2020-01-27},
	journal = {Mathematics of Operations Research},
	author = {Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Berç},
	month = nov,
	year = {2012},
}

@article{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	urldate = {2020-01-27},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{darley_fusion_2019,
	title = {The {Fusion} of {Lipid} and {DNA} {Nanotechnology}},
	volume = {10},
	issn = {2073-4425},
	doi = {10.3390/genes10121001},
	abstract = {Lipid membranes form the boundary of many biological compartments, including organelles and cells. Consisting of two leaflets of amphipathic molecules, the bilayer membrane forms an impermeable barrier to ions and small molecules. Controlled transport of molecules across lipid membranes is a fundamental biological process that is facilitated by a diverse range of membrane proteins, including ion-channels and pores. However, biological membranes and their associated proteins are challenging to experimentally characterize. These challenges have motivated recent advances in nanotechnology towards building and manipulating synthetic lipid systems. Liposomes-aqueous droplets enclosed by a bilayer membrane-can be synthesised in vitro and used as a synthetic model for the cell membrane. In DNA nanotechnology, DNA is used as programmable building material for self-assembling biocompatible nanostructures. DNA nanostructures can be functionalised with hydrophobic chemical modifications, which bind to or bridge lipid membranes. Here, we review approaches that combine techniques from lipid and DNA nanotechnology to engineer the topography, permeability, and surface interactions of membranes, and to direct the fusion and formation of liposomes. These approaches have been used to study the properties of membrane proteins, to build biosensors, and as a pathway towards assembling synthetic multicellular systems.},
	language = {eng},
	number = {12},
	journal = {Genes},
	author = {Darley, Es and Singh, Jasleen Kaur Daljit and Surace, Natalie A. and Wickham, Shelley F. J. and Baker, Matthew A. B.},
	year = {2019},
	pmid = {31816934},
	pmcid = {PMC6947036},
	keywords = {DNA nanotechnology, DNA origami, lipid nanotechnology},
}

@article{pezeshkian_multi-scale_2019,
	title = {A {Multi}-{Scale} {Approach} to {Membrane} {Remodeling} {Processes}},
	volume = {6},
	issn = {2296-889X},
	url = {https://www.frontiersin.org/articles/10.3389/fmolb.2019.00059/full},
	doi = {10.3389/fmolb.2019.00059},
	abstract = {We present a multi-scale simulation procedure to describe membrane-related biological processes that span over a wide range of length scales. At macroscopic length-scale, a membrane is described as a flexible thin film modelled by a dynamic triangulated surface with its spatial conformations governed by an elastic energy containing only a few model parameters. An implicit protein model allows us to include complex effects of membraneprotein interactions in the macroscopic description. The gist of this multi-scale approach is a scheme to calibrate the implicit protein model using finer scale simulation techniques e.g. all atom and coarse grain molecular dynamics. We previously used this approach and properly described the formation of membrane tubular invaginations upon binding of B-subunit of Shiga toxin. Here, we provide a perspective of our multi-scale approach, summarizing its main features and sketching possible routes for future development.},
	language = {English},
	urldate = {2020-01-20},
	journal = {Frontiers in Molecular Biosciences},
	author = {Pezeshkian, Weria and König, Melanie and Marrink, Siewert J. and Ipsen, John H.},
	year = {2019},
	keywords = {Dynamic triangulated surfaces, Implicit Protein Model, MARTINI coarse-grain simulation, Shiga Toxin, Simulation of continuum model, membrane remodeling},
}

@article{marrink_computational_2019,
	title = {Computational {Modeling} of {Realistic} {Cell} {Membranes}},
	volume = {119},
	issn = {1520-6890},
	doi = {10.1021/acs.chemrev.8b00460},
	abstract = {Cell membranes contain a large variety of lipid types and are crowded with proteins, endowing them with the plasticity needed to fulfill their key roles in cell functioning. The compositional complexity of cellular membranes gives rise to a heterogeneous lateral organization, which is still poorly understood. Computational models, in particular molecular dynamics simulations and related techniques, have provided important insight into the organizational principles of cell membranes over the past decades. Now, we are witnessing a transition from simulations of simpler membrane models to multicomponent systems, culminating in realistic models of an increasing variety of cell types and organelles. Here, we review the state of the art in the field of realistic membrane simulations and discuss the current limitations and challenges ahead.},
	language = {eng},
	number = {9},
	journal = {Chemical Reviews},
	author = {Marrink, Siewert J. and Corradi, Valentina and Souza, Paulo C. T. and Ingólfsson, Helgi I. and Tieleman, D. Peter and Sansom, Mark S. P.},
	month = may,
	year = {2019},
	pmid = {30623647},
	pmcid = {PMC6509646},
	pages = {6184--6226},
}

@article{mcmahon_membrane_2015,
	title = {Membrane curvature at a glance},
	volume = {128},
	issn = {1477-9137},
	doi = {10.1242/jcs.114454},
	abstract = {Membrane curvature is an important parameter in defining the morphology of cells, organelles and local membrane subdomains. Transport intermediates have simpler shapes, being either spheres or tubules. The generation and maintenance of curvature is of central importance for maintaining trafficking and cellular functions. It is possible that local shapes in complex membranes could help to define local subregions. In this Cell Science at a Glance article and accompanying poster, we summarize how generating, sensing and maintaining high local membrane curvature is an active process that is mediated and controlled by specialized proteins using general mechanisms: (i) changes in lipid composition and asymmetry, (ii) partitioning of shaped transmembrane domains of integral membrane proteins or protein or domain crowding, (iii) reversible insertion of hydrophobic protein motifs, (iv) nanoscopic scaffolding by oligomerized hydrophilic protein domains and, finally, (v) macroscopic scaffolding by the cytoskeleton with forces generated by polymerization and by molecular motors. We also summarize some of the discoveries about the functions of membrane curvature, where in addition to providing cell or organelle shape, local curvature can affect processes like membrane scission and fusion as well as protein concentration and enzyme activation on membranes.},
	language = {eng},
	number = {6},
	journal = {Journal of Cell Science},
	author = {McMahon, Harvey T. and Boucrot, Emmanuel},
	month = mar,
	year = {2015},
	pmid = {25774051},
	pmcid = {PMC4359918},
	keywords = {Amphipatic helix, Animals, BAR domain, Bilayer asymmetry, Cell Membrane, Humans, Intracellular Membranes, Lipid Bilayers, Lipids, Membrane curvature},
	pages = {1065--1070},
}

@article{pezeshkian_multi-scale_2019-1,
	title = {A {Multi}-{Scale} {Approach} to {Membrane} {Remodeling} {Processes}},
	volume = {6},
	issn = {2296-889X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6664084/},
	doi = {10.3389/fmolb.2019.00059},
	abstract = {We present a multi-scale simulation procedure to describe membrane-related biological processes that span over a wide range of length scales. At macroscopic length-scale, a membrane is described as a flexible thin film modeled by a dynamic triangulated surface with its spatial conformations governed by an elastic energy containing only a few model parameters. An implicit protein model allows us to include complex effects of membrane-protein interactions in the macroscopic description. The gist of this multi-scale approach is a scheme to calibrate the implicit protein model using finer scale simulation techniques e.g., all atom and coarse grain molecular dynamics. We previously used this approach and properly described the formation of membrane tubular invaginations upon binding of B-subunit of Shiga toxin. Here, we provide a perspective of our multi-scale approach, summarizing its main features and sketching possible routes for future development.},
	urldate = {2020-01-20},
	journal = {Frontiers in Molecular Biosciences},
	author = {Pezeshkian, Weria and König, Melanie and Marrink, Siewert J. and Ipsen, John H.},
	month = jul,
	year = {2019},
	pmid = {31396522},
	pmcid = {PMC6664084},
}

@article{leonard_developing_2019,
	title = {Developing and {Testing} of {Lipid} {Force} {Fields} with {Applications} to {Modeling} {Cellular} {Membranes}},
	volume = {119},
	issn = {0009-2665},
	url = {https://doi.org/10.1021/acs.chemrev.8b00384},
	doi = {10.1021/acs.chemrev.8b00384},
	abstract = {The amphipathic nature of the lipid molecule (hydrophilic head and hydrophobic tails) enables it to act as a barrier between fluids with various properties and to sustain an environment where the processes critical to life may proceed. While computer simulations of biomolecules primarily investigate protein conformation and binding to drug-like molecules, these interactions often occur in the context of a lipid membrane. Chemical specificity of lipid models is essential to accurately represent the complex environment of the lipid membrane. This review discusses the development and performance of currently used chemically specific lipid force fields (FF) such as the CHARMM, AMBER, GROMOS, OPLS, and MARTINI families. Considerations in lipid FF development including lipid diversity, temperature dependence, phase behavior, and effects of atomic polarizability are considered, as well as methods and goals of parametrization. Applications of these FFs to complex and diverse models for cellular membranes are summarized. Lastly, areas for future development, such as efficient inclusion of long-range Lennard–Jones interactions (significant in transitions from polar to apolar media), accurate transmembrane dipole potential, and diffusion under periodic boundary conditions are considered.},
	number = {9},
	urldate = {2020-01-20},
	journal = {Chemical Reviews},
	author = {Leonard, Alison N. and Wang, Eric and Monje-Galvan, Viviana and Klauda, Jeffery B.},
	month = may,
	year = {2019},
	pages = {6227--6269},
}

@article{lundberg_consistent_2019,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	url = {http://arxiv.org/abs/1802.03888},
	abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
	urldate = {2019-12-03},
	journal = {arXiv:1802.03888 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	month = mar,
	year = {2019},
	note = {arXiv: 1802.03888},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mosegaard_monte_1995,
	title = {Monte {Carlo} sampling of solutions to inverse problems},
	volume = {100},
	copyright = {Copyright 1995 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/94JB03097},
	doi = {10.1029/94JB03097},
	abstract = {Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines a priori information with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the a posteriori probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.). When analysing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyse and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available. The most well known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.},
	language = {en},
	number = {B7},
	urldate = {2019-11-27},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Mosegaard, Klaus and Tarantola, Albert},
	year = {1995},
	pages = {12431--12447},
}

@article{granik_single-particle_2019,
	title = {Single-{Particle} {Diffusion} {Characterization} by {Deep} {Learning}},
	volume = {117},
	issn = {0006-3495},
	url = {http://www.sciencedirect.com/science/article/pii/S0006349519305041},
	doi = {10.1016/j.bpj.2019.06.015},
	abstract = {Diffusion plays a crucial role in many biological processes including signaling, cellular organization, transport mechanisms, and more. Direct observation of molecular movement by single-particle-tracking experiments has contributed to a growing body of evidence that many cellular systems do not exhibit classical Brownian motion but rather anomalous diffusion. Despite this evidence, characterization of the physical process underlying anomalous diffusion remains a challenging problem for several reasons. First, different physical processes can exist simultaneously in a system. Second, commonly used tools for distinguishing between these processes are based on asymptotic behavior, which is experimentally inaccessible in most cases. Finally, an accurate analysis of the diffusion model requires the calculation of many observables because different transport modes can result in the same diffusion power-law α, which is typically obtained from the mean-square displacements (MSDs). The outstanding challenge in the field is to develop a method to extract an accurate assessment of the diffusion process using many short trajectories with a simple scheme that is applicable at the nonexpert level. Here, we use deep learning to infer the underlying process resulting in anomalous diffusion. We implement a neural network to classify single-particle trajectories by diffusion type: Brownian motion, fractional Brownian motion and continuous time random walk. Further, we demonstrate the applicability of our network architecture for estimating the Hurst exponent for fractional Brownian motion and the diffusion coefficient for Brownian motion on both simulated and experimental data. These networks achieve greater accuracy than time-averaged MSD analysis on simulated trajectories while only requiring as few as 25 steps. When tested on experimental data, both net and ensemble MSD analysis converge to similar values; however, the net needs only half the number of trajectories required for ensemble MSD to achieve the same confidence interval. Finally, we extract diffusion parameters from multiple extremely short trajectories (10 steps) using our approach.},
	language = {en},
	number = {2},
	urldate = {2019-11-19},
	journal = {Biophysical Journal},
	author = {Granik, Naor and Weiss, Lucien E. and Nehme, Elias and Levin, Maayan and Chein, Michael and Perlson, Eran and Roichman, Yael and Shechtman, Yoav},
	month = jul,
	year = {2019},
	pages = {185--192},
}

@article{eid_real-time_2009,
	title = {Real-{Time} {DNA} {Sequencing} from {Single} {Polymerase} {Molecules}},
	volume = {323},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science-sciencemag-org.ep.fjernadgang.kb.dk/content/323/5910/133},
	doi = {10.1126/science.1162986},
	abstract = {We present single-molecule, real-time sequencing data obtained from a DNA polymerase performing uninterrupted template-directed synthesis using four distinguishable fluorescently labeled deoxyribonucleoside triphosphates (dNTPs). We detected the temporal order of their enzymatic incorporation into a growing DNA strand with zero-mode waveguide nanostructure arrays, which provide optical observation volume confinement and enable parallel, simultaneous detection of thousands of single-molecule sequencing reactions. Conjugation of fluorophores to the terminal phosphate moiety of the dNTPs allows continuous observation of DNA synthesis over thousands of bases without steric hindrance. The data report directly on polymerase dynamics, revealing distinct polymerization states and pause sites corresponding to DNA secondary structure. Sequence data were aligned with the known reference sequence to assay biophysical parameters of polymerization for each template position. Consensus sequences were generated from the single-molecule reads at 15-fold coverage, showing a median accuracy of 99.3\%, with no systematic error beyond fluorophore-dependent error rates.
Arrays of narrow waveguides can record the action of a DNA polymerase stepping along a primer template, potentially providing a way to sequence DNA molecules.
Arrays of narrow waveguides can record the action of a DNA polymerase stepping along a primer template, potentially providing a way to sequence DNA molecules.},
	language = {en},
	number = {5910},
	urldate = {2019-11-14},
	journal = {Science},
	author = {Eid, John and Fehr, Adrian and Gray, Jeremy and Luong, Khai and Lyle, John and Otto, Geoff and Peluso, Paul and Rank, David and Baybayan, Primo and Bettman, Brad and Bibillo, Arkadiusz and Bjornson, Keith and Chaudhuri, Bidhan and Christians, Frederick and Cicero, Ronald and Clark, Sonya and Dalal, Ravindra and deWinter, Alex and Dixon, John and Foquet, Mathieu and Gaertner, Alfred and Hardenbol, Paul and Heiner, Cheryl and Hester, Kevin and Holden, David and Kearns, Gregory and Kong, Xiangxu and Kuse, Ronald and Lacroix, Yves and Lin, Steven and Lundquist, Paul and Ma, Congcong and Marks, Patrick and Maxham, Mark and Murphy, Devon and Park, Insil and Pham, Thang and Phillips, Michael and Roy, Joy and Sebra, Robert and Shen, Gene and Sorenson, Jon and Tomaney, Austin and Travers, Kevin and Trulson, Mark and Vieceli, John and Wegener, Jeffrey and Wu, Dawn and Yang, Alicia and Zaccarin, Denis and Zhao, Peter and Zhong, Frank and Korlach, Jonas and Turner, Stephen},
	month = jan,
	year = {2009},
	pmid = {19023044},
	pages = {133--138},
}

@incollection{korlach_chapter_2010,
	series = {Single {Molecule} {Tools}: {Fluorescence} {Based} {Approaches}, {Part} {A}},
	title = {Chapter 20 - {Real}-{Time} {DNA} {Sequencing} from {Single} {Polymerase} {Molecules}},
	volume = {472},
	url = {http://www.sciencedirect.com/science/article/pii/S0076687910720012},
	abstract = {Pacific Biosciences has developed a method for real-time sequencing of single DNA molecules (Eid et al., 2009), with intrinsic sequencing rates of several bases per second and read lengths into the kilobase range. Conceptually, this sequencing approach is based on eavesdropping on the activity of DNA polymerase carrying out template-directed DNA polymerization. Performed in a highly parallel operational mode, sequential base additions catalyzed by each polymerase are detected with terminal phosphate-linked, fluorescence-labeled nucleotides. This chapter will first outline the principle of this single-molecule, real-time (SMRT™) DNA sequencing method, followed by descriptions of its underlying components and typical sequencing run conditions. Two examples are provided which illustrate that, in addition to the DNA sequence, the dynamics of DNA polymerization from each enzyme molecules is directly accessible: the determination of base-specific kinetic parameters from single-molecule sequencing reads, and the characterization of DNA synthesis rate heterogeneities.},
	language = {en},
	urldate = {2019-11-14},
	booktitle = {Methods in {Enzymology}},
	publisher = {Academic Press},
	author = {Korlach, Jonas and Bjornson, Keith P. and Chaudhuri, Bidhan P. and Cicero, Ronald L. and Flusberg, Benjamin A. and Gray, Jeremy J. and Holden, David and Saxena, Ravi and Wegener, Jeffrey and Turner, Stephen W.},
	editor = {Walter, Nils G.},
	month = jan,
	year = {2010},
	doi = {10.1016/S0076-6879(10)72001-2},
	pages = {431--455},
}

@article{sotoma_selective_2016,
	title = {Selective {Labeling} of {Proteins} on {Living} {Cell} {Membranes} {Using} {Fluorescent} {Nanodiamond} {Probes}},
	volume = {6},
	issn = {2079-4991},
	doi = {10.3390/nano6040056},
	abstract = {The impeccable photostability of fluorescent nanodiamonds (FNDs) is an ideal property for use in fluorescence imaging of proteins in living cells. However, such an application requires highly specific labeling of the target proteins with FNDs. Furthermore, the surface of unmodified FNDs tends to adsorb biomolecules nonspecifically, which hinders the reliable targeting of proteins with FNDs. Here, we combined hyperbranched polyglycerol modification of FNDs with the β-lactamase-tag system to develop a strategy for selective imaging of the protein of interest in cells. The combination of these techniques enabled site-specific labeling of Interleukin-18 receptor alpha chain, a membrane receptor, with FNDs, which eventually enabled tracking of the diffusion trajectory of FND-labeled proteins on the membrane surface.},
	language = {eng},
	number = {4},
	journal = {Nanomaterials (Basel, Switzerland)},
	author = {Sotoma, Shingo and Iimura, Jun and Igarashi, Ryuji and Hirosawa, Koichiro M. and Ohnishi, Hidenori and Mizukami, Shin and Kikuchi, Kazuya and Fujiwara, Takahiro K. and Shirakawa, Masahiro and Tochio, Hidehito},
	month = mar,
	year = {2016},
	pmid = {28335184},
	pmcid = {PMC5302567},
	keywords = {membrane protein, nanodiamond, nitrogen-vacancy center, polyglycerol, β-lactamase tag},
}

@article{claveau_fluorescent_2018,
	title = {Fluorescent {Nanodiamond} {Applications} for {Cellular} {Process} {Sensing} and {Cell} {Tracking}},
	volume = {9},
	issn = {2072-666X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6187705/},
	doi = {10.3390/mi9050247},
	abstract = {Diamond nanocrystals smaller than 100 nm (nanodiamonds) are now recognized to be highly biocompatible. They can be made fluorescent with perfect photostability by creating nitrogen-vacancy (NV) color centers in the diamond lattice. The resulting fluorescent nanodiamonds (FND) have been used since the late 2000s as fluorescent probes for short- or long-term analysis. FND can be used both at the subcellular scale and the single cell scale. Their limited sub-diffraction size allows them to track intracellular processes with high spatio-temporal resolution and high contrast from the surrounding environment. FND can also track the fate of therapeutic compounds or whole cells in the organs of an organism. This review presents examples of FND applications (1) for intra and intercellular molecular processes sensing, also introducing the different potential biosensing applications based on the optically detectable electron spin resonance of NV− centers; and (2) for tracking, firstly, FND themselves to determine their biodistribution, and secondly, using FND as cell tracking probes for diagnosis or follow-up purposes in oncology and regenerative medicine.},
	number = {5},
	urldate = {2019-10-30},
	journal = {Micromachines},
	author = {Claveau, Sandra and Bertrand, Jean-Rémi and Treussart, François},
	month = may,
	year = {2018},
	pmid = {30424180},
	pmcid = {PMC6187705},
}

@misc{noauthor_qdot_nodate,
	title = {Qdot {Nanocrystals}—{Section} 6.6 - {DK}},
	url = {https://www.thermofisher.com/uk/en/home/references/molecular-probes-the-handbook/ultrasensitive-detection-technology/qdot-nanocrystal-technology.html},
	abstract = {Qdot Nanocrystals—Section 6.6},
	language = {en},
	urldate = {2019-10-30},
}

@article{walling_quantum_2009,
	title = {Quantum {Dots} for {Live} {Cell} and {In} {Vivo} {Imaging}},
	volume = {10},
	issn = {1422-0067},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2660663/},
	doi = {10.3390/ijms10020441},
	abstract = {In the past few decades, technology has made immeasurable strides to enable visualization, identification, and quantitation in biological systems. Many of these technological advancements are occurring on the nanometer scale, where multiple scientific disciplines are combining to create new materials with enhanced properties. The integration of inorganic synthetic methods with a size reduction to the nano-scale has lead to the creation of a new class of optical reporters, called quantum dots. These semiconductor quantum dot nanocrystals have emerged as an alternative to organic dyes and fluorescent proteins, and are brighter and more stable against photobleaching than standard fluorescent indicators. Quantum dots have tunable optical properties that have proved useful in a wide range of applications from multiplexed analysis such as DNA detection and cell sorting and tracking, to most recently demonstrating promise for in vivo imaging and diagnostics. This review provides an in-depth discussion of past, present, and future trends in quantum dot use with an emphasis on in vivo imaging and its related applications.},
	number = {2},
	urldate = {2019-10-30},
	journal = {International Journal of Molecular Sciences},
	author = {Walling, Maureen A and Novak, Jennifer A and Shepard, Jason R. E},
	month = feb,
	year = {2009},
	pmid = {19333416},
	pmcid = {PMC2660663},
	pages = {441--491},
}

@article{lundberg_consistent_2019-1,
	title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}},
	url = {http://arxiv.org/abs/1802.03888},
	abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
	urldate = {2019-10-27},
	journal = {arXiv:1802.03888 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
	month = mar,
	year = {2019},
	note = {arXiv: 1802.03888},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{levene_zero-mode_2003,
	title = {Zero-{Mode} {Waveguides} for {Single}-{Molecule} {Analysis} at {High} {Concentrations}},
	volume = {299},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/299/5607/682},
	doi = {10.1126/science.1079700},
	abstract = {Optical approaches for observing the dynamics of single molecules have required pico- to nanomolar concentrations of fluorophore in order to isolate individual molecules. However, many biologically relevant processes occur at micromolar ligand concentrations, necessitating a reduction in the conventional observation volume by three orders of magnitude. We show that arrays of zero-mode waveguides consisting of subwavelength holes in a metal film provide a simple and highly parallel means for studying single-molecule dynamics at micromolar concentrations with microsecond temporal resolution. We present observations of DNA polymerase activity as an example of the effectiveness of zero-mode waveguides for performing single-molecule experiments at high concentrations.
Passing light through subwavelength holes in a metal film allows investigation of single molecules at physiological concentrations.
Passing light through subwavelength holes in a metal film allows investigation of single molecules at physiological concentrations.},
	language = {en},
	number = {5607},
	urldate = {2019-10-25},
	journal = {Science},
	author = {Levene, M. J. and Korlach, J. and Turner, S. W. and Foquet, M. and Craighead, H. G. and Webb, W. W.},
	month = jan,
	year = {2003},
	pmid = {12560545},
	pages = {682--686},
}

@inproceedings{kaufman_trapping_2019,
	address = {San Francisco, United States},
	title = {Trapping and {SERS} identification of extracellular vesicles using nanohole arrays},
	isbn = {978-1-5106-2430-6 978-1-5106-2431-3},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10894/2506633/Trapping-and-SERS-identification-of-extracellular-vesicles-using-nanohole-arrays/10.1117/12.2506633.full},
	doi = {10.1117/12.2506633},
	abstract = {Extracellular vesicles are nanoscale and microscale biological vesicles actively released by nearly all cell types within the body. These small vesicles have been shown to play important biological roles, including cell-to-cell communication, coagulation and signal transduction. They have also been shown to play oncogenic roles in cancer metastasis and progression. Extracellular vesicles are composed of an aqueous cytosolic core and a phospholipid membrane, and exhibit variability in their internal and external cargoes. Developing a better understanding of the structure and diversity of the components of extracellular vesicles may hold promise in uncovering the pathways involved in the formation and progression of various cancers and diseases. Current studies of extracellular vesicles focus on bulk analysis, whereas variability amongst individual extracellular vesicles has been minimally reported in literature. In this study, we propose the use of a surface-enhanced Raman spectroscopy platform in movement towards trapping extracellular vesicles secreted from a mesenchymal stem cell line followed by probing their individual spectral signatures. Here, we propose the use of plasmonic-well based structures as a means of isolating, trapping and controlling the position of biologically relevant vesicles on plasmonic platforms. Trapping and identification of extracellular vesicles occurs without use of labelling agents, allowing for characterization of the intrinsic molecular information of individual extracellular vesicles.},
	language = {en},
	urldate = {2019-10-17},
	booktitle = {Plasmonics in {Biology} and {Medicine} {XVI}},
	publisher = {SPIE},
	author = {Kaufman, Lauren and Cooper, Tyler  T. and Wallace, Gregory Q. and Hawke, David C. and Betts, Dean and Hess, David  A. and Lagugné-Labarthet, François},
	editor = {Vo-Dinh, Tuan and Ho, Ho-Pui A. and Ray, Krishanu},
	month = mar,
	year = {2019},
	pages = {10},
}

@article{stella_conformational_2018,
	title = {Conformational {Activation} {Promotes} {CRISPR}-{Cas12a} {Catalysis} and {Resetting} of the {Endonuclease} {Activity}},
	volume = {175},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S0092867418314016},
	doi = {10.1016/j.cell.2018.10.045},
	abstract = {Summary
Cas12a, also known as Cpf1, is a type V-A CRISPR-Cas RNA-guided endonuclease that is used for genome editing based on its ability to generate specific dsDNA breaks. Here, we show cryo-EM structures of intermediates of the cleavage reaction, thus visualizing three protein regions that sense the crRNA-DNA hybrid assembly triggering the catalytic activation of Cas12a. Single-molecule FRET provides the thermodynamics and kinetics of the conformational activation leading to phosphodiester bond hydrolysis. These findings illustrate why Cas12a cuts its target DNA and unleashes unspecific cleavage activity, degrading ssDNA molecules after activation. In addition, we show that other crRNAs are able to displace the R-loop inside the protein after target DNA cleavage, terminating indiscriminate ssDNA degradation. We propose a model whereby the conformational activation of the enzyme results in indiscriminate ssDNA cleavage. The displacement of the R-loop by a new crRNA molecule will reset Cas12a specificity, targeting new DNAs.},
	number = {7},
	urldate = {2019-10-07},
	journal = {Cell},
	author = {Stella, Stefano and Mesa, Pablo and Thomsen, Johannes and Paul, Bijoya and Alcón, Pablo and Jensen, Simon B. and Saligram, Bhargav and Moses, Matias E. and Hatzakis, Nikos S. and Montoya, Guillermo},
	month = dec,
	year = {2018},
	keywords = {CRISPR-Cas, cryo-EM, genome editing, protein-RNA-DNA complex, single-molecule FRET},
	pages = {1856--1871.e21},
}

@article{berg_ilastik:_2019,
	title = {ilastik: interactive machine learning for (bio)image analysis},
	issn = {1548-7091, 1548-7105},
	shorttitle = {ilastik},
	url = {http://www.nature.com/articles/s41592-019-0582-9},
	doi = {10.1038/s41592-019-0582-9},
	language = {en},
	urldate = {2019-10-01},
	journal = {Nature Methods},
	author = {Berg, Stuart and Kutra, Dominik and Kroeger, Thorben and Straehle, Christoph N. and Kausler, Bernhard X. and Haubold, Carsten and Schiegg, Martin and Ales, Janez and Beier, Thorsten and Rudy, Markus and Eren, Kemal and Cervantes, Jaime I and Xu, Buote and Beuttenmueller, Fynn and Wolny, Adrian and Zhang, Chong and Koethe, Ullrich and Hamprecht, Fred A. and Kreshuk, Anna},
	month = sep,
	year = {2019},
}

@book{dryer_wals_2013,
	address = {Leipzig},
	title = {{WALS} {Online}},
	url = {https://wals.info/},
	urldate = {2019-09-20},
	publisher = {Max Planck Institute for Evolutionary Anthropology},
	editor = {Dryer, Matthew S. and Haspelmath, Martin},
	year = {2013},
}

@article{bjerva_what_2019,
	title = {What do {Language} {Representations} {Really} {Represent}?},
	url = {http://arxiv.org/abs/1901.02646},
	abstract = {A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just like it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, while genetic relationships---a convenient benchmark used for evaluation in previous work---appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.},
	urldate = {2019-09-20},
	journal = {arXiv:1901.02646 [cs]},
	author = {Bjerva, Johannes and Östling, Robert and Veiga, Maria Han and Tiedemann, Jörg and Augenstein, Isabelle},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.02646},
	keywords = {Computer Science - Computation and Language},
}

@article{pinho_glycosylation_2015,
	title = {Glycosylation in cancer: mechanisms and clinical implications},
	volume = {15},
	copyright = {2015 Nature Publishing Group},
	issn = {1474-1768},
	shorttitle = {Glycosylation in cancer},
	url = {https://www.nature.com/articles/nrc3982},
	doi = {10.1038/nrc3982},
	abstract = {Despite recent progress in understanding the cancer genome, there is still a relative delay in understanding the full aspects of the glycome and glycoproteome of cancer. Glycobiology has been instrumental in relevant discoveries in various biological and medical fields, and has contributed to the deciphering of several human diseases. Glycans are involved in fundamental molecular and cell biology processes occurring in cancer, such as cell signalling and communication, tumour cell dissociation and invasion, cell–matrix interactions, tumour angiogenesis, immune modulation and metastasis formation. The roles of glycans in cancer have been highlighted by the fact that alterations in glycosylation regulate the development and progression of cancer, serving as important biomarkers and providing a set of specific targets for therapeutic intervention. This Review discusses the role of glycans in fundamental mechanisms controlling cancer development and progression, and their applications in oncology.},
	language = {en},
	number = {9},
	urldate = {2019-08-13},
	journal = {Nature Reviews Cancer},
	author = {Pinho, Salomé S. and Reis, Celso A.},
	month = sep,
	year = {2015},
	pages = {540--555},
}

@misc{school_principal_2012,
	title = {Principal supervisor},
	url = {https://healthsciences.ku.dk/phd/supervision/information/},
	language = {en},
	urldate = {2019-07-10},
	author = {School, The Graduate},
	month = feb,
	year = {2012},
}

@book{jensen_rcr_2015,
	address = {Copenhagne},
	title = {{RCR} - a {Danish} textbook for courses in {Responsible} {Conduct} of {Research}},
	isbn = {978-87-92591-62-3},
	language = {en},
	publisher = {University of Copenhagen, Department of Food and Resource Economics},
	author = {Jensen, Karsten and Whiteley, Louise and Sandsoe, Peter},
	year = {2015},
	note = {OCLC: 936133149},
}

@article{aragon_theory_1976,
	title = {Theory of dynamic light scattering from polydisperse systems},
	volume = {64},
	issn = {0021-9606},
	url = {https://aip.scitation.org/doi/10.1063/1.432528},
	doi = {10.1063/1.432528},
	number = {6},
	urldate = {2019-07-08},
	journal = {The Journal of Chemical Physics},
	author = {Aragón, S. R. and Pecora, R.},
	month = mar,
	year = {1976},
	pages = {2395--2404},
}

@article{koppel_analysis_1972,
	title = {Analysis of {Macromolecular} {Polydispersity} in {Intensity} {Correlation} {Spectroscopy}: {The} {Method} of {Cumulants}},
	volume = {57},
	issn = {0021-9606},
	shorttitle = {Analysis of {Macromolecular} {Polydispersity} in {Intensity} {Correlation} {Spectroscopy}},
	url = {https://aip.scitation.org/doi/10.1063/1.1678153},
	doi = {10.1063/1.1678153},
	number = {11},
	urldate = {2019-07-08},
	journal = {The Journal of Chemical Physics},
	author = {Koppel, Dennis E.},
	month = dec,
	year = {1972},
	pages = {4814--4820},
}

@article{bargeron_measurement_1974,
	title = {Measurement of a continuous distribution of spherical particles by intensity correlation spectroscopy: {Analysis} by cumulants},
	volume = {61},
	issn = {0021-9606},
	shorttitle = {Measurement of a continuous distribution of spherical particles by intensity correlation spectroscopy},
	url = {https://aip-scitation-org.ep.fjernadgang.kb.dk/doi/abs/10.1063/1.1682225},
	doi = {10.1063/1.1682225},
	number = {5},
	urldate = {2019-07-08},
	journal = {The Journal of Chemical Physics},
	author = {Bargeron, C. B.},
	month = sep,
	year = {1974},
	pages = {2134--2138},
}

@article{perrault_mediating_2009,
	title = {Mediating {Tumor} {Targeting} {Efficiency} of {Nanoparticles} {Through} {Design}},
	volume = {9},
	issn = {1530-6984},
	url = {https://doi.org/10.1021/nl900031y},
	doi = {10.1021/nl900031y},
	abstract = {Here we systematically examined the effect of nanoparticle size (10−100 nm) and surface chemistry (i.e., poly(ethylene glycol)) on passive targeting of tumors in vivo. We found that the physical and chemical properties of the nanoparticles influenced their pharmacokinetic behavior, which ultimately determined their tumor accumulation capacity. Interestingly, the permeation of nanoparticles within the tumor is highly dependent on the overall size of the nanoparticle, where larger nanoparticles appear to stay near the vasculature while smaller nanoparticles rapidly diffuse throughout the tumor matrix. Our results provide design parameters for engineering nanoparticles for optimized tumor targeting of contrast agents and therapeutics.},
	number = {5},
	urldate = {2019-06-21},
	journal = {Nano Letters},
	author = {Perrault, Steven D. and Walkey, Carl and Jennings, Travis and Fischer, Hans C. and Chan, Warren C. W.},
	month = may,
	year = {2009},
	pages = {1909--1915},
}

@article{zhigaltsev_bottom-up_2012,
	title = {Bottom-{Up} {Design} and {Synthesis} of {Limit} {Size} {Lipid} {Nanoparticle} {Systems} with {Aqueous} and {Triglyceride} {Cores} {Using} {Millisecond} {Microfluidic} {Mixing}},
	volume = {28},
	issn = {0743-7463},
	url = {https://doi.org/10.1021/la204833h},
	doi = {10.1021/la204833h},
	abstract = {Limit size systems are defined as the smallest achievable aggregates compatible with the packing of the molecular constituents in a defined and energetically stable structure. Here we report the use of rapid microfluidic mixing for the controlled synthesis of two types of limit size lipid nanoparticle (LNP) systems, having either polar or nonpolar cores. Specifically, limit size LNP consisting of 1-palmitoyl, 2-oleoyl phosphatidylcholine (POPC), cholesterol and the triglyceride triolein were synthesized by mixing a stream of ethanol containing dissolved lipid with an aqueous stream, employing a staggered herringbone micromixer. Millisecond mixing of aqueous and ethanol streams at high flow rate ratios (FRR) was used to rapidly increase the polarity of the medium, driving bottom-up synthesis of limit size LNP systems by spontaneous assembly. For POPC/triolein systems the limit size structures consisted of a hydrophobic core of triolein surrounded by a monolayer of POPC where the diameter could be rationally engineered over the range 20–80 nm by varying the POPC/triolein ratio. In the case of POPC and POPC/cholesterol (55/45; mol/mol) the limit size systems achieved were bilayer vesicles of approximately 20 and 40 nm diameter, respectively. We further show that doxorubicin, a representative weak base drug, can be efficiently loaded and retained in limit size POPC LNP, establishing potential utility as drug delivery systems. To our knowledge this is the first report of stable triglyceride emulsions in the 20–50 nm size range, and the first time vesicular systems in the 20–50 nm size range have been generated by a scalable manufacturing method. These results establish microfluidic mixing as a powerful and general approach to access novel LNP systems, with both polar or nonpolar core structures, in the sub-100 nm size range.},
	number = {7},
	urldate = {2019-06-21},
	journal = {Langmuir},
	author = {Zhigaltsev, Igor V. and Belliveau, Nathan and Hafez, Ismail and Leung, Alex K. K. and Huft, Jens and Hansen, Carl and Cullis, Pieter R.},
	month = feb,
	year = {2012},
	pages = {3633--3640},
}

@article{zhigaltsev_production_2016,
	title = {Production of limit size nanoliposomal systems with potential utility as ultra-small drug delivery agents},
	volume = {26},
	issn = {0898-2104},
	url = {https://doi.org/10.3109/08982104.2015.1025411},
	doi = {10.3109/08982104.2015.1025411},
	abstract = {Previous studies from this group have shown that limit size lipid-based systems – defined as the smallest achievable aggregates compatible with the packing properties of their molecular constituents – can be efficiently produced using rapid microfluidic mixing technique. In this work, it is shown that similar procedures can be employed for the production of homogeneously sized unilamellar vesicular systems of 30–40 nm size range. These vesicles can be remotely loaded with the protonable drug doxorubicin and exhibit adequate drug retention properties in vitro and in vivo. In particular, it is demonstrated that whereas sub-40 nm lipid nanoparticle (LNP) systems consisting entirely of long-chain saturated phosphatidylcholines cannot be produced, the presence of such lipids may have a beneficial effect on the retention properties of limit size systems consisting of mixed lipid components. Specifically, a 33-nm diameter doxorubicin-loaded LNP system composed of 1-palmitoyl-2-oleoyl phosphatidylcholine (POPC), 1,2-dipalmitoyl phosphatidylcholine (DPPC), cholesterol, and PEGylated lipid (DSPE-PEG2000) demonstrated adequate, stable drug retention in the circulation, with a half-life for drug release of ∼12 h. These results indicate that microfluidic mixing is the technique of choice for the production of bilayer LNP systems with sizes less than 50 nm that could lead to development of a novel class of ultra-small drug delivery vehicles.},
	number = {2},
	urldate = {2019-06-21},
	journal = {Journal of Liposome Research},
	author = {Zhigaltsev, Igor V. and Tam, Ying K. and Leung, Alex K. K. and Cullis, Pieter R.},
	month = apr,
	year = {2016},
	pmid = {25856305},
	keywords = {Doxorubicin, herringbone micromixer, limit size nanoparticles, liposome, microfluidic mixing},
	pages = {96--102},
}

@article{carugo_liposome_2016,
	title = {Liposome production by microfluidics: potential and limiting factors},
	volume = {6},
	copyright = {2016 Nature Publishing Group},
	issn = {2045-2322},
	shorttitle = {Liposome production by microfluidics},
	url = {https://www.nature.com/articles/srep25876},
	doi = {10.1038/srep25876},
	abstract = {This paper provides an analysis of microfluidic techniques for the production of nanoscale lipid-based vesicular systems. In particular we focus on the key issues associated with the microfluidic production of liposomes. These include, but are not limited to, the role of lipid formulation, lipid concentration, residual amount of solvent, production method (including microchannel architecture), and drug loading in determining liposome characteristics. Furthermore, we propose microfluidic architectures for the mass production of liposomes with a view to potential industrial translation of this technology.},
	language = {en},
	urldate = {2019-06-21},
	journal = {Scientific Reports},
	author = {Carugo, Dario and Bottaro, Elisabetta and Owen, Joshua and Stride, Eleanor and Nastruzzi, Claudio},
	month = may,
	year = {2016},
	pages = {25876},
}

@article{kastner_microfluidic-controlled_2015,
	title = {Microfluidic-controlled manufacture of liposomes for the solubilisation of a poorly water soluble drug},
	volume = {485},
	issn = {0378-5173},
	url = {http://www.sciencedirect.com/science/article/pii/S0378517315001817},
	doi = {10.1016/j.ijpharm.2015.02.063},
	abstract = {Besides their well-described use as delivery systems for water-soluble drugs, liposomes have the ability to act as a solubilizing agent for drugs with low aqueous solubility. However, a key limitation in exploiting liposome technology is the availability of scalable, low-cost production methods for the preparation of liposomes. Here we describe a new method, using microfluidics, to prepare liposomal solubilising systems which can incorporate low solubility drugs (in this case propofol). The setup, based on a chaotic advection micromixer, showed high drug loading (41mol\%) of propofol as well as the ability to manufacture vesicles with at prescribed sizes (between 50 and 450nm) in a high-throughput setting. Our results demonstrate the ability of merging liposome manufacturing and drug encapsulation in a single process step, leading to an overall reduced process time. These studies emphasise the flexibility and ease of applying lab-on-a-chip microfluidics for the solubilisation of poorly water-soluble drugs.},
	number = {1},
	urldate = {2019-06-21},
	journal = {International Journal of Pharmaceutics},
	author = {Kastner, Elisabeth and Verma, Varun and Lowry, Deborah and Perrie, Yvonne},
	month = may,
	year = {2015},
	keywords = {Bilayer loading, High throughput, Liposomes, Microfluidics, Poorly soluble drugs},
	pages = {122--130},
}

@article{koppel_analysis_1972-1,
	title = {Analysis of {Macromolecular} {Polydispersity} in {Intensity} {Correlation} {Spectroscopy}: {The} {Method} of {Cumulants}},
	volume = {57},
	issn = {0021-9606, 1089-7690},
	shorttitle = {Analysis of {Macromolecular} {Polydispersity} in {Intensity} {Correlation} {Spectroscopy}},
	url = {http://aip.scitation.org/doi/10.1063/1.1678153},
	doi = {10.1063/1.1678153},
	language = {en},
	number = {11},
	urldate = {2019-06-21},
	journal = {The Journal of Chemical Physics},
	author = {Koppel, Dennis E.},
	month = dec,
	year = {1972},
	pages = {4814--4820},
}

@misc{noauthor_iso_nodate,
	title = {{ISO} 13321:1996(en), {Particle} size analysis — {Photon} correlation spectroscopy},
	url = {https://www.iso.org/obp/ui/#iso:std:iso:13321:ed-1:v1:en:sec:2.2},
	urldate = {2019-06-21},
}

@misc{noauthor_iso_nodate-1,
	title = {{ISO} 22412:2008(en), {Particle} size analysis — {Dynamic} light scattering ({DLS})},
	url = {https://www.iso.org/obp/ui/#iso:std:iso:22412:ed-1:v1:en},
	urldate = {2019-06-21},
}

@article{dong_microfluidic_2019,
	title = {Microfluidic preparation of drug-loaded {PEGylated} liposomes, and the impact of liposome size on tumour retention and penetration},
	volume = {29},
	issn = {0898-2104},
	url = {https://doi.org/10.1080/08982104.2017.1391285},
	doi = {10.1080/08982104.2017.1391285},
	abstract = {Understanding the effect of liposome size on tendency for accumulation in tumour tissue requires preparation of defined populations of different sized particles. However, controlling the size distributions without changing the lipid composition is difficult, and differences in compositions itself modify distribution behaviour. Here, a commercial microfluidic format as well as traditional methods was used to prepare doxorubicin-loaded liposomes of different size distributions but with the same lipid composition, and drug retention, biodistribution and localization in tumour tissues were evaluated. The small (∼50 nm diameter) liposomes prepared by microfluidics and large (∼75 nm diameter) liposomes displayed similar drug retention in in vitro release studies, and similar biodistribution patterns in tumour-bearing mice. However, the extent of extravasation was clearly dependent on size of the liposomes, with the small liposomes showing tissue distribution beyond the vascular area compared to the large liposomes. The use of microfluidics to prepare smaller size distribution liposomes compared to sonication methods is demonstrated, and allowed preparation of different size distribution drug carriers from the same lipid composition to enable new understanding of tissue distribution in compositionally consistent materials is demonstrated.},
	number = {1},
	urldate = {2019-06-20},
	journal = {Journal of Liposome Research},
	author = {Dong, Yao-Da and Tchung, Estefania and Nowell, Cameron and Kaga, Sadik and Leong, Nathania and Mehta, Dharmini and Kaminskas, Lisa M. and Boyd, Ben J.},
	month = jan,
	year = {2019},
	pmid = {29020849},
	keywords = {Liposome, biodistribution, microfluidics, size distribution, tumour penetration},
	pages = {1--9},
}

@article{weiss_sequential_2018,
	title = {Sequential bottom-up assembly of mechanically stabilized synthetic cells by microfluidics},
	volume = {17},
	issn = {1476-4660},
	url = {http://www.nature.com/articles/nmat5005},
	doi = {10.1038/nmat5005},
	abstract = {Compartments for the spatially and temporally controlled assembly of biological processes are essential towards cellular life. Synthetic mimics of cellular compartments based on lipid-based protocells lack the mechanical and chemical stability to allow their manipulation into a complex and fully functional synthetic cell. Here, we present a high-throughput microfluidic method to generate stable, defined sized liposomes termed ‘droplet-stabilized giant unilamellar vesicles (dsGUVs)’. The enhanced stability of dsGUVs enables the sequential loading of these compartments with biomolecules, namely purified transmembrane and cytoskeleton proteins by microfluidic pico-injection technology. This constitutes an experimental demonstration of a successful bottom-up assembly of a compartment with contents that would not self-assemble to full functionality when simply mixed together. Following assembly, the stabilizing oil phase and droplet shells are removed to release functional self-supporting protocells to an aqueous phase, enabling them to interact with physiologically relevant matrices.},
	language = {en},
	number = {1},
	urldate = {2019-06-20},
	journal = {Nature Materials},
	author = {Weiss, Marian and Frohnmayer, Johannes Patrick and Benk, Lucia Theresa and Haller, Barbara and Janiesch, Jan-Willi and Heitkamp, Thomas and Börsch, Michael and Lira, Rafael B. and Dimova, Rumiana and Lipowsky, Reinhard and Bodenschatz, Eberhard and Baret, Jean-Christophe and Vidakovic-Koch, Tanja and Sundmacher, Kai and Platzman, Ilia and Spatz, Joachim P.},
	month = jan,
	year = {2018},
	pages = {89--96},
}

@article{sedighi_rapid_2019,
	title = {Rapid optimization of liposome characteristics using a combined microfluidics and design-of-experiment approach},
	volume = {9},
	issn = {2190-3948},
	url = {https://doi.org/10.1007/s13346-018-0587-4},
	doi = {10.1007/s13346-018-0587-4},
	abstract = {Liposomes have attracted much attention as the first nanoformulations entering the clinic. The optimization of physicochemical properties of liposomes during nanomedicine development however is time-consuming and challenging despite great advances in formulation development. Here, we present a systematic approach for the rapid size optimization of liposomes. The combination of microfluidics with a design-of-experiment (DoE) approach offers a strategy to rapidly screen and optimize various liposome formulations, i.e., up to 30 liposome formulations in 1 day. Five representative liposome formulations based on clinically approved lipid compositions were formulated using systematic variations in microfluidics flow rate settings, i.e., flow rate ratio (FRR) and total flow rate (TFR). Interestingly, flow rate-dependent DoE models for the prediction of liposome characteristics could be grouped according to lipid-phase transition temperature and surface characteristics. For all formulations, the FRR had a significant impact (p {\textless} 0.001) on hydrodynamic diameter and size distribution of liposomes, while the TFR mainly affected the production rate. Liposome characteristics remained constant for TFRs above 8 mL/min. The stability study revealed an influence of lipid:cholesterol ratio (1:1 and 2:1 ratio) and presence of PEG on liposome characteristics during storage. To validate our DoE models, we formulated liposomes incorporating hydrophobic dodecanethiol-coated gold nanoparticles. This proof-of-concept step showed that flow rate settings predicted by DoE models successfully determined the size of resulting empty liposomes (109.3 ± 15.3 nm) or nanocomposites (111 ± 17.3 nm). This study indicates that a microfluidics-based formulation approach combined with DoE is suitable for the routine development of monodisperse and size-specific liposomes in a reproducible and rapid manner.},
	language = {en},
	number = {1},
	urldate = {2019-06-20},
	journal = {Drug Delivery and Translational Research},
	author = {Sedighi, Mahsa and Sieber, Sandro and Rahimi, Fereshteh and Shahbazi, Mohammad-Ali and Rezayan, Ali Hossein and Huwyler, Jörg and Witzigmann, Dominik},
	month = feb,
	year = {2019},
	keywords = {Design-of-experiment, Liposomes, Microfluidics, Nanomedicines, Physicochemical characteristics},
	pages = {404--413},
}

@misc{noauthor_glycan_nodate,
	title = {Glycan {Sequencing} {Using} {Exoglycosidases}},
	url = {https://www.sigmaaldrich.com/technical-documents/articles/biology/glycobiology/glycan-sequencing-using-exoglycosidases.html},
	abstract = {Glycan Sequencing Using Exoglycosidases},
	language = {en},
	urldate = {2019-06-20},
	journal = {Sigma-Aldrich},
}

@article{temporini_pronase-immobilized_2007,
	title = {Pronase-{Immobilized} {Enzyme} {Reactor}: an {Approach} for {Automation} in {Glycoprotein} {Analysis} by {LC}/{LC}−{ESI}/{MSn}},
	volume = {79},
	issn = {0003-2700},
	shorttitle = {Pronase-{Immobilized} {Enzyme} {Reactor}},
	url = {https://doi.org/10.1021/ac0611519},
	doi = {10.1021/ac0611519},
	abstract = {An automated analytical approach is proposed for simultaneous characterization of glycan and peptide moieties in pronase-generated glycopeptides. The proposed method is based on the use of a new pronase-immobilized enzyme reactor for the on-line rapid digestion of the target glycoprotein. By coupling the bioreactor to a Hypercarb chromatographic trap column, on-line selective glycopeptide enrichment prior to normal-phase liquid chromatography−mass spectrometry was obtained. A detailed study was carried out for integration and automation of each phase of the proposed analytical procedure. On-line digestion allowed extensive cleavage of the model protein (ribonuclease B), yielding to glycopeptides with peptide moieties up to eight amino acids, carrying the Man5−Man9 N-glycans each, selectively resolved on an Amide-80 column. The use of a linear ion trap instrument resulted in efficient ion capture and led to MS3 acquisition times and spectra quality similar to those for MS2, allowing the unambiguous identification of glycan (MS2) and peptide (MS3) sequences. The proposed procedure reduces the glycoprotein analysis time from ∼3 days, as in most of the traditional off-line methods, to ∼1 h.},
	number = {1},
	urldate = {2019-06-20},
	journal = {Analytical Chemistry},
	author = {Temporini, Caterina and Perani, Eleonora and Calleri, Enrica and Dolcini, Lorenzo and Lubda, Dieter and Caccialanza, Gabriele and Massolini, Gabriella},
	month = jan,
	year = {2007},
	pages = {355--363},
}

@article{shriver_sequencing_2000,
	title = {Sequencing of 3-{O} sulfate containing heparin decasaccharides with a partial antithrombin {III} binding site},
	volume = {97},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.97.19.10359},
	doi = {10.1073/pnas.97.19.10359},
	language = {en},
	number = {19},
	urldate = {2019-06-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Shriver, Z. and Raman, R. and Venkataraman, G. and Drummond, K. and Turnbull, J. and Toida, T. and Linhardt, R. and Biemann, K. and Sasisekharan, R.},
	month = sep,
	year = {2000},
	pages = {10359--10364},
}

@article{turnbull_strategy_1999,
	title = {A strategy for rapid sequencing of heparan sulfate and heparin saccharides},
	volume = {96},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.96.6.2698},
	doi = {10.1073/pnas.96.6.2698},
	abstract = {Sulfated glycosaminoglycans (GAGs) are linear polysaccharides of repeating disaccharide sequences on which are superimposed highly complex and variable patterns of sulfation, especially in heparan sulfate (HS). HS and the structurally related heparin exert important biological functions, primarily by interacting with proteins and regulating their activities. Evidence is accumulating that these interactions depend on specific saccharide sequences, but the lack of simple, direct techniques for sequencing GAG saccharides has been a major obstacle to progress. We describe how HS and heparin saccharides can be sequenced rapidly by using an integrated strategy with chemical and enzymic steps. Attachment of a reducing-end fluorescent tag establishes a reading frame. Partial selective chemical cleavage at internal N-sulfoglucosamine residues with nitrous acid then creates a set of fragments of defined sizes. Subsequent digestion of these fragments with combinations of exosulfatases and exoglycosidases permits the selective removal of specific sulfates and monosaccharides from their nonreducing ends. PAGE of the products yields a pattern of fluorescent bands from which the saccharide sequence can be read directly. Data are presented on sequencing of heparin tetrasaccharides and hexasaccharides of known structure; these data show the accuracy and versatility of this sequencing strategy. Data also are presented on the application of the strategy to the sequencing of an HS decasaccharide of unknown structure. Application and further development of this sequencing strategy, called integral glycan sequencing, will accelerate progress in defining the structure–activity relationships of these complex GAGs and lead to important insights into their biological functions.},
	language = {en},
	number = {6},
	urldate = {2019-06-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Turnbull, J. E. and Hopwood, J. J. and Gallagher, J. T.},
	month = mar,
	year = {1999},
	pages = {2698--2703},
}

@article{guttman_multistructure_1997,
	title = {Multistructure sequencing of {N}-linked fetuin glycans by capillary gel electrophoresis and enzyme matrix digestion},
	volume = {18},
	copyright = {Copyright © 1997 VCH Verlagsgesellschaft mbH},
	issn = {1522-2683},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/elps.1150180719},
	doi = {10.1002/elps.1150180719},
	abstract = {Oligosaccharide sequencing by specific enzymatic digestion of N-linked fetuin glycans using a carefully designed exoglycosidase matrix in conjunction with separation of the combined digests by high performance capillary gel electrophoresis is described. Due to the extremely high separation power and excellent reproducibility of capillary gel electrophoresis, in most instances no isolation of the individual oligosaccharides is necessary, enabling multistructure sequencing from a released glycan pool. By comparing the positions of the separated exoglycosidase digest fragments to maltooligosaccharides of known size, relative migration times and migration shifts are easily calculated. Hence, the particular sequence of each oligosaccharide in a released glycan pool can be proposed with high confidence. Additionally, with the use of high sensitivity laser-induced fluorescence detection, complete sequence information can be attained from picomolar amounts of purified glycoproteins.},
	language = {en},
	number = {7},
	urldate = {2019-06-20},
	journal = {ELECTROPHORESIS},
	author = {Guttman, András},
	year = {1997},
	keywords = {Capillary gel electrophoresis, Carbohydrate sequencing, Fetuin, Laser-induced fluorescence detection, Sialylated oligosaccharides},
	pages = {1136--1141},
}

@article{paice_another_1990,
	title = {Another {Stemmer}},
	volume = {24},
	issn = {0163-5840},
	url = {http://doi.acm.org/10.1145/101306.101310},
	doi = {10.1145/101306.101310},
	abstract = {In natural language processing, conflation is the process of merging or lumping together nonidentical words which refer to the same principal concept. This can relate both to words which are entirely different in form (e.g., "group" and "collection"), and to words which share some common root (e.g., "group", "grouping", "subgroups"). In the former case the words can only be mapped by referring to a dictionary or thesaurus, but in the latter case use can be made of the orthographic similarities between the forms. One popular approach is to remove affixes from the input words, thus reducing them to a stem; if this could be done correctly, all the variant forms of a word would be converted to the same standard form. Since the process is aimed at mapping for retrieval purposes, the stem need not be a linguistically correct lemma or root (see also Frakes 1982).},
	number = {3},
	urldate = {2019-06-14},
	journal = {SIGIR Forum},
	author = {Paice, Chris D.},
	month = nov,
	year = {1990},
	pages = {56--61},
}

@article{porter_algorithm_2006,
	title = {An algorithm for suffix stripping},
	volume = {40},
	issn = {0033-0337, 0033-0337},
	url = {https://www.emeraldinsight.com/doi/10.1108/00330330610681286},
	doi = {10.1108/00330330610681286},
	abstract = {Purpose – The automatic removal of sufﬁxes from words in English is of particular interest in the ﬁeld of information retrieval. This work was originally published in Program in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal.},
	language = {en},
	number = {3},
	urldate = {2019-06-14},
	journal = {Program},
	author = {Porter, M.F.},
	month = jul,
	year = {2006},
	pages = {211--218},
}

@article{knuth_two_1992,
	title = {Two notes on notation},
	url = {http://arxiv.org/abs/math/9205211},
	abstract = {The author advocates two specific mathematical notations from his popular course and joint textbook, "Concrete Mathematics". The first of these, extending an idea of Iverson, is the notation "[P]" for the function which is 1 when the Boolean condition P is true and 0 otherwise. This notation can encourage and clarify the use of characteristic functions and Kronecker deltas in sums and integrals. The second notation puts Stirling numbers on the same footing as binomial coefficients. Since binomial coefficients are written on two lines in parentheses and read "n choose k", Stirling numbers of the first kind should be written on two lines in brackets and read "n cycle k", while Stirling numbers of the second kind should be written in braces and read "n subset k". (I might say "n partition k".) The written form was first suggested by Imanuel Marx. The virtues of this notation are that Stirling partition numbers frequently appear in combinatorics, and that it more clearly presents functional relations similar to those satisfied by binomial coefficients.},
	language = {en},
	urldate = {2019-06-13},
	journal = {arXiv:math/9205211},
	author = {Knuth, Donald E.},
	month = apr,
	year = {1992},
	note = {arXiv: math/9205211},
	keywords = {Mathematics - History and Overview},
}

@article{liang_understanding_2018,
	title = {Understanding the {Loss} {Surface} of {Neural} {Networks} for {Binary} {Classification}},
	url = {http://arxiv.org/abs/1803.00909},
	abstract = {It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see [1, 2, 3]. Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classiﬁcation, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.},
	language = {en},
	urldate = {2019-06-12},
	journal = {arXiv:1803.00909 [cs, stat]},
	author = {Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, R.},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.00909},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_keras-applications/resnet50.py_nodate,
	title = {keras-applications/resnet50.py at master · keras-team/keras-applications},
	url = {https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py},
	urldate = {2019-06-12},
}

@article{lea_temporal_2016,
	title = {Temporal {Convolutional} {Networks}: {A} {Unified} {Approach} to {Action} {Segmentation}},
	shorttitle = {Temporal {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.08242},
	abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: ﬁrst, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classiﬁer that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a uniﬁed approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
	language = {en},
	urldate = {2019-06-12},
	journal = {arXiv:1608.08242 [cs]},
	author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kalchbrenner_grid_2015,
	title = {Grid {Long} {Short}-{Term} {Memory}},
	url = {http://arxiv.org/abs/1507.01526},
	abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a uniﬁed way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to signiﬁcantly outperform the standard LSTM. We then give results for two empirical tasks. We ﬁnd that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to deﬁne a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.},
	language = {en},
	urldate = {2019-06-12},
	journal = {arXiv:1507.01526 [cs]},
	author = {Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.01526},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	url = {https://www.bioinf.jku.at/publications/older/2604.pdf},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2019-06-12},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
}

@article{long_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [5] to the segmentation task. We then deﬁne a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
}

@article{petersen_[_nodate,
	title = {[ http://matrixcookbook.com ]},
	language = {en},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	pages = {72},
}

@article{jha_kinetic_2012,
	title = {Kinetic {Monte} {Carlo} {Simulations} of {Flow}-{Assisted} {Polymerization}},
	volume = {1},
	issn = {2161-1653, 2161-1653},
	url = {http://pubs.acs.org/doi/10.1021/mz300601b},
	doi = {10.1021/mz300601b},
	abstract = {We performed kinetic Monte Carlo simulations on a model of a polymerization process in the presence of a periodic oscillatory ﬂow to explore the role of mixing in polymerization reactors. Application of an oscillatory ﬂow ﬁeld helps overcome the diﬀusive limitations that develop during a polymerization process due to an increase in the molecular weights of polymer chains, thereby giving rise to high rates of polymerization. A systematic increase in the ﬂow strength results in a “dynamic” coil−stretch transition, leading to an elongation of polymer chains. Reactive ends of stretched (polymer) chains react more frequently than the reactive ends of coiled chains, which are screened by other monomers of the same chain. There exists a critical ﬂow strength for the eﬃciency of polymerization processes. The kinetic Monte Carlo simulation scheme developed here exhibit great promise for the study of dynamic properties of polymer systems.},
	language = {en},
	number = {12},
	urldate = {2019-05-28},
	journal = {ACS Macro Letters},
	author = {Jha, Prateek K. and Kuzovkov, Vladimir and Olvera de la Cruz, Monica},
	month = dec,
	year = {2012},
	pages = {1393--1397},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	urldate = {2019-05-28},
	journal = {Towards Data Science},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
}

@article{long_fully_nodate-1,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [5] to the segmentation task. We then deﬁne a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
	language = {en},
	urldate = {2019-05-27},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{bai_empirical_2018-1,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
	language = {en},
	urldate = {2019-05-27},
	journal = {arXiv:1803.01271 [cs]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01271},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{elbayad_pervasive_2018,
	title = {Pervasive {Attention}: {2D} {Convolutional} {Neural} {Networks} for {Sequence}-to-{Sequence} {Prediction}},
	shorttitle = {Pervasive {Attention}},
	url = {http://arxiv.org/abs/1808.03867},
	abstract = {Current state-of-the-art machine translation systems are based on encoder-decoder architectures, that ﬁrst encode the input sequence, and then generate an output sequence based on the input encoding. Both are interfaced with an attention mechanism that recombines a ﬁxed encoding of the source tokens based on the decoder state. We propose an alternative approach which instead relies on a single 2D convolutional neural network across both sequences. Each layer of our network re-codes source tokens on the basis of the output sequence produced so far. Attention-like properties are therefore pervasive throughout the network. Our model yields results that are competitive with state-of-the-art encoder-decoder systems, while being conceptually simpler and having fewer parameters.},
	language = {en},
	urldate = {2019-05-27},
	journal = {arXiv:1808.03867 [cs]},
	author = {Elbayad, Maha and Besacier, Laurent and Verbeek, Jakob},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.03867},
	keywords = {Computer Science - Computation and Language},
}

@article{basioti_optimizing_2019,
	title = {Optimizing {Shallow} {Networks} for {Binary} {Classification}},
	url = {http://arxiv.org/abs/1905.10161},
	abstract = {Data driven classiﬁcation that relies on neural networks is based on optimization criteria that involve some form of distance between the output of the network and the desired label. Using the same mathematical mathematical analysis, for a multitude of such measures, we can show that their optimum solution matches the ideal likelihood ratio test classiﬁer. In this work we introduce a different family of optimization problems which is not covered by the existing approaches and, therefore, opens possibilities for new training algorithms for neural network based classiﬁcation. We give examples that lead to algorithms that are simple in implementation, exhibit stable convergence characteristics and are antagonistic to the most popular existing techniques.},
	language = {en},
	urldate = {2019-05-27},
	journal = {arXiv:1905.10161 [cs, stat]},
	author = {Basioti, Kalliopi and Moustakides, George V.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.10161},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chahal_hitchhikers_2018,
	title = {A {Hitchhiker}'s {Guide} {On} {Distributed} {Training} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.11787},
	abstract = {Deep learning has led to tremendous advancements in the ﬁeld of Artiﬁcial Intelligence. One caveat however is the substantial amount of compute needed to train these deep learning models. Training a benchmark dataset like ImageNet on a single machine with a modern GPU can take upto a week, distributing training on multiple machines has been observed to drastically bring this time down. Recent work has brought down ImageNet training time to a time as low as 4 minutes by using a cluster of 2048 GPUs. This paper surveys the various algorithms and techniques used to distribute training and presents the current state of the art for a modern distributed training framework. More speciﬁcally, we explore the synchronous and asynchronous variants of distributed Stochastic Gradient Descent, various All Reduce gradient aggregation strategies and best practices for obtaining higher throughout and lower latency over a cluster such as mixed precision training, large batch training and gradient compression.},
	language = {en},
	urldate = {2019-05-27},
	journal = {arXiv:1810.11787 [cs, stat]},
	author = {Chahal, Karanbir and Grover, Manraj Singh and Dey, Kuntal},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.11787},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2019-05-27},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bjorck_understanding_2018,
	title = {Understanding {Batch} {Normalization}},
	url = {http://arxiv.org/abs/1806.02375},
	abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
	urldate = {2019-05-22},
	journal = {arXiv:1806.02375 [cs, stat]},
	author = {Bjorck, Johan and Gomes, Carla and Selman, Bart and Weinberger, Kilian Q.},
	month = may,
	year = {2018},
	note = {arXiv: 1806.02375},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2019-05-22},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
}

@article{zagoruyko_wide_2016,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
	language = {en},
	urldate = {2019-05-08},
	journal = {arXiv:1605.07146 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{noauthor_cs231n_nodate,
	title = {{CS231n} {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.github.io/neural-networks-3/#sanitycheck},
	urldate = {2019-05-08},
}

@article{karim_lstm_2018,
	title = {{LSTM} {Fully} {Convolutional} {Networks} for {Time} {Series} {Classification}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2017.2779939},
	abstract = {Fully convolutional neural networks (FCNs) have been shown to achieve the state-of-the-art performance on the task of classifying time series sequences. We propose the augmentation of fully convolutional networks with long short term memory recurrent neural network (LSTM RNN) sub-modules for time series classification. Our proposed models significantly enhance the performance of fully convolutional networks with a nominal increase in model size and require minimal preprocessing of the data set. The proposed long short term memory fully convolutional network (LSTM-FCN) achieves the state-of-the-art performance compared with others. We also explore the usage of attention mechanism to improve time series classification with the attention long short term memory fully convolutional network (ALSTM-FCN). The attention mechanism allows one to visualize the decision process of the LSTM cell. Furthermore, we propose refinement as a method to enhance the performance of trained models. An overall analysis of the performance of our model is provided and compared with other techniques.},
	journal = {IEEE Access},
	author = {Karim, F. and Majumdar, S. and Darabi, H. and Chen, S.},
	year = {2018},
	keywords = {ALSTM-FCN, Computer architecture, Convolution, Convolutional neural network, FCNs, Feature extraction, LSTM RNN, LSTM cell, LSTM-FCN, Machine learning, Machine learning algorithms, Recurrent neural networks, Time series analysis, attention long short term memory fully convolutional network, data set preprocessing, decision process, fully convolutional neural networks, learning (artificial intelligence), long short term memory recurrent neural network, long short term memory recurrent neural network sub-modules, pattern classification, recurrent neural nets, time series, time series classification},
	pages = {1662--1669},
}

@inproceedings{salakhutdinov_restricted_2007,
	address = {Corvalis, Oregon},
	title = {Restricted {Boltzmann} machines for collaborative filtering},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273596},
	doi = {10.1145/1273496.1273596},
	abstract = {Most of the existing approaches to collaborative ﬁltering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM’s), can be used to model tabular data, such as user’s ratings of movies. We present eﬃcient learning and inference procedures for this class of models and demonstrate that RBM’s can be successfully applied to the Netﬂix data set, containing over 100 million user/movie ratings. We also show that RBM’s slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6\% better than the score of Netﬂix’s own system.},
	language = {en},
	urldate = {2019-04-21},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning - {ICML} '07},
	publisher = {ACM Press},
	author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
	year = {2007},
	pages = {791--798},
}

@misc{noauthor_auto-sklearn_nodate,
	title = {auto-sklearn — {AutoSklearn} 0.4.0 documentation},
	url = {https://automl.github.io/auto-sklearn/stable/},
	urldate = {2019-04-05},
}

@article{ghaddar_high_2018,
	title = {High dimensional data classification and feature selection using support vector machines},
	volume = {265},
	issn = {0377-2217},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221717307713},
	doi = {10.1016/j.ejor.2017.08.040},
	abstract = {In many big-data systems, large amounts of information are recorded and stored for analytics purposes. Often however, this vast amount of information does not offer additional benefits for optimal decision making, but may rather be complicating and too costly for collection, storage, and processing. For instance, tumor classification using high-throughput microarray data is challenging due to the presence of a large number of noisy features that do not contribute to the reduction of classification errors. For such problems, the general aim is to find a limited number of genes that highly differentiate among the classes. Thus in this paper, we address a specific class of machine learning, namely the problem of feature selection within support vector machine classification that deals with finding an accurate binary classifier that uses a minimal number of features. We introduce a new approach based on iteratively adjusting a bound on the l1-norm of the classifier vector in order to force the number of selected features to converge towards the desired maximum limit. We analyze two real-life classification problems with high dimensional features. The first case is the medical diagnosis of tumors based on microarray data where we present a generic approach for cancer classification based on gene expression. The second case deals with sentiment classification of on-line reviews from Amazon, Yelp, and IMDb. The results show that the proposed classification and feature selection approach is simple, computationally tractable, and achieves low error rates which are key for the construction of advanced decision-support systems.},
	number = {3},
	urldate = {2019-04-04},
	journal = {European Journal of Operational Research},
	author = {Ghaddar, Bissan and Naoum-Sawaya, Joe},
	month = mar,
	year = {2018},
	keywords = {Analytics, Classification, Feature selection, Machine learning, Support vector machines},
	pages = {993--1004},
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	abstract = {We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {English},
	number = {1},
	journal = {Journal of the Royal Statistical Society Series B-Methodological},
	author = {Tibshirani, R.},
	year = {1996},
	note = {WOS:A1996TU31400017},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288},
}

@article{mckinney_data_2010,
	title = {Data {Structures} for {Statistical} {Computing} in {Python}},
	abstract = {In this paper we are concerned with the practical issues of working with data sets common to ﬁnance, statistics, and other related ﬁelds. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss speciﬁc design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
	language = {en},
	author = {McKinney, Wes},
	year = {2010},
	pages = {6},
}

@misc{noauthor_scrapy_nodate,
	title = {Scrapy 1.6 documentation — {Scrapy} 1.6.0 documentation},
	url = {https://docs.scrapy.org/en/latest/},
	urldate = {2019-04-02},
}

@misc{noauthor_selenium_nodate,
	title = {Selenium {Client} {Driver} — {Selenium} 3.14 documentation},
	url = {https://seleniumhq.github.io/selenium/docs/api/py/index.html},
	urldate = {2019-04-02},
}

@misc{noauthor_karakterfordeling_nodate,
	title = {Karakterfordeling},
	url = {http://karakterstatistik.stads.ku.dk/},
	urldate = {2019-03-27},
}

@article{efron_least_2004,
	title = {Least angle regression},
	volume = {32},
	issn = {0090-5364},
	abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C-p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
	language = {English},
	number = {2},
	journal = {Annals of Statistics},
	author = {Efron, B. and Hastie, T. and Johnstone, I. and Tibshirani, R.},
	month = apr,
	year = {2004},
	note = {WOS:000221411000001},
	keywords = {boosting, coefficient paths, lasso, linear regression, selection, variable   selection},
	pages = {407--451},
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	urldate = {2019-04-02},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	month = oct,
	year = {2011},
	pages = {2825−2830},
}

@misc{paisley_columbiax:_2018,
	address = {Colombia University},
	type = {Lecture},
	title = {{ColumbiaX}: {Machine} {Learning}},
	author = {Paisley, John},
	month = jan,
	year = {2018},
}

@book{sterling_1001_2013,
	address = {Hoboken, New Jersey},
	series = {--{For} dummies},
	title = {1001 algebra {II} practice problems for dummies},
	isbn = {978-1-118-44662-1},
	abstract = {Practice solving the problems that you'll encounter in your Algebra II course. This guide starts with a review of Algebra basics and ends with sequences, sets, and counting techniques. Each practice question includes not only an answer but a step-by-step explanation --},
	publisher = {For Dummies, a Wiley Brand},
	author = {Sterling, Mary Jane},
	year = {2013},
	note = {OCLC: ocn828416495},
	keywords = {Algebra, Problems, exercises, etc},
}

@book{sterling_1001_2013-1,
	address = {Hoboken, New Jersey},
	series = {--{For} dummies},
	title = {1001 algebra {II} practice problems for dummies},
	isbn = {978-1-118-44662-1},
	abstract = {Practice solving the problems that you'll encounter in your Algebra II course. This guide starts with a review of Algebra basics and ends with sequences, sets, and counting techniques. Each practice question includes not only an answer but a step-by-step explanation --},
	publisher = {For Dummies, a Wiley Brand},
	author = {Sterling, Mary Jane},
	year = {2013},
	note = {OCLC: ocn828416495},
	keywords = {Algebra, Problems, exercises, etc},
}

@book{noauthor_notitle_nodate,
}

@book{hastie_elements_2017,
	address = {New York, NY},
	edition = {Second edition, corrected at 12th printing 2017},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	language = {en},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
	year = {2017},
	note = {OCLC: 995842694},
}

@book{bishop_pattern_2009,
	address = {New York, NY},
	edition = {Corrected at 8th printing 2009},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2 978-1-4939-3843-8},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2009},
	note = {OCLC: 845772798},
}

@article{courtney_comments_2008,
	title = {Comments {Regarding} "{On} the {Nature} of {Science}"},
	url = {http://arxiv.org/abs/0812.4932},
	abstract = {An attempt to redefine science in the 21st century (BK Jennings, On the Nature of Science, Physics in Canada, 63(7) 2007) has abandoned traditional notions of natural law and objective reality, blurred the distinctions between natural science and natural history, elevated Occam's razor from an epistemological preference to a scientific principle, and elevated peer-review and experimental care as equals with repeatable experiment as arbiters of scientific validity. Our comments review the long-established axioms of the scientific method, remind readers of the distinctions between science and history, disprove the generality of Occam's razor by counter example, and highlight the risks of accepting additional scientific arbiters as equal to repeatable experiment.},
	urldate = {2019-04-02},
	journal = {arXiv:0812.4932 [physics]},
	author = {Courtney, Amy and Courtney, Michael},
	month = dec,
	year = {2008},
	note = {arXiv: 0812.4932},
	keywords = {Physics - History and Philosophy of Physics},
}

@misc{noauthor_occams_nodate,
	title = {Occam's razor - {Wikiwand}},
	url = {https://www.wikiwand.com/en/Occam%27s_razor},
	urldate = {2019-04-01},
}

@article{lerner_feature_2001,
	title = {Feature representation and signal classification in fluorescence in-situ hybridization image analysis},
	volume = {31},
	issn = {10834427},
	url = {http://ieeexplore.ieee.org/document/983421/},
	doi = {10.1109/3468.983421},
	abstract = {Fast and accurate analysis of fluorescence in-situ hybridization (FISH) images for signal counting will depend mainly upon two components: a classifier to discriminate between artifacts and valid signals of several fluorophores (colors), and well discriminating features to represent the signals. Our previous work has focused on the first component. To investigate the second component, we evaluate candidate feature sets by illustrating the probability density functions (pdfs) and scatter plots for the features. The analysis provides first insight into dependencies between features, indicates the relative importance of members of a feature set, and helps in identifying sources of potential classification errors. Class separability yielded by different feature subsets is evaluated using the accuracy of several neural network (NN)-based classification strategies, some of them hierarchical, as well as using a feature selection technique making use of a scatter criterion. The complete analysis recommends several intensity and hue features for representing FISH signals. Represented by these features, around 90\% of valid signals and artifacts of two fluorophores are correctly classified using the NN. Although applied to cytogenetics, the paper presents a comprehensive, unifying methodology of qualitative and quantitative evaluation of pattern feature representation essential for accurate image classification. This methodology is applicable to many other real-world pattern recognition problems.},
	language = {en},
	number = {6},
	urldate = {2019-03-28},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {Lerner, B. and Clocksin, W.F. and Dhanjal, S. and Hulten, M.A. and Bishop, C.M.},
	month = nov,
	year = {2001},
	pages = {655--665},
}

@article{sergyan_color_2007,
	title = {Color {Content}-based {Image} {Classification}},
	abstract = {In content-based image retrieval systems the most efficient and simple searches are the color-based searches, which can be realized in several color spaces and by several color descriptors. In this paper the possibility of image classification using certain color descriptors is examined, and the usage of different color spaces and descriptors depending on the image database domain is presented.},
	language = {en},
	author = {Sergyán, Szabolcs},
	year = {2007},
	pages = {8},
}

@article{pooley_posterior-based_2019,
	title = {Posterior-based proposals for speeding up {Markov} chain {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1903.10221},
	abstract = {Markov chain Monte Carlo (MCMC) is widely used for Bayesian inference in models of complex systems. Performance, however, is often unsatisfactory in models with many latent variables due to so-called poor mixing, necessitating development of application specific implementations. This limits rigorous use of real-world data to inform development and testing of models in applications ranging from statistical genetics to finance. This paper introduces "posterior-based proposals" (PBPs), a new type of MCMC update applicable to a huge class of statistical models (whose conditional dependence structures are represented by directed acyclic graphs). PBPs generates large joint updates in parameter and latent variable space, whilst retaining good acceptance rates (typically 33 percent). Evaluation against standard approaches (Gibbs or Metropolis-Hastings updates) shows performance improvements by a factor of 2 to over 100 for widely varying model types: an individual-based model for disease diagnostic test data, a financial stochastic volatility model and mixed and generalised linear mixed models used in statistical genetics. PBPs are competitive with similarly targeted state-of-the-art approaches such as Hamiltonian MCMC and particle MCMC, and importantly work under scenarios where these approaches do not. PBPs therefore represent an additional general purpose technique that can be usefully applied in a wide variety of contexts.},
	urldate = {2019-03-28},
	journal = {arXiv:1903.10221 [stat]},
	author = {Pooley, C. M. and Bishop, S. C. and Doeschl-Wilson, A. and Marion, G.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.10221},
	keywords = {Statistics - Applications, Statistics - Computation, Statistics - Methodology},
}

@article{hojjatinia_identification_2019,
	title = {Identification of {Markov} {Jump} {Autoregressive} {Processes} from {Large} {Noisy} {Data} {Sets}},
	url = {http://arxiv.org/abs/1903.11058},
	abstract = {This paper introduces a novel methodology for the identiﬁcation of switching dynamics for switched autoregressive linear models. Switching behavior is assumed to follow a Markov model. The system’s outputs are contaminated by possibly large values of measurement noise. Although the procedure provided can handle other noise distributions, for simplicity, it is assumed that the distribution is Normal with unknown variance. Given noisy input-output data, we aim at identifying switched system coeﬃcients, parameters of the noise distribution, dynamics of switching and probability transition matrix of Markovian model. System dynamics are estimated using previous results which exploit algebraic constraints that system trajectories have to satisfy. Switching dynamics are computed with solving a maximum likelihood estimation problem. The eﬃciency of proposed approach is shown with several academic examples. Although the noise to output ratio can be high, the method is shown to be extremely eﬀective in the situations where a large number of measurements is available.},
	language = {en},
	urldate = {2019-03-28},
	journal = {arXiv:1903.11058 [cs, eess]},
	author = {Hojjatinia, Sarah and Lagoa, Constantino M.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.11058},
	keywords = {Computer Science - Systems and Control, Electrical Engineering and Systems Science - Signal Processing},
}

@article{fryzlewicz_wild_2014,
	title = {Wild binary segmentation for multiple change-point detection},
	volume = {42},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1411.0858},
	doi = {10.1214/14-AOS1245},
	abstract = {We propose a new technique, called wild binary segmentation (WBS), for consistent estimation of the number and locations of multiple change-points in data. We assume that the number of change-points can increase to infinity with the sample size. Due to a certain random localisation mechanism, WBS works even for very short spacings between the change-points and/or very small jump magnitudes, unlike standard binary segmentation. On the other hand, despite its use of localisation, WBS does not require the choice of a window or span parameter, and does not lead to a significant increase in computational complexity. WBS is also easy to code. We propose two stopping criteria for WBS: one based on thresholding and the other based on what we term the `strengthened Schwarz information criterion'. We provide default recommended values of the parameters of the procedure and show that it offers very good practical performance in comparison with the state of the art. The WBS methodology is implemented in the R package wbs, available on CRAN. In addition, we provide a new proof of consistency of binary segmentation with improved rates of convergence, as well as a corresponding result for WBS.},
	language = {en},
	number = {6},
	urldate = {2019-03-28},
	journal = {The Annals of Statistics},
	author = {Fryzlewicz, Piotr},
	month = dec,
	year = {2014},
	note = {arXiv: 1411.0858},
	keywords = {Mathematics - Statistics Theory},
	pages = {2243--2281},
}

@article{jiang_recovery_2019,
	title = {Recovery of a mixture of {Gaussians} by sum-of-norms clustering},
	url = {http://arxiv.org/abs/1902.07137},
	abstract = {Sum-of-norms clustering is a method for assigning n points in Rd to K clusters, 1 ≤ K ≤ n, using convex optimization. Recently, Panahi et al. proved that sum-ofnorms clustering is guaranteed to recover a mixture of Gaussians under the restriction that the number of samples is not too large. The purpose of this note is to lift this restriction, i.e., show that sum-of-norms clustering with equal weights can recover a mixture of Gaussians even as the number of samples tends to inﬁnity. Our proof relies on an interesting characterization of clusters computed by sum-of-norms clustering that was developed inside a proof of the agglomeration conjecture by Chiquet et al. Because we believe this theorem has independent interest, we restate and reprove the Chiquet et al. result herein.},
	language = {en},
	urldate = {2019-03-28},
	journal = {arXiv:1902.07137 [cs, stat]},
	author = {Jiang, Tao and Vavasis, Stephen and Zhai, Chen Wen},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.07137},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{truong_selective_2018,
	title = {Selective review of offline change point detection methods},
	url = {http://arxiv.org/abs/1801.00718},
	abstract = {This article presents a selective survey of algorithms for the oﬄine detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
	language = {en},
	urldate = {2019-03-28},
	journal = {arXiv:1801.00718 [cs, stat]},
	author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.00718},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Statistics - Methodology},
}

@article{zheng_feature_nodate,
	title = {Feature {Engineering} for {Machine} {Learning}},
	language = {en},
	author = {Zheng, Alice and Casari, Amanda},
	pages = {217},
}

@article{caruana_obtaining_nodate,
	title = {Obtaining {Calibrated} {Probabilities} from {Boosting}},
	abstract = {Boosted decision trees typically yield good accuracy, precision, and ROC area. However, because the outputs from boosting are not well calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. We empirically demonstrate why AdaBoost predicts distorted probabilities and examine three calibration methods for correcting this distortion: Platt Scaling, Isotonic Regression, and Logistic Correction. We also experiment with boosting using log-loss instead of the usual exponential loss. Experiments show that Logistic Correction and boosting with log-loss work well when boosting weak models such as decision stumps, but yield poor performance when boosting more complex models such as full decision trees. Platt Scaling and Isotonic Regression, however, signiﬁcantly improve the probabilities predicted by both boosted stumps and boosted trees. After calibration, boosted full decision trees predict better probabilities than other learning methods such as SVMs, neural nets, bagged decision trees, and KNNs, even after these methods are calibrated.},
	language = {en},
	author = {Caruana, Alexandru Niculescu-Mizil Rich},
	pages = {6},
}

@article{tomita_random_2015,
	title = {Random {Projection} {Forests}},
	url = {http://arxiv.org/abs/1506.03410},
	abstract = {Ensemble methods---particularly those based on decision trees---have recently demonstrated superior performance in a variety of machine learning settings. We introduce a generalization of many existing decision tree methods called "Random Projection Forests" (RPF), which is any decision forest that uses (possibly data dependent and random) linear projections. Using this framework, we introduce a special case, called "Lumberjack", using very sparse random projections, that is, linear combinations of a small subset of features. Lumberjack obtains statistically significantly improved accuracy over Random Forests, Gradient Boosted Trees, and other approaches on a standard benchmark suites for classification with varying dimension, sample size, and number of classes. To illustrate how, why, and when Lumberjack outperforms other methods, we conduct extensive simulated experiments, in vectors, images, and nonlinear manifolds. Lumberjack typically yields improved performance over existing decision trees ensembles, while mitigating computational efficiency and scalability, and maintaining interpretability. Lumberjack can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.},
	urldate = {2019-03-21},
	journal = {arXiv:1506.03410 [cs, stat]},
	author = {Tomita, Tyler M. and Browne, James and Shen, Cencheng and Patsolic, Jesse L. and Yim, Jason and Priebe, Carey E. and Burns, Randal and Maggioni, Mauro and Vogelstein, Joshua T.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03410},
	keywords = {68T10, Computer Science - Machine Learning, I.5.2, Statistics - Machine Learning},
}

@article{tomita_random_2015-1,
	title = {Random {Projection} {Forests}},
	url = {http://arxiv.org/abs/1506.03410},
	abstract = {Ensemble methods---particularly those based on decision trees---have recently demonstrated superior performance in a variety of machine learning settings. We introduce a generalization of many existing decision tree methods called "Random Projection Forests" (RPF), which is any decision forest that uses (possibly data dependent and random) linear projections. Using this framework, we introduce a special case, called "Lumberjack", using very sparse random projections, that is, linear combinations of a small subset of features. Lumberjack obtains statistically significantly improved accuracy over Random Forests, Gradient Boosted Trees, and other approaches on a standard benchmark suites for classification with varying dimension, sample size, and number of classes. To illustrate how, why, and when Lumberjack outperforms other methods, we conduct extensive simulated experiments, in vectors, images, and nonlinear manifolds. Lumberjack typically yields improved performance over existing decision trees ensembles, while mitigating computational efficiency and scalability, and maintaining interpretability. Lumberjack can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.},
	urldate = {2019-03-21},
	journal = {arXiv:1506.03410 [cs, stat]},
	author = {Tomita, Tyler M. and Browne, James and Shen, Cencheng and Patsolic, Jesse L. and Yim, Jason and Priebe, Carey E. and Burns, Randal and Maggioni, Mauro and Vogelstein, Joshua T.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.03410},
	keywords = {68T10, Computer Science - Machine Learning, I.5.2, Statistics - Machine Learning},
}

@article{anderson_test_1954,
	title = {A {Test} of {Goodness} of {Fit}},
	volume = {49},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2281537},
	doi = {10.2307/2281537},
	abstract = {[Some (large sample) significance points are tabulated for a distribution-free test of goodness of fit which was introduced earlier by the authors. The test, which uses the actual observations without grouping, is sensitive to discrepancies at the tails of the distribution rather than near the median. An illustration is given, using a numerical example used previously by Birnbaum in illustrating the Kolmogorov test.]},
	number = {268},
	urldate = {2019-03-20},
	journal = {Journal of the American Statistical Association},
	author = {Anderson, T. W. and Darling, D. A.},
	year = {1954},
	pages = {765--769},
}

@article{rahimi_random_nodate,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
	language = {en},
	author = {Rahimi, Ali and Recht, Ben},
	pages = {8},
}

@inproceedings{chen_xgboost:_2016,
	address = {San Francisco, California, USA},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2019-03-20},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} - {KDD} '16},
	publisher = {ACM Press},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	pages = {785--794},
}

@article{fernandez-delgado_we_nodate,
	title = {Do we {Need} {Hundreds} of {Classiﬁers} to {Solve} {Real} {World} {Classiﬁcation} {Problems}?},
	abstract = {We evaluate 179 classiﬁers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classiﬁers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classiﬁers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve signiﬁcant conclusions about the classiﬁer behavior, not dependent on the data set collection. The classiﬁers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1\% of the maximum accuracy overcoming 90\% in the 84.3\% of the data sets. However, the difference is not statistically signiﬁcant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3\% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classiﬁers (3 out of 5 bests classiﬁers are RF), followed by SVM (4 classiﬁers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
	language = {en},
	author = {Fernandez-Delgado, Manuel and Cernadas, Eva and Barro, Senen and Amorim, Dinani},
	pages = {49},
}

@article{feroz_importance_2013,
	title = {Importance {Nested} {Sampling} and the {MultiNest} {Algorithm}},
	url = {http://arxiv.org/abs/1306.2144},
	abstract = {Bayesian inference involves two main computational challenges. First, in estimating the parameters of some model for the data, the posterior distribution may well be highly multi-modal: a regime in which the convergence to stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques becomes incredibly slow. Second, in selecting between a set of competing models the necessary estimation of the Bayesian evidence for each is, by definition, a (possibly high-dimensional) integration over the entire parameter space; again this can be a daunting computational task, although new Monte Carlo (MC) integration algorithms offer solutions of ever increasing efficiency. Nested sampling (NS) is one such contemporary MC strategy targeted at calculation of the Bayesian evidence, but which also enables posterior inference as a by-product, thereby allowing simultaneous parameter estimation and model selection. The widely-used MultiNest algorithm presents a particularly efficient implementation of the NS technique for multi-modal posteriors. In this paper we discuss importance nested sampling (INS), an alternative summation of the MultiNest draws, which can calculate the Bayesian evidence at up to an order of magnitude higher accuracy than `vanilla' NS with no change in the way MultiNest explores the parameter space. This is accomplished by treating as a (pseudo-)importance sample the totality of points collected by MultiNest, including those previously discarded under the constrained likelihood sampling of the NS algorithm. We apply this technique to several challenging test problems and compare the accuracy of Bayesian evidences obtained with INS against those from vanilla NS.},
	urldate = {2019-03-19},
	journal = {arXiv:1306.2144 [astro-ph, physics:physics, stat]},
	author = {Feroz, F. and Hobson, M. P. and Cameron, E. and Pettitt, A. N.},
	month = jun,
	year = {2013},
	note = {arXiv: 1306.2144},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Data Analysis, Statistics and Probability, Statistics - Computation},
}

@article{feroz_multinest:_2009,
	title = {{MultiNest}: an efficient and robust {Bayesian} inference tool for cosmology and particle physics},
	volume = {398},
	issn = {00358711, 13652966},
	shorttitle = {{MultiNest}},
	url = {http://arxiv.org/abs/0809.3437},
	doi = {10.1111/j.1365-2966.2009.14548.x},
	abstract = {We present further development and the first public release of our multimodal nested sampling algorithm, called MultiNest. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz \& Hobson (2008), which itself significantly outperformed existing MCMC techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MultiNest algorithm is demonstrated by application to two toy problems and to a cosmological inference problem focussing on the extension of the vanilla \${\textbackslash}Lambda\$CDM model to include spatial curvature and a varying equation of state for dark energy. The MultiNest software, which is fully parallelized using MPI and includes an interface to CosmoMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SuperBayeS package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org},
	number = {4},
	urldate = {2019-03-19},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Feroz, F. and Hobson, M. P. and Bridges, M.},
	month = oct,
	year = {2009},
	note = {arXiv: 0809.3437},
	keywords = {Astrophysics},
	pages = {1601--1614},
}

@article{cross_review:_2016,
	title = {Review: {Mechanochemistry} of the kinesin‐1 {ATPase}},
	volume = {105},
	issn = {0006-3525},
	shorttitle = {Review},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4924600/},
	doi = {10.1002/bip.22862},
	abstract = {Kinesins are P‐loop NTPases that can do mechanical work. Like small G‐proteins, to which they are related, kinesins execute a program of active site conformational changes that cleaves the terminal phosphate from an NTP substrate. But unlike small G‐proteins, kinesins can amplify and harness these conformational changes in order to exert force. In this short review I summarize current ideas about how the kinesin active site works and outline how the active site chemistry is coupled to the larger‐scale structural cycle of the kinesin motor domain. Focusing largely on kinesin‐1, the best‐studied kinesin, I discuss how the active site switch machinery of kinesin cycles between three distinct states, how docking of the neck linker stabilizes two of these states, and how tension‐sensitive and position‐sensitive neck linker docking may modulate both the hydrolysis step of ATP turnover and the trapping of product ADP in the active site. © 2016 Wiley Periodicals, Inc. Biopolymers 105: 476–482, 2016.},
	number = {8},
	urldate = {2019-01-17},
	journal = {Biopolymers},
	author = {Cross, R. A.},
	month = aug,
	year = {2016},
	pmid = {27120111},
	pmcid = {PMC4924600},
	pages = {476--482},
}

@article{howard_movement_1989,
	title = {Movement of {Microtubules} by {Single} {Kinesin} {Molecules}},
	volume = {342},
	issn = {0028-0836},
	doi = {10.1038/342154a0},
	language = {English},
	number = {6246},
	journal = {Nature},
	author = {Howard, J. and Hudspeth, Aj and Vale, Rd},
	month = nov,
	year = {1989},
	note = {WOS:A1989AY73900058},
	pages = {154--158},
}

@article{huxley_proposed_1971,
	title = {Proposed {Mechanism} of {Force} {Generation} in {Striated} {Muscle}},
	volume = {233},
	issn = {0028-0836},
	doi = {10.1038/233533a0},
	language = {English},
	number = {5321},
	journal = {Nature},
	author = {Huxley, Af and Simmons, Rm},
	year = {1971},
	note = {WOS:A1971K598200022},
	pages = {533--\&},
}

@article{hrkach_preclinical_2012,
	title = {Preclinical development and clinical translation of a {PSMA}-targeted docetaxel nanoparticle with a differentiated pharmacological profile},
	volume = {4},
	issn = {1946-6242},
	doi = {10.1126/scitranslmed.3003651},
	abstract = {We describe the development and clinical translation of a targeted polymeric nanoparticle (TNP) containing the chemotherapeutic docetaxel (DTXL) for the treatment of patients with solid tumors. DTXL-TNP is targeted to prostate-specific membrane antigen, a clinically validated tumor antigen expressed on prostate cancer cells and on the neovasculature of most nonprostate solid tumors. DTXL-TNP was developed from a combinatorial library of more than 100 TNP formulations varying with respect to particle size, targeting ligand density, surface hydrophilicity, drug loading, and drug release properties. Pharmacokinetic and tissue distribution studies in rats showed that the NPs had a blood circulation half-life of about 20 hours and minimal liver accumulation. In tumor-bearing mice, DTXL-TNP exhibited markedly enhanced tumor accumulation at 12 hours and prolonged tumor growth suppression compared to a solvent-based DTXL formulation (sb-DTXL). In tumor-bearing mice, rats, and nonhuman primates, DTXL-TNP displayed pharmacokinetic characteristics consistent with prolonged circulation of NPs in the vascular compartment and controlled release of DTXL, with total DTXL plasma concentrations remaining at least 100-fold higher than sb-DTXL for more than 24 hours. Finally, initial clinical data in patients with advanced solid tumors indicated that DTXL-TNP displays a pharmacological profile differentiated from sb-DTXL, including pharmacokinetics characteristics consistent with preclinical data and cases of tumor shrinkage at doses below the sb-DTXL dose typically used in the clinic.},
	language = {eng},
	number = {128},
	journal = {Science Translational Medicine},
	author = {Hrkach, Jeffrey and Von Hoff, Daniel and Mukkaram Ali, Mir and Andrianova, Elizaveta and Auer, Jason and Campbell, Tarikh and De Witt, David and Figa, Michael and Figueiredo, Maria and Horhota, Allen and Low, Susan and McDonnell, Kevin and Peeke, Erick and Retnarajan, Beadle and Sabnis, Abhimanyu and Schnipper, Edward and Song, Jeffrey J. and Song, Young Ho and Summa, Jason and Tompsett, Douglas and Troiano, Greg and Van Geen Hoven, Tina and Wright, Jim and LoRusso, Patricia and Kantoff, Philip W. and Bander, Neil H. and Sweeney, Christopher and Farokhzad, Omid C. and Langer, Robert and Zale, Stephen},
	month = apr,
	year = {2012},
	pmid = {22491949},
	keywords = {Animals, Antigens, Surface, Cell Line, Tumor, Docetaxel, Glutamate Carboxypeptidase II, Humans, Male, Mice, Nanoparticles, Polymers, Rats, Taxoids, Xenograft Model Antitumor Assays},
	pages = {128ra39},
}

@article{moghimi_complement_2010,
	series = {Nanomedicine and {Drug} {Delivery} ({NanoDDS}'09)},
	title = {Complement activation cascade triggered by {PEG}–{PL} engineered nanomedicines and carbon nanotubes: {The} challenges ahead},
	volume = {146},
	issn = {0168-3659},
	shorttitle = {Complement activation cascade triggered by {PEG}–{PL} engineered nanomedicines and carbon nanotubes},
	url = {http://www.sciencedirect.com/science/article/pii/S0168365910002579},
	doi = {10.1016/j.jconrel.2010.04.003},
	abstract = {Since their introduction, poly(ethylene glycol)–phospholipid (PEG–PL) conjugates have found many applications in design and engineering of nanosized delivery systems for controlled delivery of pharmaceuticals especially to non-macrophage targets. However, there are reports of idiosyncratic reactions to certain PEG–PL engineered nanomedicines in both experimental animals and man. These reactions are classified as pseudoallergy and may be associated with cardiopulmonary disturbance and other related symptoms of anaphylaxis. Recent studies suggest that complement activation may be a contributing, but not a rate limiting factor, in eliciting hypersensitivity reactions to such nanomedicines in sensitive individuals. This is rather surprising since PEGylated structures are generally assumed to suppress protein adsorption and blood opsonization events including complement. Here, we examine the molecular basis of complement activation by PEG–PL engineered nanomedicines and carbon nanotubes and discuss the challenges ahead.},
	number = {2},
	urldate = {2019-01-16},
	journal = {Journal of Controlled Release},
	author = {Moghimi, S. M. and Andersen, A. J. and Hashemi, S. H. and Lettiero, B. and Ahmadvand, D. and Hunter, A. C. and Andresen, T. L. and Hamad, I. and Szebeni, J.},
	month = sep,
	year = {2010},
	keywords = {Carbon nanotube, Complement activation, Liposome, Micelle, Poly(ethylene glycol), Pseudoallergy},
	pages = {175--181},
}

@article{cheng_formulation_2007,
	title = {Formulation of {Functionalized} {PLGA}-{PEG} {Nanoparticles} for {In} {Vivo} {Targeted} {Drug} {Delivery}},
	volume = {28},
	issn = {0142-9612},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2925222/},
	doi = {10.1016/j.biomaterials.2006.09.047},
	abstract = {Nanoparticle (NP) size has been shown to significantly effect the biodistribution of targeted and non-targeted NPs in an organ specific manner. Herein we have developed NPs from carboxy-terminated poly (d,l-lactide-co-glycolide)-block-poly(ethylene glycol) (PLGA-b-PEG-COOH) polymer and studied the effects of altering the following formulation parameters on the size of NPs, including: 1) polymer concentration, 2) drug loading, 3) water miscibility of solvent, and 4) the ratio of water to solvent. We found that NP mean volumetric size correlates linearly with polymer concentration for NPs between 70 and 250 nm in diameter (linear coefficient = 0.99 for NPs formulated with solvents studied). NPs with desirable size, drug loading, and polydispersity were conjugated to the A10 RNA aptamer (Apt) that binds to the Prostate Specific Membrane Antigen (PSMA), and NP and NP-Apt biodistribution was evaluated in a LNCaP (PSMA+) xenograft mouse model of PCa. The surface functionalization of NPs with the A10 PSMA aptamer significantly enhanced delivery of NPs to tumors vs. equivalent NPs lacking the A10 PSMA aptamer (a 3.77-fold increase at 24 hrs; NP-Apt 0.83\% ± 0.21\% vs. NP 0.22\% ± 0.07\% of injected dose per gram of tissue; mean ± s.d., n = 4, p = 0.002). The ability to control NP size together with targeted delivery may result in favorable biodistribution and development of clinically relevant targeted therapies.},
	number = {5},
	urldate = {2019-01-16},
	journal = {Biomaterials},
	author = {Cheng, Jianjun and Teply, Benjamin A. and Sherifi, Ines and Sung, Josephine and Luther, Gaurav and Gu, Frank X. and Levy-Nissenbaum, Etgar and Radovic-Moreno, Aleksandar F. and Langer, Robert and Farokhzad, Omid C.},
	month = feb,
	year = {2007},
	pmid = {17055572},
	pmcid = {PMC2925222},
	pages = {869--876},
}

@article{loos_functionalized_2014,
	title = {Functionalized polystyrene nanoparticles as a platform for studying bio–nano interactions},
	volume = {5},
	issn = {2190-4286},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4311717/},
	doi = {10.3762/bjnano.5.250},
	abstract = {Nanoparticles of various shapes, sizes, and materials carrying different surface modifications have numerous technological and biomedical applications. Yet, the mechanisms by which nanoparticles interact with biological structures as well as their biological impact and hazards remain poorly investigated. Due to their large surface to volume ratio, nanoparticles usually exhibit properties that differ from those of bulk materials. Particularly, the surface chemistry of the nanoparticles is crucial for their durability and solubility in biological media as well as for their biocompatibility and biodistribution. Polystyrene does not degrade in the cellular environment and exhibits no short-term cytotoxicity. Because polystyrene nanoparticles can be easily synthesized in a wide range of sizes with distinct surface functionalizations, they are perfectly suited as model particles to study the effects of the particle surface characteristics on various biological parameters. Therefore, we have exploited polystyrene nanoparticles as a convenient platform to study bio–nano interactions. This review summarizes studies on positively and negatively charged polystyrene nanoparticles and compares them with clinically used superparamagnetic iron oxide nanoparticles.,},
	urldate = {2019-01-16},
	journal = {Beilstein Journal of Nanotechnology},
	author = {Loos, Cornelia and Syrovets, Tatiana and Musyanovych, Anna and Mailänder, Volker and Landfester, Katharina and Nienhaus, G Ulrich and Simmet, Thomas},
	month = dec,
	year = {2014},
	pmid = {25671136},
	pmcid = {PMC4311717},
	pages = {2403--2412},
}

@article{schottler_protein_2016,
	title = {Protein adsorption is required for stealth effect of poly(ethylene glycol)- and poly(phosphoester)-coated nanocarriers},
	volume = {11},
	copyright = {2016 Nature Publishing Group},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/nnano.2015.330},
	doi = {10.1038/nnano.2015.330},
	abstract = {The current gold standard to reduce non-specific cellular uptake of drug delivery vehicles is by covalent attachment of poly(ethylene glycol) (PEG). It is thought that PEG can reduce protein adsorption and thereby confer a stealth effect. Here, we show that polystyrene nanocarriers that have been modified with PEG or poly(ethyl ethylene phosphate) (PEEP) and exposed to plasma proteins exhibit a low cellular uptake, whereas those not exposed to plasma proteins show high non-specific uptake. Mass spectrometric analysis revealed that exposed nanocarriers formed a protein corona that contains an abundance of clusterin proteins (also known as apolipoprotein J). When the polymer-modified nanocarriers were incubated with clusterin, non-specific cellular uptake could be reduced. Our results show that in addition to reducing protein adsorption, PEG, and now PEEPs, can affect the composition of the protein corona that forms around nanocarriers, and the presence of distinct proteins is necessary to prevent non-specific cellular uptake.},
	language = {en},
	number = {4},
	urldate = {2019-01-16},
	journal = {Nature Nanotechnology},
	author = {Schöttler, Susanne and Becker, Greta and Winzen, Svenja and Steinbach, Tobias and Mohr, Kristin and Landfester, Katharina and Mailänder, Volker and Wurm, Frederik R.},
	month = apr,
	year = {2016},
	pages = {372--377},
}

@article{bertrand_mechanistic_2017,
	title = {Mechanistic understanding of in vivo protein corona formation on polymeric nanoparticles and impact on pharmacokinetics},
	volume = {8},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-017-00600-w},
	doi = {10.1038/s41467-017-00600-w},
	language = {en},
	number = {1},
	urldate = {2019-01-16},
	journal = {Nature Communications},
	author = {Bertrand, Nicolas and Grenier, Philippe and Mahmoudi, Morteza and Lima, Eliana M. and Appel, Eric A. and Dormont, Flavio and Lim, Jong-Min and Karnik, Rohit and Langer, Robert and Farokhzad, Omid C.},
	month = dec,
	year = {2017},
}

@article{gildersleeve_improved_2008,
	title = {Improved {Procedure} for {Direct} {Coupling} of {Carbohydrates} to {Proteins} via {Reductive} {Amination}},
	volume = {19},
	issn = {1043-1802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2629553/},
	doi = {10.1021/bc800153t},
	abstract = {Carbohydrate-protein conjugates are utilized extensively in basic research and as immunogens in a variety of bacterial vaccines and cancer vaccines. As a result, there have been significant efforts to develop simple and reliable methods for the construction of these conjugates. While direct coupling via reductive amination is an appealing approach, the reaction is typically very inefficient. In this paper, we report improved reaction conditions providing an approximately 500\% increase in yield. In addition to optimizing a series of standard reaction parameters, we found that addition of 500 mM sodium sulfate improves the coupling efficiency. To illustrate the utility of these conditions, a series of high mannose BSA conjugates were produced and incorporated into a carbohydrate microarray. Ligand binding to ConA could be observed and apparent affinity constants (Kds) measured using the array were in good agreement with values reported by surface plasmon resonance. The results show that the conditions are suitable for microgram scale reactions, are compatible with complex carbohydrates, and produce biologically active conjugates.},
	number = {7},
	urldate = {2019-01-08},
	journal = {Bioconjugate chemistry},
	author = {Gildersleeve, Jeffrey C. and Oyelaran, Oyindasola and Simpson, John T. and Allred, Benjamin},
	month = jul,
	year = {2008},
	pmid = {18597509},
	pmcid = {PMC2629553},
	pages = {1485--1490},
}

@article{levene_zero-mode_2003,
	title = {Zero-{Mode} {Waveguides} for {Single}-{Molecule} {Analysis} at {High} {Concentrations}},
	volume = {299},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/299/5607/682},
	doi = {10.1126/science.1079700},
	abstract = {Optical approaches for observing the dynamics of single molecules have required pico- to nanomolar concentrations of fluorophore in order to isolate individual molecules. However, many biologically relevant processes occur at micromolar ligand concentrations, necessitating a reduction in the conventional observation volume by three orders of magnitude. We show that arrays of zero-mode waveguides consisting of subwavelength holes in a metal film provide a simple and highly parallel means for studying single-molecule dynamics at micromolar concentrations with microsecond temporal resolution. We present observations of DNA polymerase activity as an example of the effectiveness of zero-mode waveguides for performing single-molecule experiments at high concentrations.
Passing light through subwavelength holes in a metal film allows investigation of single molecules at physiological concentrations.
Passing light through subwavelength holes in a metal film allows investigation of single molecules at physiological concentrations.},
	language = {en},
	number = {5607},
	urldate = {2019-01-08},
	journal = {Science},
	author = {Levene, M. J. and Korlach, J. and Turner, S. W. and Foquet, M. and Craighead, H. G. and Webb, W. W.},
	month = jan,
	year = {2003},
	pmid = {12560545},
	pages = {682--686},
}

@article{varki_evolutionary_2011,
	title = {Evolutionary {Forces} {Shaping} the {Golgi} {Glycosylation} {Machinery}: {Why} {Cell} {Surface} {Glycans} {Are} {Universal} to {Living} {Cells}},
	volume = {3},
	issn = {1943-0264},
	shorttitle = {Evolutionary {Forces} {Shaping} the {Golgi} {Glycosylation} {Machinery}},
	url = {http://cshperspectives.cshlp.org/lookup/doi/10.1101/cshperspect.a005462},
	doi = {10.1101/cshperspect.a005462},
	language = {en},
	number = {6},
	urldate = {2019-01-08},
	journal = {Cold Spring Harbor Perspectives in Biology},
	author = {Varki, A.},
	month = jun,
	year = {2011},
	pages = {a005462--a005462},
}

@article{moremen_vertebrate_2012,
	title = {Vertebrate protein glycosylation: diversity, synthesis and function},
	volume = {13},
	copyright = {2012 Nature Publishing Group},
	issn = {1471-0080},
	shorttitle = {Vertebrate protein glycosylation},
	url = {https://www.nature.com/articles/nrm3383},
	doi = {10.1038/nrm3383},
	abstract = {Protein glycosylation is a ubiquitous post-translational modification found in all domains of life. Despite their significant complexity in animal systems, glycan structures have crucial biological and physiological roles, from contributions in protein folding and quality control to involvement in a large number of biological recognition events. As a result, they impart an additional level of 'information content' to underlying polypeptide structures. Improvements in analytical methodologies for dissecting glycan structural diversity, along with recent developments in biochemical and genetic approaches for studying glycan biosynthesis and catabolism, have provided a greater understanding of the biological contributions of these complex structures in vertebrates.},
	language = {en},
	number = {7},
	urldate = {2019-01-08},
	journal = {Nature Reviews Molecular Cell Biology},
	author = {Moremen, Kelley W. and Tiemeyer, Michael and Nairn, Alison V.},
	month = jul,
	year = {2012},
	pages = {448--462},
}

@article{churko_overview_2013,
	title = {Overview of {High} {Throughput} {Sequencing} {Technologies} to {Elucidate} {Molecular} {Pathways} in {Cardiovascular} {Diseases}},
	volume = {112},
	issn = {0009-7330, 1524-4571},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCRESAHA.113.300939},
	doi = {10.1161/CIRCRESAHA.113.300939},
	language = {en},
	number = {12},
	urldate = {2019-01-08},
	journal = {Circulation Research},
	author = {Churko, Jared M. and Mantalas, Gary L. and Snyder, Michael P. and Wu, Joseph C.},
	month = jun,
	year = {2013},
	pages = {1613--1623},
}

@article{aoki-kinoshita_introduction_2008,
	title = {An {Introduction} to {Bioinformatics} for {Glycomics} {Research}},
	volume = {4},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2398734/},
	doi = {10.1371/journal.pcbi.1000075},
	number = {5},
	urldate = {2019-01-08},
	journal = {PLoS Computational Biology},
	author = {Aoki-Kinoshita, Kiyoko F.},
	month = may,
	year = {2008},
	pmid = {18516240},
	pmcid = {PMC2398734},
}

@article{paegel_microfluidic_2003,
	title = {Microfluidic devices for {DNA} sequencing: sample preparation and electrophoretic analysis},
	volume = {14},
	issn = {0958-1669},
	shorttitle = {Microfluidic devices for {DNA} sequencing},
	url = {http://www.sciencedirect.com/science/article/pii/S0958166902000046},
	doi = {10.1016/S0958-1669(02)00004-6},
	abstract = {Modern DNA sequencing ‘factories’ have revolutionized biology by completing the human genome sequence, but in the race to completion we are left with inefficient, cumbersome, and costly macroscale processes and supporting facilities. During the same period, microfabricated DNA sequencing, sample processing and analysis devices have advanced rapidly toward the goal of a ‘sequencing lab-on-a-chip’. Integrated microfluidic processing dramatically reduces analysis time and reagent consumption, and eliminates costly and unreliable macroscale robotics and laboratory apparatus. A microfabricated device for high-throughput DNA sequencing that couples clone isolation, template amplification, Sanger extension, purification, and electrophoretic analysis in a single microfluidic circuit is now attainable.},
	number = {1},
	urldate = {2019-01-04},
	journal = {Current Opinion in Biotechnology},
	author = {Paegel, Brian M and Blazej, Robert G and Mathies, Richard A},
	month = feb,
	year = {2003},
	pages = {42--50},
}

@article{rissin_distinct_2008,
	title = {Distinct and {Long}-{Lived} {Activity} {States} of {Single} {Enzyme} {Molecules}},
	volume = {130},
	issn = {0002-7863, 1520-5126},
	url = {http://pubs.acs.org/doi/abs/10.1021/ja711414f},
	doi = {10.1021/ja711414f},
	abstract = {Individual enzyme molecules have been observed to possess discrete and different turnover rates due to the presence of long-lived activity states. These stable activity states are thought to result from different molecular conformations or post-translational modiﬁcations. The distributions in kinetic activity observed in previous studies were obtained from small numbers of single enzyme molecules. Due to this limitation, it has not been possible to fully characterize the different kinetic and equilibrium binding parameters of single enzyme molecules. In this paper, we analyze hundreds of single β-galactosidase molecules simultaneously; using a high-density array of 50 000 fL-reaction chambers, we conﬁrm the presence of long-lived kinetic states within a population of enzyme molecules. Our analysis has isolated the source of kinetic variability to kcat. The results explain the kinetic variability within enzyme molecule populations and offer a deeper understanding of the unique properties of single enzyme molecules. Gaining a more fundamental understanding of how individual enzyme molecules work within a population should provide insight into how they affect downstream biochemical processes. If the results reported here can be generalized to other enzymes, then the stochastic nature of individual enzyme molecule kinetics should have a substantial impact on the overall metabolic activity within a cell.},
	language = {en},
	number = {15},
	urldate = {2019-01-04},
	journal = {Journal of the American Chemical Society},
	author = {Rissin, David M. and Gorris, Hans H. and Walt, David R.},
	month = apr,
	year = {2008},
	pages = {5349--5353},
}

@article{gorris_stochastic_2007,
	title = {Stochastic inhibitor release and binding from single-enzyme molecules},
	volume = {104},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2077050/},
	doi = {10.1073/pnas.0705411104},
	abstract = {Inhibition kinetics of single-β-galactosidase molecules with the slow-binding inhibitor d-galactal have been characterized by segregating individual enzyme molecules in an array of 50,000 ultra small reaction containers and observing substrate turnover changes with fluorescence microscopy. Inhibited and active states of β-galactosidase could be clearly distinguished, and the large array size provided very good statistics. With a pre-steady-state experiment, we demonstrated the stochastic character of inhibitor release, which obeys first-order kinetics. Under steady-state conditions, the quantitative detection of substrate turnover changes over long time periods revealed repeated inhibitor binding and release events, which are accompanied by conformational changes of the enzyme's catalytic site. We proved that the rate constants of inhibitor release and binding derived from stochastic changes in the substrate turnover are consistent with bulk-reaction kinetics.},
	number = {45},
	urldate = {2019-01-04},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Gorris, Hans H. and Rissin, David M. and Walt, David R.},
	month = nov,
	year = {2007},
	pmid = {17965235},
	pmcid = {PMC2077050},
	pages = {17680--17685},
}

@article{kragstrup_simple_2013,
	title = {A simple set of validation steps identifies and removes false results in a sandwich enzyme-linked immunosorbent assay caused by anti-animal {IgG} antibodies in plasma from arthritis patients},
	volume = {2},
	issn = {2193-1801},
	doi = {10.1186/2193-1801-2-263},
	abstract = {Rheumatoid arthritis (RA) and spondyloarthritis (SpA) are chronic diseases characterized by activation of the immune system and production of antibodies. Thus, rheumatoid factor, anti-animal IgG antibodies and heterophilic antibodies in plasma samples from arthritis patients can interfere with immunoassays such as sandwich enzyme-linked immunosorbent assay (ELISA) systems often used in arthritis research. However, standard methodologies on how to test for false results caused by these antibodies are lacking. The objective of this study was to design a simple set of steps to validate a sandwich ELISA before using it for measuring analytes in plasma from arthritis patients. An interleukin-24 (IL-24) sandwich ELISA system was prepared with a monoclonal mouse capture antibody and a polyclonal goat detection antibody and tested for interference by rheumatoid factor, anti-animal IgG antibodies and heterophilic antibodies. Plasma samples from 23 patients with RA and SpA were used. No differences were found between plasma samples measured in wells coated with anti-IL-24 specific antibody and in wells coated with isotype control antibody (false positive results), and recombinant human IL-24 was not recovered in spiked samples (false negative results). This interference was removed after preincubating the plasma samples from patients with arthritis with goat or bovine IgG, suggesting that anti-animal IgG antibodies found in the plasma of the arthritis patients caused the false results. Additional testing showed that the signal-to-noise ratio could be increased by titration of the capture and detection antibodies and by using the ELAST amplification system. Finally, the calculated concentration of IL-24 was increased in ethylenediaminetetraacetic acid (EDTA) plasma compared to heparin plasma and serum and decreased with repetitive freeze/thaw cycles of the samples illustrating how sample handling could additionally contribute to the variations reported by different laboratories in measurement of the same analyte. This study proposes a simple set of validation steps to evaluate and optimize a sandwich ELISA before using it for measuring analytes in plasma from arthritis patients. Anti-animal IgG antibodies are also present in healthy individuals, suggesting that validation of ELISA systems for measuring non-arthritis samples could also be improved by this simple set of validation steps.},
	language = {eng},
	number = {1},
	journal = {SpringerPlus},
	author = {Kragstrup, Tue W. and Vorup-Jensen, Thomas and Deleuran, Bent and Hvid, Malene},
	month = dec,
	year = {2013},
	pmid = {23875127},
	pmcid = {PMC3695686},
	keywords = {Anti-animal IgG antibodies, Arthritis, ELAST amplification system, Enzyme-linked immunosorbent assay, Heterophilic antibodies, Immunoassay, Interference, Multiplex, Rheumatoid factor},
	pages = {263},
}

@misc{noauthor_cytokine_nodate,
	title = {Cytokine {Quantification} in {Drug} {Development} - - {A} comparison of sensitive immunoassay platforms},
	url = {https://www.bionity.com/en/whitepapers/100005/cytokine-quantification-in-drug-development.html},
	abstract = {Immuno-PCR (IPCR) and ElectroChemiLuminescence (ECL) are technologies for improved performance of immunoassays. The specific advantages of the different technologies, using enzymatic or electrical ...},
	language = {en},
	urldate = {2019-01-04},
}

@article{de_la_rica_plasmonic_2012,
	title = {Plasmonic {ELISA} for the ultrasensitive detection of disease biomarkers with the naked eye},
	volume = {7},
	copyright = {2012 Nature Publishing Group},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/nnano.2012.186},
	doi = {10.1038/nnano.2012.186},
	abstract = {In resource-constrained countries, affordable methodologies for the detection of disease biomarkers at ultralow concentrations can potentially improve the standard of living1,2. However, current strategies for ultrasensitive detection often require sophisticated instruments that may not be available in laboratories with fewer resources3,4,5,6,7,8,9,10,11. Here, we circumvent this problem by introducing a signal generation mechanism for biosensing that enables the detection of a few molecules of analyte with the naked eye. The enzyme label of an enzyme-linked immunosorbent assay (ELISA) controls the growth of gold nanoparticles and generates coloured solutions with distinct tonality when the analyte is present. Prostate specific antigen (PSA) and HIV-1 capsid antigen p24 were detected in whole serum at the ultralow concentration of 1 × 10−18 g ml−1. p24 was also detected with the naked eye in the sera of HIV-infected patients showing viral loads undetectable by a gold standard nucleic acid-based test.},
	language = {en},
	number = {12},
	urldate = {2019-01-04},
	journal = {Nature Nanotechnology},
	author = {de la Rica, Roberto and Stevens, Molly M.},
	month = dec,
	year = {2012},
	pages = {821--824},
}

@article{weiland_[enzyme-linked_1978,
	title = {[{The} enzyme-linked immunosorbent assay ({ELISA})--a new serodiagnostic method for the detection of parasitic infections (author's transl)]},
	volume = {120},
	issn = {0341-3098},
	abstract = {The enzyme-linked immunosorbent assay (ELISA) is a new serodiagnostic aid for the indirect demonstration of parasites. Examples of the application of ELISA are given in a review of the literature. Antibodies against Babesia and Besnoitia could be detected in various domestic and laboratory animals. The application of ELISA for the detection of toxoplasma antibodies also under the influence of hammondia infections is discussed. Larva migrans visceralis could be demonstrated using ELISA in mono-infected mice and dogs, cross reactions with other helminth antibodies in human sera do not, however, permit a reliable diagnosis of larva migrans visceralis. The enzyme-linked immunosorbent assay may be recommended as a screening test for contamination investigations.},
	language = {ger},
	number = {44},
	journal = {MMW, Munchener medizinische Wochenschrift},
	author = {Weiland, G.},
	month = nov,
	year = {1978},
	pmid = {100702},
	keywords = {Animals, Antibodies, Apicomplexa, Babesia, Cross Reactions, Dogs, Enzyme-Linked Immunosorbent Assay, Immunoenzyme Techniques, Larva Migrans, Mice, Parasitic Diseases, Toxoplasma, Trypanosomiasis},
	pages = {1457--1460},
}

@article{rissin_digital_2006,
	title = {Digital {Concentration} {Readout} of {Single} {Enzyme} {Molecules} {Using} {Femtoliter} {Arrays} and {Poisson} {Statistics}},
	volume = {6},
	issn = {1530-6984},
	url = {https://doi.org/10.1021/nl060227d},
	doi = {10.1021/nl060227d},
	abstract = {Methods for accurately quantifying the concentration of a particular analyte in solution are all based on ensemble responses in which many analyte molecules give rise to the measured signal. In this paper, single molecules of β-galactosidase were monitored using a 1 mm diameter fiber optic bundle with 2.4 × 105 individually sealed, femtoliter microwell reactors. By observation of the buildup of fluorescent products from single enzyme molecule catalysis over the array of reaction vessels and by application of a Poisson statistical analysis, a digital concentration readout was obtained. This approach should prove useful for single molecule enzymology and ultrasensitive bioassays. More generally, the ability to determine concentration by counting individual molecules offers a new approach to analysis of dilute solutions.},
	number = {3},
	urldate = {2018-12-19},
	journal = {Nano Letters},
	author = {Rissin, David M. and Walt, David R.},
	month = mar,
	year = {2006},
	pages = {520--523},
}

@article{rissin_single-molecule_2010,
	title = {Single-molecule enzyme-linked immunosorbent assay detects serum proteins at subfemtomolar concentrations},
	volume = {28},
	issn = {1087-0156, 1546-1696},
	url = {http://www.nature.com/articles/nbt.1641},
	doi = {10.1038/nbt.1641},
	abstract = {The detection of single protein molecules1,2 in blood could help identify many new diagnostic protein markers. We report an approach for detecting hundreds to thousands of individual protein molecules simultaneously that enables the detection of very low concentrations of proteins. Proteins are captured on microscopic beads and labeled with an enzyme, such that each bead has either one or zero enzyme-labeled proteins. By isolating these beads in arrays of 50-femtoliter reaction chambers, single proteins can be detected by fluorescence imaging. By singulating molecules in these arrays, {\textasciitilde}10–20 enzymes can be detected in 100 μL ({\textasciitilde}10−19 M). Single molecule enzyme-linked immunosorbent assays (digital ELISA) based on singulation of enzyme labels enabled the detection of clinically-relevant proteins in serum at concentrations ({\textless}10−15 M) much lower than conventional ELISA3-5. Digital ELISA detected prostate specific antigen in all tested sera from patients who had undergone radical prostatectomy, down to 14 fg/mL (0.4 fM).},
	language = {en},
	number = {6},
	urldate = {2018-12-19},
	journal = {Nature Biotechnology},
	author = {Rissin, David M and Kan, Cheuk W and Campbell, Todd G and Howes, Stuart C and Fournier, David R and Song, Linan and Piech, Tomasz and Patel, Purvish P and Chang, Lei and Rivnak, Andrew J and Ferrell, Evan P and Randall, Jeffrey D and Provuncher, Gail K and Walt, David R and Duffy, David C},
	month = jun,
	year = {2010},
	pages = {595--599},
}

@article{zhang_ultrasensitive_2018,
	title = {Ultrasensitive quantification of tumor {mRNAs} in extracellular vesicles with an integrated microfluidic digital analysis chip},
	volume = {18},
	issn = {1473-0197, 1473-0189},
	url = {http://xlink.rsc.org/?DOI=C8LC01071D},
	doi = {10.1039/C8LC01071D},
	language = {en},
	number = {24},
	urldate = {2018-12-19},
	journal = {Lab on a Chip},
	author = {Zhang, Peng and Crow, Jennifer and Lella, Divya and Zhou, Xin and Samuel, Glenson and Godwin, Andrew K. and Zeng, Yong},
	year = {2018},
	pages = {3790--3801},
}

@article{vogelstein_digital_1999,
	title = {Digital {PCR}},
	volume = {96},
	issn = {0027-8424},
	doi = {10.1073/pnas.96.16.9236},
	abstract = {The identification of predefined mutations expected to be present in a minor fraction of a cell population is important for a variety of basic research and clinical applications. Here, we describe an approach for transforming the exponential, analog nature of the PCR into a linear, digital signal suitable for this purpose. Single molecules are isolated by dilution and individually amplified by PCR; each product is then analyzed separately for the presence of mutations by using fluorescent probes. The feasibility of the approach is demonstrated through the detection of a mutant ras oncogene in the stool of patients with colorectal cancer. The process provides a reliable and quantitative measure of the proportion of variant sequences within a DNA sample.},
	language = {English},
	number = {16},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Vogelstein, B. and Kinzler, K. W.},
	month = aug,
	year = {1999},
	note = {WOS:000081835500084},
	keywords = {cancer patients, genomic dna, malignant-melanoma, molecular beacons, oncogene mutations, peripheral-blood, point mutations, polymerase chain-reaction, ras gene-mutations, tumor-progression},
	pages = {9236--9241},
}

@article{heyries_megapixel_2011,
	title = {Megapixel digital {PCR}},
	volume = {8},
	copyright = {2011 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1640},
	doi = {10.1038/nmeth.1640},
	abstract = {We present a microfluidic 'megapixel' digital PCR device that uses surface tension–based sample partitioning and dehydration control to enable high-fidelity single DNA molecule amplification in 1,000,000 reactors of picoliter volume with densities up to 440,000 reactors cm−2. This device achieves a dynamic range of 107, single-nucleotide-variant detection below one copy per 100,000 wild-type sequences and the discrimination of a 1\% difference in chromosome copy number.},
	language = {en},
	number = {8},
	urldate = {2018-12-19},
	journal = {Nature Methods},
	author = {Heyries, Kevin A. and Tropini, Carolina and VanInsberghe, Michael and Doolin, Callum and Petriv, Oleh I. and Singhal, Anupam and Leung, Kaston and Hughesman, Curtis B. and Hansen, Carl L.},
	month = aug,
	year = {2011},
	pages = {649--651},
}

@article{liu_rapid_2012,
	title = {Rapid detection of live methicillin-resistant \textit{{Staphylococcus} aureus} by using an integrated microfluidic system capable of ethidium monoazide pre-treatment and molecular diagnosis},
	volume = {6},
	issn = {1932-1058},
	url = {http://aip.scitation.org/doi/10.1063/1.4748358},
	doi = {10.1063/1.4748358},
	language = {en},
	number = {3},
	urldate = {2018-12-19},
	journal = {Biomicrofluidics},
	author = {Liu, Yu-Hsin and Wang, Chih-Hung and Wu, Jiunn-Jong and Lee, Gwo-Bin},
	month = sep,
	year = {2012},
	pages = {034119},
}

@article{dhar_research_2015,
	title = {Research highlights: microfluidic-enabled single-cell epigenetics},
	volume = {15},
	issn = {1473-0189},
	shorttitle = {Research highlights},
	url = {https://pubs.rsc.org/en/content/articlelanding/2015/lc/c5lc90101d},
	doi = {10.1039/C5LC90101D},
	abstract = {Individual cells are the fundamental unit of life with diverse functions from metabolism to motility. In multicellular organisms, a single genome can give rise to tremendous variability across tissues at the single-cell level due to epigenetic differences in the genes that are expressed. Signals from the local environment or a history of signals can drive these variations, and tissues have many cell types that play separate roles. This epigenetic heterogeneity is of biological importance in normal functions such as tissue morphogenesis and can contribute to development or resistance of cancer, or other disease states. Therefore, an improved understanding of variations at the single cell level are fundamental to understanding biology and developing new approaches to combating disease. Traditional approaches to characterize epigenetic modifications of chromatin or the transcriptome of cells have often focused on blended responses of many cells in a tissue; however, such bulk measures lose spatial and temporal differences that occur from cell to cell, and cannot uncover novel or rare populations of cells. Here we highlight a flurry of recent activity to identify the mRNA profiles from thousands of single-cells as well as chromatin accessibility and histone marks on single to few hundreds of cells. Microfluidics and microfabrication have played a central role in the range of new techniques, and will likely continue to impact their further development towards routine single-cell epigenetic analysis.},
	language = {en},
	number = {21},
	urldate = {2018-12-19},
	journal = {Lab on a Chip},
	author = {Dhar, Manjima and Khojah, Reem and Tay, Andy and Carlo, Dino Di},
	month = oct,
	year = {2015},
	pages = {4109--4113},
}

@article{chronis_microfluidics_2007,
	title = {Microfluidics for \textit{in vivo} imaging of neuronal and behavioral activity in \textit{{Caenorhabditis} elegans}},
	volume = {4},
	copyright = {2007 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth1075},
	doi = {10.1038/nmeth1075},
	abstract = {The nematode C. elegans is an excellent model organism for studying behavior at the neuronal level. Because of the organism's small size, it is challenging to deliver stimuli to C. elegans and monitor neuronal activity in a controlled environment. To address this problem, we developed two microfluidic chips, the 'behavior' chip and the 'olfactory' chip for imaging of neuronal and behavioral responses in C. elegans. We used the behavior chip to correlate the activity of AVA command interneurons with the worm locomotion pattern. We used the olfactory chip to record responses from ASH sensory neurons exposed to high-osmotic-strength stimulus. Observation of neuronal responses in these devices revealed previously unknown properties of AVA and ASH neurons. The use of these chips can be extended to correlate the activity of sensory neurons, interneurons and motor neurons with the worm's behavior.},
	language = {en},
	number = {9},
	urldate = {2018-12-19},
	journal = {Nature Methods},
	author = {Chronis, Nikos and Zimmer, Manuel and Bargmann, Cornelia I.},
	month = sep,
	year = {2007},
	pages = {727--731},
}

@article{shendure_next-generation_2008,
	title = {Next-generation {DNA} sequencing},
	volume = {26},
	copyright = {2008 Nature Publishing Group},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt1486},
	doi = {10.1038/nbt1486},
	abstract = {DNA sequence represents a single format onto which a broad range of biological phenomena can be projected for high-throughput data collection. Over the past three years, massively parallel DNA sequencing platforms have become widely available, reducing the cost of DNA sequencing by over two orders of magnitude, and democratizing the field by putting the sequencing capacity of a major genome center in the hands of individual investigators. These new technologies are rapidly evolving, and near-term challenges include the development of robust protocols for generating sequencing libraries, building effective new approaches to data-analysis, and often a rethinking of experimental design. Next-generation DNA sequencing has the potential to dramatically accelerate biological and biomedical research, by enabling the comprehensive analysis of genomes, transcriptomes and interactomes to become inexpensive, routine and widespread, rather than requiring significant production-scale efforts.},
	language = {en},
	number = {10},
	urldate = {2018-12-19},
	journal = {Nature Biotechnology},
	author = {Shendure, Jay and Ji, Hanlee},
	month = oct,
	year = {2008},
	pages = {1135--1145},
}

@article{sjostrom_multiplex_2013,
	title = {Multiplex analysis of enzyme kinetics and inhibition by droplet microfluidics using picoinjectors},
	volume = {13},
	issn = {1473-0189},
	doi = {10.1039/c3lc41398e},
	abstract = {Enzyme kinetics and inhibition is important for a wide range of disciplines including pharmacology, medicine and industrial bioprocess technology. We present a novel microdroplet-based device for extensive characterization of the reaction kinetics of enzyme substrate inhibitor systems in a single experiment utilizing an integrated droplet picoinjector for bioanalysis. This device enables the scanning of multiple fluorescently-barcoded inhibitor concentrations and substrate conditions in a single, highly time-resolved experiment yielding the Michaelis constant (Km), the turnover number (kcat) and the enzyme inhibitor dissociation constants (ki, ki'). Using this device we determine Km and kcat for β-galactosidase and the fluorogenic substrate Resorufin β-D-galactopyranoside (RBG) to be 442 μM and 1070 s(-1), respectively. Furthermore, we examine the inhibitory effects of isopropyl-β-D-thiogalactopyranoside (IPTG) on β-galactosidase. This system has a number of potential applications, for example it could be used to screen inhibitors to pharmaceutically relevant enzymes and to characterize engineered enzyme variants for biofuels production, in both cases acquiring detailed information about the enzyme catalysis and enzyme inhibitor interaction at high throughput and low cost.},
	language = {eng},
	number = {9},
	journal = {Lab on a Chip},
	author = {Sjostrom, Staffan L. and Joensson, Haakan N. and Svahn, Helene Andersson},
	month = may,
	year = {2013},
	pmid = {23478908},
	keywords = {Escherichia coli, Escherichia coli Proteins, Kinetics, Microfluidic Analytical Techniques, Models, Chemical, beta-Galactosidase},
	pages = {1754--1761},
}

@article{abate_dna_2013,
	title = {{DNA} sequence analysis with droplet-based microfluidics},
	volume = {13},
	issn = {1473-0197},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4090915/},
	doi = {10.1039/c3lc50905b},
	abstract = {Droplet-based microfluidic techniques can form and process micrometer scale droplets at thousands per second. Each droplet can house an individual biochemical reaction, allowing millions of reactions to be performed in minutes with small amounts of total reagent. This versatile approach has been used for engineering enzymes, quantifying concentrations of DNA in solution, and screening protein crystallization conditions. Here, we use it to read the sequences of DNA molecules with a FRET-based assay. Using probes of different sequences, we interrogate a target DNA molecule for polymorphisms. With a larger probe set, additional polymorphisms can be interrogated as well as targets of arbitrary sequence.},
	number = {24},
	urldate = {2018-12-17},
	journal = {Lab on a chip},
	author = {Abate, Adam R. and Hung, Tony and Sperling, Ralph A. and Mary, Pascaline and Rotem, Assaf and Agresti, Jeremy J. and Weiner, Michael A. and Weitz, David A.},
	month = dec,
	year = {2013},
	pmid = {24185402},
	pmcid = {PMC4090915},
	pages = {4864--4869},
}

@article{chiu_small_2017,
	title = {Small but {Perfectly} {Formed}? {Successes}, {Challenges}, and {Opportunities} for {Microfluidics} in the {Chemical} and {Biological} {Sciences}},
	volume = {2},
	issn = {2451-9294},
	shorttitle = {Small but {Perfectly} {Formed}?},
	url = {http://www.sciencedirect.com/science/article/pii/S2451929417300335},
	doi = {10.1016/j.chempr.2017.01.009},
	abstract = {Summary
Microfluidic systems are pervasive in many areas of experimental science, but what are the real advantages of this technology? We describe some of the features and properties that make microfluidic devices unique experimental tools. In addition to pointing out some of the less effective uses of this technology, we assess the most successful applications of microfluidics over the last two decades and highlight the areas where they had the greatest impact. We also propose applications where microfluidic systems could be applied to the greatest effect in the future.},
	number = {2},
	urldate = {2018-12-17},
	journal = {Chem},
	author = {Chiu, Daniel T. and deMello, Andrew J. and Di Carlo, Dino and Doyle, Patrick S. and Hansen, Carl and Maceiczyk, Richard M. and Wootton, Robert C. R.},
	month = feb,
	year = {2017},
	keywords = {SDG3: Good health and well-being, diagnostics, droplets, genomics, microfluidics, nanotechnology, organ-on-a-chip, single-cell analysis, single-molecule detection},
	pages = {201--223},
}

@article{syedmoradi_point_2017,
	title = {Point of care testing: {The} impact of nanotechnology},
	volume = {87},
	issn = {0956-5663},
	shorttitle = {Point of care testing},
	url = {http://www.sciencedirect.com/science/article/pii/S0956566316308417},
	doi = {10.1016/j.bios.2016.08.084},
	abstract = {Point-of-care (POC) diagnostic devices are integral in the health care system and particularly for the diagnosis and monitoring of diseases. POC testing has a variety of advantages including the ability to provide rapid and accurate results, ease of use, low cost, and little need for specialized equipment. One of the goals of POC testing is the development of a chip-based, miniaturized, portable, and self-containing system that allows for the assay of different analytes in complex samples. To achieve these goals, many researchers have focused on paper-based and printed electrode technologies as the material for fabricating POC diagnostic systems. These technologies are affordable, sensitive, user-friendly, rapid, and scalable for manufacturing. Moreover, the combination such devices with nanomaterials provide a path for the development of highly sensitive and selective biosensors for future generation POC tools. This review article discusses present technologies in on-site or at home POC diagnostic assays implemented in paper-based microfluidic and screen printing devices over the past decade as well as in the near future. In addition, recent advances in the application of nanomaterials such as gold nanoparticles, carbon nanotubes (CNTs), magnetic nanoparticles, and graphene in POC devices will be reviewed. The factors that limit POC testing to become real world products and future directions are also identified.},
	urldate = {2018-12-17},
	journal = {Biosensors and Bioelectronics},
	author = {Syedmoradi, Leila and Daneshpour, Maryam and Alvandipour, Mehrdad and Gomez, Frank A. and Hajghassem, Hassan and Omidfar, Kobra},
	month = jan,
	year = {2017},
	keywords = {Biosensor, In vitro diagnosis, Lab-on-a-chip, Microfluidic, Nanotechnology, Point-of-care (POC)},
	pages = {373--387},
}

@article{verma_sliding-strip_2018,
	title = {Sliding-strip microfluidic device enables {ELISA} on paper},
	volume = {99},
	issn = {0956-5663},
	url = {http://www.sciencedirect.com/science/article/pii/S0956566317304815},
	doi = {10.1016/j.bios.2017.07.034},
	abstract = {This article describes a 3D microfluidic paper-based analytical device that can be used to conduct an enzyme-linked immunosorbent assay (ELISA). The device comprises two parts: a sliding strip (which contains the active sensing area) and a structure surrounding the sliding strip (which holds stored reagents—buffers, antibodies, and enzymatic substrate—and distributes fluid). Running an ELISA involves adding sample (e.g. blood) and water, moving the sliding strip at scheduled times, and analyzing the resulting color in the sensing area visually or using a flatbed scanner. We demonstrate that this device can be used to detect C-reactive protein (CRP)—a biomarker for neonatal sepsis, pelvic inflammatory disease, and inflammatory bowel diseases—at a concentration range of 1–100ng/mL in 1000-fold diluted blood (1–100µg/mL in undiluted blood). The accuracy of the device (as characterized by the area under the receiver operator characteristics curve) is 89\% and 83\% for cut-offs of 10ng/mL (for neonatal sepsis and pelvic inflammatory disease) and 30ng/mL (for inflammatory bowel diseases) CRP in 1000-fold diluted blood respectively. In resource-limited settings, the device can be used as a part of a kit (containing the device, a fixed-volume capillary, a pre-filled tube, a syringe, and a dropper); this kit would cost {\textasciitilde} \$0.50 when produced in large scale ({\textgreater}100,000 devices/week). This kit has the technical characteristics to be employed as a pre-screening tool, when combined with other data such as patient history and clinical signs.},
	urldate = {2018-12-17},
	journal = {Biosensors and Bioelectronics},
	author = {Verma, Mohit S. and Tsaloglou, Maria-Nefeli and Sisley, Tyler and Christodouleas, Dionysios and Chen, Austin and Milette, Jonathan and Whitesides, George M.},
	month = jan,
	year = {2018},
	keywords = {Blood, C-reactive protein, Inflammatory bowel diseases, Receiver operator characteristics, Resource-limited, Sepsis},
	pages = {77--84},
}

@article{machado_multiplexed_2018,
	title = {Multiplexed capillary microfluidic immunoassay with smartphone data acquisition for parallel mycotoxin detection},
	volume = {99},
	issn = {0956-5663},
	url = {http://www.sciencedirect.com/science/article/pii/S0956566317304797},
	doi = {10.1016/j.bios.2017.07.032},
	abstract = {The field of microfluidics holds great promise for the development of simple and portable lab-on-a-chip systems. The use of capillarity as a means of fluidic manipulation in lab-on-a-chip systems can potentially reduce the complexity of the instrumentation and allow the development of user-friendly devices for point-of-need analyses. In this work, a PDMS microchannel-based, colorimetric, autonomous capillary chip provides a multiplexed and semi-quantitative immunodetection assay. Results are acquired using a standard smartphone camera and analyzed with a simple gray scale quantification procedure. The performance of this device was tested for the simultaneous detection of the mycotoxins ochratoxin A (OTA), aflatoxin B1 (AFB1) and deoxynivalenol (DON) which are strictly regulated food contaminants with severe detrimental effects on human and animal health. The multiplexed assay was performed approximately within 10min and the achieved sensitivities of{\textless}40, 0.1–0.2 and{\textless}10ng/mL for OTA, AFB1 and DON, respectively, fall within the majority of currently enforced regulatory and/or recommended limits. Furthermore, to assess the potential of the device to analyze real samples, the immunoassay was successfully validated for these 3 mycotoxins in a corn-based feed sample after a simple sample preparation procedure.},
	urldate = {2018-12-17},
	journal = {Biosensors and Bioelectronics},
	author = {Machado, Jessica M. D. and Soares, Ruben R. G. and Chu, Virginia and Conde, João P.},
	month = jan,
	year = {2018},
	keywords = {Capillarity, Colorimetry, Microfluidics, Multiplexing, Mycotoxins, Smartphone},
	pages = {40--46},
}

@article{rothbauer_recent_2018,
	title = {Recent advances in microfluidic technologies for cell-to-cell interaction studies},
	volume = {18},
	issn = {1473-0197, 1473-0189},
	url = {http://xlink.rsc.org/?DOI=C7LC00815E},
	doi = {10.1039/C7LC00815E},
	language = {en},
	number = {2},
	urldate = {2018-12-17},
	journal = {Lab on a Chip},
	author = {Rothbauer, Mario and Zirath, Helene and Ertl, Peter},
	year = {2018},
	pages = {249--270},
}

@article{rothbauer_recent_2018-1,
	title = {Recent advances in microfluidic technologies for cell-to-cell interaction studies},
	volume = {18},
	issn = {1473-0197, 1473-0189},
	url = {http://xlink.rsc.org/?DOI=C7LC00815E},
	doi = {10.1039/C7LC00815E},
	language = {en},
	number = {2},
	urldate = {2018-12-17},
	journal = {Lab on a Chip},
	author = {Rothbauer, Mario and Zirath, Helene and Ertl, Peter},
	year = {2018},
	pages = {249--270},
}

@article{zhou_reduced_2018,
	title = {Reduced graphene oxide/{BiFeO3} nanohybrids-based signal-on photoelectrochemical sensing system for prostate-specific antigen detection coupling with magnetic microfluidic device},
	volume = {101},
	issn = {0956-5663},
	url = {http://www.sciencedirect.com/science/article/pii/S0956566317306966},
	doi = {10.1016/j.bios.2017.10.027},
	abstract = {A novel magnetic controlled photoelectrochemical (PEC) sensing system was designed for sensitive detection of prostate-specific antigen (PSA) using reduced graphene oxide-functionalized BiFeO3 (rGO-BiFeO3) as the photoactive material and target-triggered hybridization chain reaction (HCR) for signal amplification. Remarkably enhanced PEC performance could be obtained by using rGO-BiFeO3 as the photoelectrode material due to its accelerated charge transfer and improved the visible light absorption. Additionally, efficient and simple operation could be achieved by introducing magnetic controlled flow-through device. The assay mainly involved in anchor DNA-conjugated magnetic bead (MB-aDNA), PSA aptamer/trigger DNA (Apt-tDNA) and two glucose oxidase-labeled hairpins (H1-GOx and H2-GOx). Upon addition of target PSA, the analyte initially reacted with the aptamer to release the trigger DNA, which partially hybridized with the anchor DNA on the MB. Thereafter, the unpaired trigger DNA on the MB opened the hairpin DNA structures in sequence and propagated a chain reaction of hybridization events between two alternating hairpins to form a long nicked double-helix with numerous GOx enzymes on it. Subsequently, the enzymatic product (H2O2) generated and consumed the photo-excited electrons from rGO-BiFeO3 under visible light irradiation to enhance the photocurrent. Under optimal conditions, the magnetic controlled PEC sensing system exhibited good photocurrent responses toward target PSA within the linear range of 0.001 − 100ng/mL with a detection limit of 0.31pg/mL. Moreover, favorable selectivity, good stability and satisfactory accuracy were obtained. The excellent analytical performance suggested that the rGO-BiFeO3-based PEC sensing platform could be a promising tool for sensitive, efficient and low cost detection of PSA in disease diagnostics.},
	urldate = {2018-12-17},
	journal = {Biosensors and Bioelectronics},
	author = {Zhou, Qian and Lin, Youxiu and Zhang, Kangyao and Li, Meijin and Tang, Dianping},
	month = mar,
	year = {2018},
	keywords = {BiFeO, Magnetic microfluidic device, Photoelectrochemical sensing, Prostate-specific antigen, Reduced graphene oxide},
	pages = {146--152},
}

@article{schneider_automating_2018,
	title = {Automating drug discovery},
	volume = {17},
	copyright = {2017 Nature Publishing Group},
	issn = {1474-1784},
	url = {https://www.nature.com/articles/nrd.2017.232},
	doi = {10.1038/nrd.2017.232},
	abstract = {Small-molecule drug discovery can be viewed as a challenging multidimensional problem in which various characteristics of compounds — including efficacy, pharmacokinetics and safety — need to be optimized in parallel to provide drug candidates. Recent advances in areas such as microfluidics-assisted chemical synthesis and biological testing, as well as artificial intelligence systems that improve a design hypothesis through feedback analysis, are now providing a basis for the introduction of greater automation into aspects of this process. This could potentially accelerate time frames for compound discovery and optimization and enable more effective searches of chemical space. However, such approaches also raise considerable conceptual, technical and organizational challenges, as well as scepticism about the current hype around them. This article aims to identify the approaches and technologies that could be implemented robustly by medicinal chemists in the near future and to critically analyse the opportunities and challenges for their more widespread application.},
	language = {en},
	number = {2},
	urldate = {2018-12-17},
	journal = {Nature Reviews Drug Discovery},
	author = {Schneider, Gisbert},
	month = feb,
	year = {2018},
	pages = {97--113},
}

@article{basauri_predictive_2019,
	title = {Predictive model for the design of reactive micro-separations},
	volume = {209},
	issn = {1383-5866},
	url = {http://www.sciencedirect.com/science/article/pii/S1383586618318069},
	doi = {10.1016/j.seppur.2018.09.028},
	abstract = {Solvent extraction at micro-scale has known increased interest in recent years. Consequently, numerous theoretical and experimental studies focused on diagnosis, detection and recovery of either biological substrates or base metals, isotopes and rare earths have been reported so far in the last few years. Within this context, the present work reports a thorough analysis of the coupling between the hydrodynamics, mass transfer and chemical reaction kinetics in multiphase micro-systems; the predictive mathematical model describes under non-steady state, fluid-wall interaction and interaction between the fluids in contact to finally predict the solute mass transport rate between the flowing liquid phases when a fully developed profile and a flat interface are not initially assumed unlike previously reported works. This approach allows tracking the interface along the complete micro device geometry and makes the model adaptive to the specific needs of each micro fluidic application. The analysis has been developed for a model system based on the mass transfer of Cr (VI) from an aqueous feed phase in a stratified 2-layer flow system in a Y-Y shape micro device. Two different scenarios have been considered: (a) two homogeneous phases, where water is the receptor phase (b) an heterogeneous system where the solute moves from the feed solution to a receptor phase composed of Shellsol D-70 and Alamine 336 as the selective extractant. Model simulations accounting for the system reaction parameters already reported have been assessed against a set of experimental runs; the two systems under consideration provided data that satisfactorily fitted simulations with an error less than 10\%, thus, validating model calculations. Thus, this rigorous and flexible model seeks to provide a useful tool for the design of micro separation processes by predicting the technical performance for numerous applications at micro scale.},
	urldate = {2018-12-17},
	journal = {Separation and Purification Technology},
	author = {Basauri, Arantza and Gomez-Pastora, Jennifer and Fallanza, Marcos and Bringas, Eugenio and Ortiz, Inmaculada},
	month = jan,
	year = {2019},
	keywords = {CDF modelling, Diffusion, Facilitated transport, Microfluidics, Solvent extraction},
	pages = {900--907},
}

@article{wu_microfluidic_2019,
	title = {Microfluidic {DNA} combing for parallel single-molecule analysis},
	volume = {30},
	issn = {0957-4484},
	url = {http://stacks.iop.org/0957-4484/30/i=4/a=045101},
	doi = {10.1088/1361-6528/aaeddc},
	abstract = {DNA combing is a widely used method for stretching and immobilising DNA molecules on a surface. Fluorescent labelling of genomic information enables high-resolution optical analysis of DNA at the single-molecule level. Despite its simplicity, the application of DNA combing in diagnostic workflows is still limited, mainly due to difficulties in analysing multiple small-volume DNA samples in parallel. Here, we report a simple and versatile microfluidic DNA combing technology ( μ DC), which allows manipulating, stretching and imaging of multiple, microliter scale DNA samples by employing a manifold of parallel microfluidic channels. Using DNA molecules with repetitive units as molecular rulers, we demonstrate that the μ DC technology allows uniform stretching of DNA molecules. The stretching ratio remains consistent along individual molecules as well as between different molecules in the various channels, allowing simultaneous quantitative analysis of different samples loaded into parallel channels. Furthermore, we demonstrate the application of μ DC to characterise UVB-induced DNA damage levels in human embryonic kidney cells and the spatial correlation between DNA damage sites. Our results point out the potential application of μ DC for quantitative and comparative single-molecule studies of genomic features. The extremely simple design of μ DC makes it suitable for integration into other microfluidic platforms to facilitate high-throughput DNA analysis in biological research and medical point-of-care applications.},
	language = {en},
	number = {4},
	urldate = {2018-12-17},
	journal = {Nanotechnology},
	author = {Wu, Shuyi and Jeffet, Jonathan and Grunwald, Assaf and Sharim, Hila and Gilat, Noa and Torchinsky, Dmitry and {Quanshui Zheng} and Zirkin, Shahar and Xu, Luping and Ebenstein, Yuval},
	year = {2019},
	pages = {045101},
}

@article{cao_microfluidic_2019,
	title = {Microfluidic exponential rolling circle amplification for sensitive {microRNA} detection directly from biological samples},
	volume = {279},
	issn = {0925-4005},
	url = {http://www.sciencedirect.com/science/article/pii/S0925400518317568},
	doi = {10.1016/j.snb.2018.09.121},
	abstract = {There is an urgent need of sensitive bioanalytical platforms for sensitive and precise quantification of low-abundance microRNA targets in complex biological samples, including liquid biopsies of tumors. Many of current miRNA biosensing methods require laborious sample pretreatment procedures, including extraction of total RNA, which largely limits their biomedical and clinical applications. Herein we developed an integrated Microfluidic Exponential Rolling Circle Amplification (MERCA) platform for sensitive and specific detection of microRNAs directly in minimally processed samples. The MERCA system integrates and streamlines solid-phase miRNA isolation, miRNA-adapter ligation, and a dual-phase exponential rolling circle amplification (eRCA) assay in one analytical workflow. By marrying the advantages of microfluidics in leveraging bioassay performance with the high sensitivity of eRCA, our method affords a remarkably low limit of detection at {\textless}10 zeptomole levels, with the ability to discriminate single-nucleotide difference. Using the MERCA chip, we demonstrated quantitative detection of miRNAs in total RNA, raw cell lysate, and cell-derived exosomes. Comparing with the parallel TaqMan RT-qPCR measurements verified the adaptability of the MERCA system for detection of miRNA biomarkers in complex biological materials. In particular, high sensitivity of our method enables direct detection of low-level exosomal miRNAs in as few as 2 × 106 exosomes. Such analytical capability immediately addresses the unmet challenge in sample consumption, a key setback in clinical development of exosome-based liquid biopsies. Therefore, the MERCA would provide a useful platform to facilitate miRNA analysis in broad biological and clinical applications.},
	urldate = {2018-12-17},
	journal = {Sensors and Actuators B: Chemical},
	author = {Cao, Hongmei and Zhou, Xin and Zeng, Yong},
	month = jan,
	year = {2019},
	keywords = {Cell-derived exosomes, Complex biological samples, Microfluidic exponential rolling circle amplification (MERCA), Raw cell lysate, microRNA detection},
	pages = {447--457},
}

@article{lara_identification_2017,
	title = {Identification of {Receptor} {Binding} to the {Biomolecular} {Corona} of {Nanoparticles}},
	volume = {11},
	issn = {1936-0851},
	url = {https://doi.org/10.1021/acsnano.6b07933},
	doi = {10.1021/acsnano.6b07933},
	abstract = {Biomolecules adsorbed on nanoparticles are known to confer a biological identity to nanoparticles, mediating the interactions with cells and biological barriers. However, how these molecules are presented on the particle surface in biological milieu remains unclear. The central aim of this study is to identify key protein recognition motifs and link them to specific cell-receptor interactions. Here, we employed an immuno-mapping technique to quantify epitope presentations of two major proteins in the serum corona, low-density lipoprotein and immunoglobulin G. Combining with a purpose-built receptor expression system, we show that both proteins present functional motifs to allow simultaneous recognition by low-density lipoprotein receptor and Fc-gamma receptor I of the corona. Our results suggest that the “labeling” of nanoparticles by biomolecular adsorption processes allows for multiple pathways in biological processes in which they may be “mistaken” for endogenous objects, such as lipoproteins, and exogenous ones, such as viral infections.},
	number = {2},
	urldate = {2018-12-12},
	journal = {ACS Nano},
	author = {Lara, Sandra and Alnasser, Fatima and Polo, Ester and Garry, David and Lo Giudice, Maria Cristina and Hristov, Delyan R. and Rocks, Louise and Salvati, Anna and Yan, Yan and Dawson, Kenneth A.},
	month = feb,
	year = {2017},
	pages = {1884--1893},
}

@article{huang_three-dimensional_2008,
	title = {Three-{Dimensional} {Super}-{Resolution} {Imaging} by {Stochastic} {Optical} {Reconstruction} {Microscopy}},
	volume = {319},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1153529},
	doi = {10.1126/science.1153529},
	language = {en},
	number = {5864},
	urldate = {2018-09-03},
	journal = {Science},
	author = {Huang, B. and Wang, W. and Bates, M. and Zhuang, X.},
	month = feb,
	year = {2008},
	pages = {810--813},
}

@article{elizondo_influence_2012,
	title = {Influence of the {Preparation} {Route} on the {Supramolecular} {Organization} of {Lipids} in a {Vesicular} {System}},
	volume = {134},
	issn = {0002-7863},
	url = {https://doi.org/10.1021/ja2086678},
	doi = {10.1021/ja2086678},
	abstract = {A confocal fluorescence microscopy-based assay was used for studying the influence of the preparation route on the supramolecular organization of lipids in a vesicular system. In this work, vesicles composed of cholesterol and CTAB (1/1 mol \%) or cholesterol and DOPC (2/8 mol \%) and incorporating two membrane dyes were prepared by either a compressed fluid (CF)-based method (DELOS-susp) or a conventional film hydration procedure. They were subsequently immobilized and imaged individually using a confocal fluorescence microscope. Two integrated fluorescence intensities, Idye1 and Idye2, were assigned to each tracked vesicle, and their ratio, Idye1/Idye2, was used for quantifying the degree of membrane inhomogeneity between individual vesicles within each sample. A distribution of Idye1/Idye2 values was obtained for all the studied vesicular systems, indicating intrasample heterogeneity. The degree of inhomogeneity (DI) was similar for Chol/DOPC vesicles prepared by both procedures. In contrast, DI was more than double for the hydration method compared to the CF-based method in the case of Chol/CTAB vesicles, which can suffer from lipid demixing during film formation. These findings reveal a more homogeneous vesicle formation path by CFs, which warranted good homogeneity of the vesicular system, independently of the lipid mixture used.},
	number = {4},
	urldate = {2018-06-11},
	journal = {Journal of the American Chemical Society},
	author = {Elizondo, Elisa and Larsen, Jannik and Hatzakis, Nikos S. and Cabrera, Ingrid and Bjørnholm, Thomas and Veciana, Jaume and Stamou, Dimitrios and Ventosa, Nora},
	month = feb,
	year = {2012},
	pages = {1918--1921},
}

@article{izeddin_psf_2012,
	title = {{PSF} shaping using adaptive optics for three-dimensional single-molecule super-resolution imaging and tracking},
	volume = {20},
	issn = {1094-4087},
	doi = {10.1364/OE.20.004957},
	abstract = {We present a novel approach for three-dimensional localization of single molecules using adaptive optics. A 52-actuator deformable mirror is used to both correct aberrations and induce two-dimensional astigmatism in the point-spread-function. The dependence of the z-localization precision on the degree of astigmatism is discussed. We achieve a z-localization precision of 40 nm for fluorescent proteins and 20 nm for fluorescent dyes, over an axial depth of similar to 800 nm. We illustrate the capabilities of our approach for three-dimensional high-resolution microscopy with superresolution images of actin filaments in fixed cells and single-molecule tracking of quantum-dot labeled transmembrane proteins in live HeLa cells. (C) 2012 Optical Society of America},
	language = {English},
	number = {5},
	journal = {Optics Express},
	author = {Izeddin, Ignacio and El Beheiry, Mohamed and Andilla, Jordi and Ciepielewski, Daniel and Darzacq, Xavier and Dahan, Maxime},
	month = feb,
	year = {2012},
	note = {WOS:000301053200014},
	keywords = {Active or adaptive optics, Medical optics instrumentation, Three-dimensional microscopy, aberration correction, algorithms, computation, diffraction-limit, fluorescence   microscopy, photoactivation localization microscopy, quantum-dot tracking, reconstruction microscopy, time, wide-field microscopy},
	pages = {4957--4967},
}

@article{bringmann_multivariate_2018,
	title = {Multivariate {Analysis} of {Orthogonal} {Range} {Searching} and {Graph} {Distances} {Parameterized} by {Treewidth}},
	url = {http://arxiv.org/abs/1805.07135},
	abstract = {We show that the eccentricities, diameter, radius, and Wiener index of an undirected \$n\$-vertex graph with nonnegative edge lengths can be computed in time \$O(n{\textbackslash}cdot {\textbackslash}binom\{k+{\textbackslash}lceil{\textbackslash}log n{\textbackslash}rceil\}\{k\} {\textbackslash}cdot 2{\textasciicircum}k k{\textasciicircum}2 {\textbackslash}log n)\$, where \$k\$ is the treewidth of the graph. For every \${\textbackslash}epsilon{\textgreater}0\$, this bound is \$n{\textasciicircum}\{1+{\textbackslash}epsilon\}{\textbackslash}exp O(k)\$, which matches a hardness result of Abboud, Vassilevska Williams, and Wang (SODA 2015) and closes an open problem in the multivariate analysis of polynomial-time computation. To this end, we show that the analysis of an algorithm of Cabello and Knauer (Comp. Geom., 2009) in the regime of non-constant treewidth can be improved by revisiting the analysis of orthogonal range searching, improving bounds of the form \${\textbackslash}log{\textasciicircum}d n\$ to \${\textbackslash}binom\{d+{\textbackslash}lceil{\textbackslash}log n{\textbackslash}rceil\}\{d\}\$, as originally observed by Monier (J. Alg. 1980). We also investigate the parameterization by vertex cover number.},
	language = {en},
	urldate = {2018-05-26},
	journal = {arXiv:1805.07135 [cs]},
	author = {Bringmann, Karl and Husfeldt, Thore and Magnusson, Måns},
	month = may,
	year = {2018},
	note = {arXiv: 1805.07135},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{fawcett_roc_nodate,
	title = {{ROC} {Graphs}: {Notes} and {Practical} {Considerations} for {Researchers}},
	abstract = {Receiver Operating Characteristics (ROC) graphs are a useful technique for organizing classiﬁers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been increasingly adopted in the machine learning and data mining research communities. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. This article serves both as a tutorial introduction to ROC graphs and as a practical guide for using them in research.},
	language = {en},
	journal = {Pattern Recognition Letters},
	author = {Fawcett, Tom},
	pages = {38},
}

@article{tenzer_nanoparticle_2011,
	title = {Nanoparticle {Size} {Is} a {Critical} {Physicochemical} {Determinant} of the {Human} {Blood} {Plasma} {Corona}: {A} {Comprehensive} {Quantitative} {Proteomic} {Analysis}},
	volume = {5},
	issn = {1936-0851},
	shorttitle = {Nanoparticle {Size} {Is} a {Critical} {Physicochemical} {Determinant} of the {Human} {Blood} {Plasma} {Corona}},
	url = {https://doi.org/10.1021/nn201950e},
	doi = {10.1021/nn201950e},
	abstract = {In biological fluids, proteins associate with nanoparticles, leading to a protein “corona” defining the biological identity of the particle. However, a comprehensive knowledge of particle-guided protein fingerprints and their dependence on nanomaterial properties is incomplete. We studied the long-lived (“hard”) blood plasma derived corona on monodispersed amorphous silica nanoparticles differing in size (20, 30, and 100 nm). Employing label-free liquid chromatography mass spectrometry, one- and two-dimensional gel electrophoresis, and immunoblotting the composition of the protein corona was analyzed not only qualitatively but also quantitatively. Detected proteins were bioinformatically classified according to their physicochemical and biological properties. Binding of the 125 identified proteins did not simply reflect their relative abundance in the plasma but revealed an enrichment of specific lipoproteins as well as proteins involved in coagulation and the complement pathway. In contrast, immunoglobulins and acute phase response proteins displayed a lower affinity for the particles. Protein decoration of the negatively charged particles did not correlate with protein size or charge, demonstrating that electrostatic effects alone are not the major driving force regulating the nanoparticle–protein interaction. Remarkably, even differences in particle size of only 10 nm significantly determined the nanoparticle corona, although no clear correlation with particle surface volume, protein size, or charge was evident. Particle size quantitatively influenced the particle’s decoration with 37\% of all identified proteins, including (patho)biologically relevant candidates. We demonstrate the complexity of the plasma corona and its still unresolved physicochemical regulation, which need to be considered in nanobioscience in the future.},
	number = {9},
	urldate = {2018-10-21},
	journal = {ACS Nano},
	author = {Tenzer, Stefan and Docter, Dominic and Rosfa, Susanne and Wlodarski, Alexandra and Kuharev, Jörg and Rekik, Alexander and Knauer, Shirley K. and Bantz, Christoph and Nawroth, Thomas and Bier, Carolin and Sirirattanapan, Jarinratn and Mann, Wolf and Treuel, Lennart and Zellner, Reinhard and Maskos, Michael and Schild, Hansjörg and Stauber, Roland H.},
	month = sep,
	year = {2011},
	pages = {7155--7167},
}

@article{salvati_transferrin-functionalized_2013,
	title = {Transferrin-functionalized nanoparticles lose their targeting capabilities when a biomolecule corona adsorbs on the surface},
	volume = {8},
	issn = {1748-3387, 1748-3395},
	url = {http://www.nature.com/articles/nnano.2012.237},
	doi = {10.1038/nnano.2012.237},
	language = {en},
	number = {2},
	urldate = {2018-10-09},
	journal = {Nature Nanotechnology},
	author = {Salvati, Anna and Pitek, Andrzej S. and Monopoli, Marco P. and Prapainop, Kanlaya and Bombelli, Francesca Baldelli and Hristov, Delyan R. and Kelly, Philip M. and Åberg, Christoffer and Mahon, Eugene and Dawson, Kenneth A.},
	month = feb,
	year = {2013},
	pages = {137--143},
}

@article{rust_sub-diffraction-limit_2006,
	title = {Sub-diffraction-limit imaging by stochastic optical reconstruction microscopy ({STORM})},
	volume = {3},
	copyright = {2006 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth929},
	doi = {10.1038/nmeth929},
	abstract = {We have developed a high-resolution fluorescence microscopy method based on high-accuracy localization of photoswitchable fluorophores. In each imaging cycle, only a fraction of the fluorophores were turned on, allowing their positions to be determined with nanometer accuracy. The fluorophore positions obtained from a series of imaging cycles were used to reconstruct the overall image. We demonstrated an imaging resolution of 20 nm. This technique can, in principle, reach molecular-scale resolution.},
	language = {en},
	number = {10},
	urldate = {2018-10-20},
	journal = {Nature Methods},
	author = {Rust, Michael J. and Bates, Mark and Zhuang, Xiaowei},
	month = oct,
	year = {2006},
	pages = {793--796},
}

@article{dempsey_evaluation_2011,
	title = {Evaluation of fluorophores for optimal performance in localization-based super-resolution imaging},
	volume = {8},
	copyright = {2011 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1768},
	doi = {10.1038/nmeth.1768},
	abstract = {One approach to super-resolution fluorescence imaging uses sequential activation and localization of individual fluorophores to achieve high spatial resolution. Essential to this technique is the choice of fluorescent probes; the properties of the probes, including photons per switching event, on-off duty cycle, photostability and number of switching cycles, largely dictate the quality of super-resolution images. Although many probes have been reported, a systematic characterization of the properties of these probes and their impact on super-resolution image quality has been described in only a few cases. Here we quantitatively characterized the switching properties of 26 organic dyes and directly related these properties to the quality of super-resolution images. This analysis provides guidelines for characterization of super-resolution probes and a resource for selecting probes based on performance. Our evaluation identified several photoswitchable dyes with good to excellent performance in four independent spectral ranges, with which we demonstrated low–cross-talk, four-color super-resolution imaging.},
	language = {en},
	number = {12},
	urldate = {2018-02-21},
	journal = {Nature Methods},
	author = {Dempsey, Graham T. and Vaughan, Joshua C. and Chen, Kok Hao and Bates, Mark and Zhuang, Xiaowei},
	month = dec,
	year = {2011},
	pages = {1027--1036},
}

@article{schoch_lipid_2018,
	title = {Lipid diffusion in the distal and proximal leaflets of supported lipid bilayer membranes studied by single particle tracking},
	volume = {148},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.5010341},
	doi = {10.1063/1.5010341},
	language = {en},
	number = {12},
	urldate = {2018-05-21},
	journal = {The Journal of Chemical Physics},
	author = {Schoch, Rafael L. and Barel, Itay and Brown, Frank L. H. and Haran, Gilad},
	month = mar,
	year = {2018},
	pages = {123333},
}

@article{preus_optimal_2016,
	title = {Optimal {Background} {Estimators} in {Single}-{Molecule} {FRET} {Microscopy}},
	volume = {111},
	issn = {0006-3495},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5034361/},
	doi = {10.1016/j.bpj.2016.07.047},
	abstract = {Single-molecule total internal reflection fluorescence (TIRF) microscopy constitutes an umbrella of powerful tools that facilitate direct observation of the biophysical properties, population heterogeneities, and interactions of single biomolecules without the need for ensemble synchronization. Due to the low signal/noise ratio in single-molecule TIRF microscopy experiments, it is important to determine the local background intensity, especially when the fluorescence intensity of the molecule is used quantitatively. Here we compare and evaluate the performance of different aperture-based background estimators used particularly in single-molecule Förster resonance energy transfer. We introduce the general concept of multiaperture signatures and use this technique to demonstrate how the choice of background can affect the measured fluorescence signal considerably. A new, to our knowledge, and simple background estimator is proposed, called the local statistical percentile (LSP). We show that the LSP background estimator performs as well as current background estimators at low molecular densities and significantly better in regions of high molecular densities. The LSP background estimator is thus suited for single-particle TIRF microscopy of dense biological samples in which the intensity itself is an observable of the technique.},
	number = {6},
	urldate = {2018-03-21},
	journal = {Biophysical Journal},
	author = {Preus, Søren and Hildebrandt, Lasse L. and Birkedal, Victoria},
	month = sep,
	year = {2016},
	pmid = {27653486},
	pmcid = {PMC5034361},
	pages = {1278--1286},
}

@book{barlow_statistics:_1989,
	address = {Chichester, England ; New York},
	series = {The {Manchester} physics series},
	title = {Statistics: {A} {Guide} to the {Use} of {Statistical} {Methods} in the {Physical} {Sciences}},
	isbn = {978-0-471-92294-0 978-0-471-92295-7},
	shorttitle = {Statistics},
	url = {https://www.wiley.com/en-us/Statistics%3A+A+Guide+to+the+Use+of+Statistical+Methods+in+the+Physical+Sciences-p-9780471922957},
	abstract = {The Manchester Physics Series General Editors: D. J. Sandiford; F.  Mandl; A. C. Phillips Department of Physics and Astronomy,  University of Manchester Properties of Matter B. H. Flowers and E.  Mendoza Optics Second Edition F. G. Smith and J. H. Thomson  Statistical Physics Second Edition F. Mandl Electromagnetism Second  Edition I. S. Grant and W. R. Phillips Statistics R. J. Barlow  Solid State Physics Second Edition J. R. Hook and H. E. Hall  Quantum Mechanics F. Mandl Particle Physics Second Edition B. R.  Martin and G. Shaw The Physics of Stars Second Edition A.C.  Phillips Computing for Scientists R. J. Barlow and A. R. Barnett  Written by a physicist, Statistics is tailored to the needs of  physical scientists, containing and explaining all they need to  know. It concentrates on parameter estimation, especially the  methods of Least Squares and Maximum Likelihood, but other  techniques, such as hypothesis testing, Bayesian statistics and  non-parametric methods are also included. Intended for reasonably  numerate scientists it contains all the basic formulae, their  derivations and applications, together with some more advanced  ones. Statistics features:  * Comprehensive coverage of the essential techniques physical  scientists are likely to need.  * A wealth of examples, and problems with their answers.  * Flexible structure and organisation allows it to be used as a  course text and a reference.  * A review of the basics, so that little prior knowledge is  required.},
	language = {en-us},
	urldate = {2018-06-05},
	publisher = {Wiley},
	author = {Barlow, Roger J.},
	year = {1989},
	keywords = {Statistical physics},
}

@article{granados-miralles_unraveling_2016,
	title = {Unraveling structural and magnetic information during growth of nanocrystalline {SrFe} $_{\textrm{12}}$ {O} $_{\textrm{19}}$},
	volume = {4},
	issn = {2050-7526, 2050-7534},
	url = {http://xlink.rsc.org/?DOI=C6TC03803D},
	doi = {10.1039/C6TC03803D},
	language = {en},
	number = {46},
	urldate = {2018-12-13},
	journal = {Journal of Materials Chemistry C},
	author = {Granados-Miralles, Cecilia and Saura-Múzquiz, Matilde and Bøjesen, Espen D. and Jensen, Kirsten M. Ø. and Andersen, Henrik L. and Christensen, Mogens},
	year = {2016},
	pages = {10903--10913},
}

@article{saura-muzquiz_nanoengineered_2018,
	title = {Nanoengineered {High}-{Performance} {Hexaferrite} {Magnets} by {Morphology}-{Induced} {Alignment} of {Tailored} {Nanoplatelets}},
	issn = {2574-0970, 2574-0970},
	url = {http://pubs.acs.org/doi/10.1021/acsanm.8b01748},
	doi = {10.1021/acsanm.8b01748},
	abstract = {Magnetic materials are ubiquitous in electric devices and motors making them indispensable for modern-day society. The hexaferrites currently constitute the most widely used permanent magnets (PMs), accounting for 85\% (by weight) of the global sales of PMs. This work presents a complete bottom-up nanostructuring protocol for preparation of magnetically aligned, high-performance hexaferrite PMs with a record-high (BH)max for dry-processed ferrites. The procedure includes the supercritical hydrothermal ﬂow synthesis of anisotropic magnetic-single-domain strontium hexaferrite (SrFe12O19) nanocrystallites of various sizes, and their subsequent compaction into bulk magnets by spark plasma sintering (SPS). Interestingly, Rietveld modeling of neutron powder diﬀraction data reveals a signiﬁcant diﬀerence between the magnetic structure of the thinnest nanoplatelets and the bulk compound, indicating the Sr-containing atomic layer to be the termination layer. Subsequently, high-density SrFe12O19 magnets ({\textgreater}95\% of the theoretical density) are produced by SPS of the ﬂow-synthesized nanoplatelets. Texture analysis by X-ray pole ﬁgure measurements demonstrates how the anisotropic shape of the nanoplatelets causes a self-induced alignment during SPS, without application of an external magnetic ﬁeld. The self-induced texture is accompanied by crystallite growth along the magnetic easy-axis, i.e., the thickness of the platelets, resulting in high-performance PMs with square hysteresis curves and (BH)max of 30 kJ/m3. The (BH)max is further enhanced by annealing, reaching 36 kJ/m3 after 4 h at 850 °C, which exceeds the (BH)max of the highest grade of dry-processed commercial ferrites worldwide.},
	language = {en},
	urldate = {2018-12-13},
	journal = {ACS Applied Nano Materials},
	author = {Saura-Múzquiz, Matilde and Granados-Miralles, Cecilia and Andersen, Henrik L. and Stingaciu, Marian and Avdeev, Maxim and Christensen, Mogens},
	month = nov,
	year = {2018},
}

@article{fischer_structural_2011,
	title = {Structural purity of magnetite nanoparticles in magnetotactic bacteria},
	volume = {8},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/cgi/doi/10.1098/rsif.2010.0576},
	doi = {10.1098/rsif.2010.0576},
	language = {en},
	number = {60},
	urldate = {2018-12-13},
	journal = {Journal of The Royal Society Interface},
	author = {Fischer, A. and Schmitz, M. and Aichmayer, B. and Fratzl, P. and Faivre, D.},
	month = jul,
	year = {2011},
	pages = {1011--1018},
}

@article{li_magnetic_2016,
	title = {Magnetic {Properties} of {Strontium} {Hexaferrite} {Nanostructures} {Measured} with {Magnetic} {Force} {Microscopy}},
	volume = {6},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep25985},
	doi = {10.1038/srep25985},
	language = {en},
	number = {1},
	urldate = {2018-12-13},
	journal = {Scientific Reports},
	author = {Li, Qiang and Song, Jie and Saura-Múzquiz, Matilde and Besenbacher, Flemming and Christensen, Mogens and Dong, Mingdong},
	month = sep,
	year = {2016},
}

@inproceedings{kotake_selective_2011,
	title = {Selective kinesin and dynein immobilizaton and electrical microtubule manipulation for bidirectional microtubule motility},
	doi = {10.1109/MEMSYS.2011.5734690},
	abstract = {We present a method to make microtubules (MTs) glide bidirectionally depending on their polarities in a closed chamber, in which kinesin and dynein are coated on top and bottom glass surfaces, respectively. Once top Au-coated and bottom ITO-coated glass coverslip were assembled to a flow chamber, surface modification and sequential protein injections realized selective dynein and kinesin coatings. Therefore, plus ends of MTs lead on the top dynein/Au-coated surface, and minus ends lead on the bottom kinesin/ITO-coated surface. These gliding MTs were removed one surface and collected to the other surface by applying voltage for controlling MT gliding directions. These fundamental techniques are essential to drive MTs back and forth in a microfluidic system.},
	booktitle = {2011 {IEEE} 24th {International} {Conference} on {Micro} {Electro} {Mechanical} {Systems}},
	author = {Kotake, H. and Yokokawa, R. and Kanno, I. and Kotera, H.},
	month = jan,
	year = {2011},
	keywords = {Au-coated glass, Coatings, Electrodes, Glass, Proteins, Substrates, Surface treatment, Voltage control, bidirectional microtubule motility, bioMEMS, bottom ITO-coated glass coverslip, cell motility, dynein immobilizaton, electrical microtubule manipulation, flow chamber, microfluidic system, microfluidics, microtubule gliding, molecular biophysics, proteins, selective kinesin immobilizaton, sequential protein injections, surface modification},
	pages = {1373--1376},
}

@article{korten_automated_2018,
	title = {An automated \textit{in vitro} motility assay for high-throughput studies of molecular motors},
	volume = {18},
	issn = {1473-0197, 1473-0189},
	url = {http://xlink.rsc.org/?DOI=C8LC00547H},
	doi = {10.1039/C8LC00547H},
	language = {en},
	number = {20},
	urldate = {2018-12-03},
	journal = {Lab on a Chip},
	author = {Korten, Till and Tavkin, Elena and Scharrel, Lara and Kushwaha, Vandana Singh and Diez, Stefan},
	year = {2018},
	pages = {3196--3206},
}

@article{subramaniyan_parimalam_microtubule_2017,
	title = {Microtubule density and landing rate as parameters to analyze tau protein in the {MT}-kinesin “gliding” assay},
	volume = {238},
	issn = {0925-4005},
	url = {http://www.sciencedirect.com/science/article/pii/S0925400516311170},
	doi = {10.1016/j.snb.2016.07.082},
	abstract = {Microtubule-associated protein (MAP) tau is a well-established hallmark of a large group of age related neurodegenerative diseases collectively called tauopathies. Under pathological conditions the equilibrium of tau binding to the MTs is perturbed, either by misregulation in the expression levels of specific tau isoforms or by MAPT gene mutations. Preclinical detection of such misregulated tau proteins in cerebrospinal fluid (CSF) is desirable for differential diagnosis and effective prognosis of neurodegeneration. Conventional tau protein detection methods utilize tau isoform-specific antibodies. Such immuno-based protocols, including enzyme-linked immunosorbent assay (ELISA) and Western blots have appropriate sensitivity and specificity, but often show high variability and are time consuming. Here, we established a non-immuno tau protein detection method utilizing microtubule (MT)-kinesin “gliding”assay. All the six tau isoforms expressed in the human brain (0N3R, 1N3R, 2N3R, 0N4R, 1N4R and 2N4R) and five MAPT gene mutants (V248L, G272V, P301L, V337M and R406W) were studied. The landing rate, binding density and gliding velocity of MTs with respect to each tau type were determined and are proposed as tau detection parameters. The detection parameters depicted the type of tau bound to the MTs. Furthermore, MT landing rate and density were found to be superior to gliding velocity in differentiating tau isoforms and mutants. The 3R vs. 4R isoforms, their admixtures, wild vs. mutant 2N4R and specific mutants were differentiated. Our data show that MT-kinesin gliding assay provides a convenient, lab-on-a-chip (LOC) compatible and antibody-free protocol for tau protein analysis.},
	urldate = {2018-12-03},
	journal = {Sensors and Actuators B: Chemical},
	author = {Subramaniyan Parimalam, Subhathirai and Tarhan, Mehmet C. and Karsten, Stanislav L. and Fujita, Hiroyuki and Shintaku, Hirofumi and Kotera, Hidetoshi and Yokokawa, Ryuji},
	month = jan,
	year = {2017},
	keywords = {Tauopathies, biomarker, gliding assay, kinesin, microtubules, tau detection},
	pages = {954--961},
}

@article{amrutha_targeted_2017,
	title = {Targeted {Activation} of {Molecular} {Transportation} by {Visible} {Light}},
	volume = {11},
	issn = {1936-0851},
	url = {https://doi.org/10.1021/acsnano.7b06059},
	doi = {10.1021/acsnano.7b06059},
	abstract = {Regulated transportation of nanoscale objects with a high degree of spatiotemporal precision is a prerequisite for the development of targeted molecular delivery. In vitro integration of the kinesin-microtubule motor system with synthetic molecules offers opportunities to develop controllable molecular shuttles for lab-on-a-chip applications. We attempted a combination of the kinesin-microtubule motor system with push–pull type azobenzene tethered inhibitory peptides (azo-peptides) through which reversible, spatiotemporal control over the kinesin motor activity was achieved locally by a single, visible wavelength. The fast thermal relaxation of the cis-isomers of azo-peptides offered us quick and complete resetting of the trans-state in the dark, circumventing the requirement of two distinct wavelengths for two-way switching of kinesin-driven microtubule motility. Herein, we report the manipulation of selected, single microtubule movement while keeping other microtubules at complete rest. The photoresponsive inhibitors discussed herein would help in realizing complex bionanodevices.},
	number = {12},
	urldate = {2018-12-03},
	journal = {ACS Nano},
	author = {Amrutha, Ammathnadu S. and Kumar, K. R. Sunil and Kikukawa, Takashi and Tamaoki, Nobuyuki},
	month = dec,
	year = {2017},
	pages = {12292--12301},
}

@article{itel_matrix_2018,
	title = {Matrix {Vesicles}-{Containing} {Microreactors} as {Support} for {Bonelike} {Osteoblasts} to {Enhance} {Biomineralization}},
	volume = {10},
	issn = {1944-8244, 1944-8252},
	url = {http://pubs.acs.org/doi/10.1021/acsami.8b10886},
	doi = {10.1021/acsami.8b10886},
	abstract = {Therapeutic cell mimicry aims to provide a source of cell-like assemblies, which exhibit the core structural or functional properties of their natural counterparts with broad envisioned applications in biomedicine. Bone tissue engineering (BTE) aims at promoting and inciting the natural healing process of, for instance, critically sized bone defects. Microreactors designed to co-assemble with biological boneforming osteoblasts like SaOS-2 cells to start biomineralization are reported for the ﬁrst time. The alginate-based microparticles are equipped with active alkaline phosphatase-loaded artiﬁcial liposomes or SaOS-2-derived matrix vesicles (MVs). Spheroids assembled from SaOS-2 cells and microreactors not only exhibit higher cell viability, but also show enhanced biomineralization when MVs are present. The active biomineralization stimulation of the microreactors is illustrated by colorimetric calcium quantiﬁcation and micro-computed tomography. These ﬁndings show the promising potential of applying cell mimicry in BTE.},
	language = {en},
	number = {36},
	urldate = {2018-12-03},
	journal = {ACS Applied Materials \& Interfaces},
	author = {Itel, Fabian and Skovhus Thomsen, Jesper and Städler, Brigitte},
	month = sep,
	year = {2018},
	pages = {30180--30190},
}

@article{tanner_aiding_2013,
	title = {Aiding {Nature}’s {Organelles}: {Artificial} {Peroxisomes} {Play} {Their} {Role}},
	volume = {13},
	issn = {1530-6984, 1530-6992},
	shorttitle = {Aiding {Nature}’s {Organelles}},
	url = {http://pubs.acs.org/doi/10.1021/nl401215n},
	doi = {10.1021/nl401215n},
	abstract = {A major goal in medical research is to develop artiﬁcial organelles that can implant in cells to treat pathological conditions or to support the design of artiﬁcial cells. Several attempts have been made to encapsulate or entrap enzymes, proteins, or mimics in polymer compartments, but only few of these nanoreactors were active in cells, and none was proven to mimic a speciﬁc natural organelle. Here, we show the necessary steps for the development of an artiﬁcial organelle mimicking a natural organelle, the peroxisome. The system, based on two enzymes that work in tandem in polymer vesicles, with a membrane rendered permeable by inserted channel proteins was optimized in terms of natural peroxisome properties and function. The uptake, absence of toxicity, and in situ activity in cells exposed to oxidative stress demonstrated that the artiﬁcial peroxisomes detoxify superoxide radicals and H2O2 after endosomal escape. Our artiﬁcial peroxisome combats oxidative stress in cells, a factor in various pathologies (e.g., arthritis, Parkinson’s, cancer, AIDS), and oﬀers a versatile strategy to develop other “cell implants” for cell dysfunction.},
	language = {en},
	number = {6},
	urldate = {2018-12-03},
	journal = {Nano Letters},
	author = {Tanner, Pascal and Balasubramanian, Vimalkumar and Palivan, Cornelia G.},
	month = jun,
	year = {2013},
	pages = {2875--2883},
}

@article{prasad_multifunctional_2014,
	title = {Multifunctional {Albumin}–{MnO} $_{\textrm{2}}$ {Nanoparticles} {Modulate} {Solid} {Tumor} {Microenvironment} by {Attenuating} {Hypoxia}, {Acidosis}, {Vascular} {Endothelial} {Growth} {Factor} and {Enhance} {Radiation} {Response}},
	volume = {8},
	issn = {1936-0851, 1936-086X},
	url = {http://pubs.acs.org/doi/10.1021/nn405773r},
	doi = {10.1021/nn405773r},
	abstract = {Insuﬃcient oxygenation (hypoxia), acidic pH (acidosis), and elevated levels of reactive oxygen species (ROS), such as H2O2, are characteristic abnormalities of the tumor microenvironment (TME). These abnormalities promote tumor aggressiveness, metastasis, and resistance to therapies. To date, there is no treatment available for comprehensive modulation of the TME. Approaches so far have been limited to regulating hypoxia, acidosis, or ROS individually, without accounting for their interdependent eﬀects on tumor progression and response to treatments. Hence we have engineered multifunctional and colloidally stable bioinorganic nanoparticles composed of polyelectrolyteÀalbumin complex and MnO2 nanoparticles (A-MnO2 NPs) and utilized the reactivity of MnO2 toward peroxides for regulation of the TME with simultaneous oxygen generation and pH increase. In vitro studies showed that these NPs can generate oxygen by reacting with H2O2 produced by cancer cells under hypoxic conditions. A-MnO2 NPs simultaneously increased tumor oxygenation by 45\% while increasing tumor pH from pH 6.7 to pH 7.2 by reacting with endogenous H2O2 produced within the tumor in a murine breast tumor model. Intratumoral treatment with NPs also led to the downregulation of two major regulators in tumor progression and aggressiveness, that is, hypoxia-inducible factor-1 alpha and vascular endothelial growth factor in the tumor. Combination treatment of the tumors with NPs and ionizing radiation signiﬁcantly inhibited breast tumor growth, increased DNA double strand breaks and cancer cell death as compared to radiation therapy alone. These results suggest great potential of A-MnO2 NPs for modulation of the TME and enhancement of radiation response in the treatment of cancer.},
	language = {en},
	number = {4},
	urldate = {2018-12-03},
	journal = {ACS Nano},
	author = {Prasad, Preethy and Gordijo, Claudia R. and Abbasi, Azhar Z. and Maeda, Azusa and Ip, Angela and Rauth, Andrew Michael and DaCosta, Ralph S. and Wu, Xiao Yu},
	month = apr,
	year = {2014},
	pages = {3202--3212},
}

@article{korten_selective_2012,
	title = {Selective {Control} of {Gliding} {Microtubule} {Populations}},
	volume = {12},
	issn = {1530-6984, 1530-6992},
	url = {http://pubs.acs.org/doi/10.1021/nl203632y},
	doi = {10.1021/nl203632y},
	abstract = {First lab-on-chip devices based on active transport by biomolecular motors have been demonstrated for basic detection and sorting applications. However, to fully employ the advantages of such hybrid nanotechnology, versatile spatial and temporal control mechanisms are required. Using a thermo-responsive polymer, we demonstrate the selective starting and stopping of modified microtubules gliding on a kinesin-1-coated surface. This approach allows the self-organized separation of multiple microtubule populations and their respective cargoes.},
	language = {en},
	number = {1},
	urldate = {2018-12-03},
	journal = {Nano Letters},
	author = {Korten, Till and Birnbaum, Wolfgang and Kuckling, Dirk and Diez, Stefan},
	month = jan,
	year = {2012},
	pages = {348--353},
}

@article{moyer_shape-dependent_2015,
	title = {Shape-{Dependent} {Targeting} of {Injured} {Blood} {Vessels} by {Peptide} {Amphiphile} {Supramolecular} {Nanostructures}},
	volume = {11},
	issn = {16136810},
	url = {http://doi.wiley.com/10.1002/smll.201403429},
	doi = {10.1002/smll.201403429},
	language = {en},
	number = {23},
	urldate = {2018-12-03},
	journal = {Small},
	author = {Moyer, Tyson J. and Kassam, Hussein A. and Bahnson, Edward S. M. and Morgan, Courtney E. and Tantakitti, Faifan and Chew, Teng L. and Kibbe, Melina R. and Stupp, Samuel I.},
	month = jun,
	year = {2015},
	pages = {2750--2755},
}

@article{abdelmohsen_dynamic_2016,
	title = {Dynamic {Loading} and {Unloading} of {Proteins} in {Polymeric} {Stomatocytes}: {Formation} of an {Enzyme}-{Loaded} {Supramolecular} {Nanomotor}},
	volume = {10},
	issn = {1936-0851, 1936-086X},
	shorttitle = {Dynamic {Loading} and {Unloading} of {Proteins} in {Polymeric} {Stomatocytes}},
	url = {http://pubs.acs.org/doi/10.1021/acsnano.5b07689},
	doi = {10.1021/acsnano.5b07689},
	abstract = {Self-powered artiﬁcial nanomotors are currently attracting increased interest as mimics of biological motors but also as potential components of nanomachinery, robotics, and sensing devices. We have recently described the controlled shape transformation of polymersomes into bowl-shaped stomatocytes and the assembly of platinum-driven nanomotors. However, the platinum encapsulation inside the structures was low; only 50\% of the structures contained the catalyst and required both high fuel concentrations for the propelling of the nanomotors and harsh conditions for the shape transformation. Application of the nanomotors in a biological setting requires the nanomotors to be eﬃciently propelled by a naturally available energy source and at biological relevant concentrations. Here we report a strategy for enzyme entrapment and nanomotor assembly via controlled and reversible folding of polymersomes into stomatocytes under mild conditions, allowing the encapsulation of the proteins inside the stomach with almost 100\% eﬃciency and retention of activity. The resulting enzyme-driven nanomotors are capable of propelling these structures at low fuel concentrations (hydrogen peroxide or glucose) via a one-enzyme or two-enzyme system. The conﬁnement of the enzymes inside the stomach does not hinder their activity and in fact facilitates the transfer of the substrates, while protecting them from the deactivating inﬂuences of the media. This is particularly important for future applications of nanomotors in biological settings especially for systems where fast autonomous movement occurs at physiological concentrations of fuel.},
	language = {en},
	number = {2},
	urldate = {2018-12-03},
	journal = {ACS Nano},
	author = {Abdelmohsen, Loai K. E. A. and Nijemeisland, Marlies and Pawar, Gajanan M. and Janssen, Geert-Jan A. and Nolte, Roeland J. M. and van Hest, Jan C. M. and Wilson, Daniela A.},
	month = feb,
	year = {2016},
	pages = {2652--2660},
}

@article{nooney_synthesis_2002,
	title = {Synthesis of nanoscale mesoporous silica spheres with controlled particle size},
	volume = {14},
	issn = {0897-4756},
	doi = {10.1021/cm0204371},
	abstract = {A simple one-step procedure is described for the synthesis of spherical mesoporous silica, in which the size of the particles is controlled over a range of diameters from 65 to 740 nm by varying the initial silicate/surfactant concentration under dilute conditions. The particles were characterized using X-ray diffraction, transmission electron microscopy, and liquid nitrogen adsorption. Synthesis using a charged template, cetyltrimethylammonium bromide, under aqueous conditions yielded particles of irregular spherical shape with highly ordered mesoporous channels. Synthesis under ethanol/water cosolvent conditions yielded smooth spheres with a starburst mesopore structure extending from the center of the particle to the circumference. All materials were thermally stable and exhibited two steps in their liquid nitrogen isotherms corresponding to reversible channel filling and non-reversible adsorption between particles. Mesopore volumes varied from 0.64 to 0.93 cm(3) g(-1) and surface areas varied from 917 to 1373 m(2) g(-1). From analysis of mesopore geometry and overall particle shape a three-stage mechanism for synthesis is proposed.},
	language = {English},
	number = {11},
	journal = {Chemistry of Materials},
	author = {Nooney, R. I. and Thirunavukkarasu, D. and Chen, Y. M. and Josephs, R. and Ostafin, A. E.},
	month = nov,
	year = {2002},
	note = {WOS:000179377400039},
	keywords = {mcm-41, molecular-sieves, morphology, nanoparticles, organization, pore-size, porosity, spectroscopy, surfactant, systems},
	pages = {4721--4728},
}

@misc{noauthor_page_nodate,
	title = {Page not found · {GitHub}},
	url = {https://github.com/eembees/mbsms},
	urldate = {2018-10-22},
}

@article{grubbs_sample_1950,
	title = {Sample {Criteria} for {Testing} {Outlying} {Observations}},
	volume = {21},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177729885},
	doi = {10.1214/aoms/1177729885},
	abstract = {The problem of testing outlying observations, although an old one, is of considerable importance in applied statistics. Many and various types of significance tests have been proposed by statisticians interested in this field of application. In this connection, we bring out in the Histrical Comments notable advances toward a clear formulation of the problem and important points which should be considered in attempting a complete solution. In Section 4 we state some of the situations the experimental statistician will very likely encounter in practice, these considerations being based on experience. For testing the significance of the largest observation in a sample of size nnn from a normal population, we propose the statistic S2nS2=∑n−1i=1(xi−x¯n)2∑ni=1(xi−x¯)2Sn2S2=∑i=1n−1(xi−x¯n)2∑i=1n(xi−x¯)2{\textbackslash}frac\{S{\textasciicircum}2\_n\}\{S{\textasciicircum}2\} = {\textbackslash}frac\{{\textbackslash}sum{\textasciicircum}\{n-1\}\_\{i=1\} (x\_i - {\textbackslash}bar x\_n){\textasciicircum}2\}\{{\textbackslash}sum{\textasciicircum}n\_\{i=1\} (x\_i - {\textbackslash}bar x){\textasciicircum}2\} where x1≤x2≤⋯≤xn,x¯n=1n−1∑n−1i=1xix1≤x2≤⋯≤xn,x¯n=1n−1∑i=1n−1xix\_1 {\textbackslash}leq x\_2 {\textbackslash}leq {\textbackslash}cdots {\textbackslash}leq x\_n, {\textbackslash}bar x\_n = {\textbackslash}frac\{1\}\{n - 1\} {\textbackslash}sum{\textasciicircum}\{n-1\}\_\{i=1\} x\_i and x¯=1n∑ni=1xi.x¯=1n∑i=1nxi.{\textbackslash}bar x = {\textbackslash}frac\{1\}\{n\}{\textbackslash}sum{\textasciicircum}\{n\}\_\{i=1\} x\_i. A similar statistic, S21/S2S12/S2S{\textasciicircum}2\_1/S{\textasciicircum}2, can be used for testing whether the smallest observation is too low. It turns out that S2nS2=1−1n−1(xn−x¯s)2=1−1n−1T2n,Sn2S2=1−1n−1(xn−x¯s)2=1−1n−1Tn2,{\textbackslash}frac\{S{\textasciicircum}2\_n\}\{S{\textasciicircum}2\} = 1 - {\textbackslash}frac\{1\}\{n - 1\} {\textbackslash}big({\textbackslash}frac\{x\_n - {\textbackslash}bar x\}\{s\}{\textbackslash}big){\textasciicircum}2 = 1 - {\textbackslash}frac\{1\}\{n - 1\} T{\textasciicircum}2\_n, where s2=1nσ(xi−x¯)2,s2=1nσ(xi−x¯)2,s{\textasciicircum}2 = {\textbackslash}frac\{1\}\{n\}{\textbackslash}sigma(x\_i - {\textbackslash}bar x){\textasciicircum}2, and TnTnT\_n is the studentized extreme deviation already suggested by E. Pearson and C. Chandra Sekar [1] for testing the significance of the largest observation. Based on previous work by W. R. Thompson [12], Pearson and Chandra Sekar were able to obtain certain percentage points of TnTnT\_n without deriving the exact distribution of TnTnT\_n. The exact distribution of S2n/S2Sn2/S2S{\textasciicircum}2\_n/S{\textasciicircum}2 (or TnTnT\_n) is apparently derived for the first time by the present author. For testing whether the two largest observations are too large we propose the statistic S2n−1,nS2=∑n−2i=1(xi−x¯n−1,n)2∑ni=1(xi−x¯)2,x¯n−1,n=1n−2∑n−2i=1xiSn−1,n2S2=∑i=1n−2(xi−x¯n−1,n)2∑i=1n(xi−x¯)2,x¯n−1,n=1n−2∑i=1n−2xi{\textbackslash}frac\{S{\textasciicircum}2\_\{n-1,n\}\}\{S{\textasciicircum}2\} = {\textbackslash}frac\{{\textbackslash}sum{\textasciicircum}\{n-2\}\_\{i=1\} (x\_i - {\textbackslash}bar x\_\{n-1,n\}){\textasciicircum}2\}\{{\textbackslash}sum{\textasciicircum}n\_\{i=1\} (x\_i - {\textbackslash}bar x){\textasciicircum}2\},{\textbackslash}quad{\textbackslash}bar x\_\{n-1,n\} = {\textbackslash}frac\{1\}\{n - 2\} {\textbackslash}sum{\textasciicircum}\{n-2\}\_\{i=1\} x\_i and a similar statistic, S21,2/S2S1,22/S2S{\textasciicircum}2\_\{1,2\}/S{\textasciicircum}2, can be used to test the significance of the two smallest observations. The probability distributions of the above sample statistics S2=∑ni=1(xi−x¯)2wherex¯=1n∑ni=1xiS2=∑i=1n(xi−x¯)2wherex¯=1n∑i=1nxiS{\textasciicircum}2 = {\textbackslash}sum{\textasciicircum}n\_\{i=1\} (x\_i - {\textbackslash}bar x){\textasciicircum}2 {\textbackslash}text\{where\} {\textbackslash}bar x = {\textbackslash}frac\{1\}\{n\} {\textbackslash}sum{\textasciicircum}n\_\{i=1\} x\_i S2n=∑n−1i=1(xi−x¯n)2wherex¯n=1n−1∑n−1i=1xiSn2=∑i=1n−1(xi−x¯n)2wherex¯n=1n−1∑i=1n−1xiS{\textasciicircum}2\_n = {\textbackslash}sum{\textasciicircum}\{n-1\}\_\{i=1\} (x\_i - {\textbackslash}bar x\_n){\textasciicircum}2 {\textbackslash}text\{where\} {\textbackslash}bar x\_n = {\textbackslash}frac\{1\}\{n-1\} {\textbackslash}sum{\textasciicircum}\{n-1\}\_\{i=1\} x\_i S21=∑ni=2(xi−x¯1)2wherex¯1=1n−1∑ni=2xiS12=∑i=2n(xi−x¯1)2wherex¯1=1n−1∑i=2nxiS{\textasciicircum}2\_1 = {\textbackslash}sum{\textasciicircum}n\_\{i=2\} (x\_i - {\textbackslash}bar x\_1){\textasciicircum}2 {\textbackslash}text\{where\} {\textbackslash}bar x\_1 = {\textbackslash}frac\{1\}\{n-1\} {\textbackslash}sum{\textasciicircum}n\_\{i=2\} x\_i are derived for a normal parent and tables of appropriate percentage points are given in this paper (Table I and Table V). Although the efficiencies of the above tests have not been completely investigated under various models for outlying observations, it is apparent that the proposed sample criteria have considerable intuitive appeal. In deriving the distributions of the sample statistics for testing the largest (or smallest) or the two largest (or two smallest) observations, it was first necessary to derive the distribution of the difference between the extreme observation and the sample mean in terms of the population σσ{\textbackslash}sigma. This probabilityX1≤x2≤x3⋯≤xnX1≤x2≤x3⋯≤xnX\_1 {\textbackslash}leq x\_2 {\textbackslash}leq x\_3 {\textbackslash}cdots {\textbackslash}leq x\_n s2=1n∑ni=1(xi−x¯)2x¯=1n∑ni=1xis2=1n∑i=1n(xi−x¯)2x¯=1n∑i=1nxis{\textasciicircum}2 = {\textbackslash}frac\{1\}\{n\} {\textbackslash}sum{\textasciicircum}n\_\{i=1\} (x\_i - {\textbackslash}bar x){\textasciicircum}2 {\textbackslash}quad {\textbackslash}bar x = {\textbackslash}frac\{1\}\{n\} {\textbackslash}sum{\textasciicircum}n\_\{i=1\} x\_i distribution was apparently derived first by A. T. McKay [11] who employed the method of characteristic functions. The author was not aware of the work of McKay when the simplified derivation for the distribution of xn−x¯σxn−x¯σ{\textbackslash}frac\{x\_n - {\textbackslash}bar x\}\{{\textbackslash}sigma\} outlined in Section 5 below was worked out by him in the spring of 1945, McKay's result being called to his attention by C. C. Craig. It has been noted also that K. R. Nair [20] worked out independently and published the same derivation of the distribution of the extreme minus the mean arrived at by the present author--see Biometrika, Vol. 35, May, 1948. We nevertheless include part of this derivation in Section 5 below as it was basic to the work in connection with the derivations given in Sections 8 and 9. Our table is considerably more extensive than Nair's table of the probability integral of the extreme deviation from the sample mean in normal samples, since Nair's table runs from n=2n=2n = 2 to n=9,n=9,n = 9, whereas our Table II is for n=2n=2n = 2 to n=25n=25n = 25. The present work is concluded with some examples.},
	language = {EN},
	number = {1},
	urldate = {2018-10-22},
	journal = {The Annals of Mathematical Statistics},
	author = {Grubbs, Frank E.},
	month = mar,
	year = {1950},
	mrnumber = {MR33993},
	zmnumber = {0036.21003},
	pages = {27--58},
}

@article{uttamapinant_genetic_2015,
	title = {Genetic {Code} {Expansion} {Enables} {Live}-{Cell} and {Super}-{Resolution} {Imaging} of {Site}-{Specifically} {Labeled} {Cellular} {Proteins}},
	volume = {137},
	issn = {0002-7863, 1520-5126},
	url = {http://pubs.acs.org/doi/10.1021/ja512838z},
	doi = {10.1021/ja512838z},
	abstract = {Methods to site-speciﬁcally and densely label proteins in cellular ultrastructures with small, bright, and photostable ﬂuorophores would substantially advance super-resolution imaging. Recent advances in genetic code expansion and bioorthogonal chemistry have enabled the site-speciﬁc labeling of proteins. However, the eﬃcient incorporation of unnatural amino acids into proteins and the speciﬁc, ﬂuorescent labeling of the intracellular ultrastructures they form for subdiﬀraction imaging has not been accomplished. Two challenges have limited progress in this area: (i) the low eﬃciency of unnatural amino acid incorporation that limits labeling density and therefore spatial resolution and (ii) the uncharacterized speciﬁcity of intracellular labeling that will deﬁne signal-tonoise, and ultimately resolution, in imaging. Here we demonstrate the eﬃcient production of cystoskeletal proteins (β-actin and vimentin) containing bicyclo[6.1.0]nonyne-lysine at genetically deﬁned sites. We demonstrate their selective ﬂuorescent labeling with respect to the proteome of living cells using tetrazine-ﬂuorophore conjugates, creating densely labeled cytoskeletal ultrastructures. STORM imaging of these densely labeled ultrastructures reveals subdiﬀraction features, including nuclear actin ﬁlaments. This work enables the site-speciﬁc, live-cell, ﬂuorescent labeling of intracellular proteins at high density for super-resolution imaging of ultrastructural features within cells.},
	language = {en},
	number = {14},
	urldate = {2018-10-21},
	journal = {Journal of the American Chemical Society},
	author = {Uttamapinant, Chayasith and Howe, Jonathan D. and Lang, Kathrin and Beránek, Václav and Davis, Lloyd and Mahesh, Mohan and Barry, Nicholas P. and Chin, Jason W.},
	month = apr,
	year = {2015},
	pages = {4602--4605},
}

@article{feiner-gracia_super-resolution_2017,
	title = {Super-{Resolution} {Microscopy} {Unveils} {Dynamic} {Heterogeneities} in {Nanoparticle} {Protein} {Corona}},
	volume = {13},
	issn = {16136810},
	url = {http://doi.wiley.com/10.1002/smll.201701631},
	doi = {10.1002/smll.201701631},
	language = {en},
	number = {41},
	urldate = {2018-10-21},
	journal = {Small},
	author = {Feiner-Gracia, Natalia and Beck, Michaela and Pujals, Sílvia and Tosi, Sébastien and Mandal, Tamoghna and Buske, Christian and Linden, Mika and Albertazzi, Lorenzo},
	month = nov,
	year = {2017},
	pages = {1701631},
}

@article{hamad-schifferli_exploiting_2015,
	title = {Exploiting the novel properties of protein coronas: emerging applications in nanomedicine},
	volume = {10},
	issn = {1743-5889, 1748-6963},
	shorttitle = {Exploiting the novel properties of protein coronas},
	url = {https://www.futuremedicine.com/doi/10.2217/nnm.15.6},
	doi = {10.2217/nnm.15.6},
	language = {en},
	number = {10},
	urldate = {2018-10-21},
	journal = {Nanomedicine},
	author = {Hamad-Schifferli, Kimberly},
	month = may,
	year = {2015},
	pages = {1663--1674},
}

@article{monopoli_biomolecular_2012,
	title = {Biomolecular coronas provide the biological identity of nanosized materials},
	volume = {7},
	issn = {1748-3387, 1748-3395},
	url = {http://www.nature.com/articles/nnano.2012.207},
	doi = {10.1038/nnano.2012.207},
	language = {en},
	number = {12},
	urldate = {2018-10-21},
	journal = {Nature Nanotechnology},
	author = {Monopoli, Marco P. and Åberg, Christoffer and Salvati, Anna and Dawson, Kenneth A.},
	month = dec,
	year = {2012},
	pages = {779--786},
}

@article{mei_protein-corona-by-design_2018,
	title = {Protein-{Corona}-by-{Design} in {2D}: {A} {Reliable} {Platform} to {Decode} {Bio}-{Nano} {Interactions} for the {Next}-{Generation} {Quality}-by-{Design} {Nanomedicines}},
	volume = {30},
	issn = {0935-9648},
	shorttitle = {Protein-{Corona}-by-{Design} in {2D}},
	doi = {10.1002/adma.201802732},
	abstract = {Hard corona (HC) protein, i.e., the environmental proteins of the biological medium that are bound to a nanosurface, is known to affect the biological fate of a nanomedicine. Due to the size, curvature, and specific surface area (SSA) 3-factor interactions inherited in the traditional 3D nanoparticle, HC-dependent bio-nano interactions are often poorly probed and interpreted. Here, the first HC-by-design case study in 2D is demonstrated that sequentially and linearly changes the HC quantity using functionalized graphene oxide (GO) nanosheets. The HC quantity and HC quality are analyzed using NanoDrop and label-free liquid chromatography-mass spectrometry (LC-MS) followed by principal component analysis (PCA). Cellular responses (uptake and cytotoxicity in J774 cell model) are compared using imaging cytometry and the modified lactate dehydrogenase assays, respectively. Cellular uptake linearly and solely correlates with HC quantity (R-2 = 0.99634). The nanotoxicity, analyzed by retrospective design of experiment (DoE), is found to be dependent on the nanomaterial uptake (primary), HC composition (secondary), and nanomaterial exposure dose (tertiary). This unique 2D design eliminates the size-curvature-SSA multifactor interactions and can serve as a reliable screening platform to uncover HC-dependent bio-nano interactions to enable the next-generation quality-by-design (QbD) nanomedicines for better clinical translation.},
	language = {English},
	number = {40},
	journal = {Advanced Materials},
	author = {Mei, Kuo-Ching and Ghazaryan, Artur and Teoh, Er Zhen and Summers, Huw D. and Li, Yueting and Ballesteros, Belen and Piasecka, Justyna and Walters, Adam and Hider, Robert C. and Mailaender, Volker and Al-Jamal, Khuloud T.},
	month = oct,
	year = {2018},
	note = {WOS:000446056700010},
	keywords = {click chemistry, cytotoxicity, drug delivery, graphene, graphene-based nanomaterials, mechanochemistry, nanomedicine, nanoparticles, oxide, protein corona, solvent-free, toxicity, water},
	pages = {1802732},
}

@article{nakanishi_adsorption_nodate,
	title = {On the {Adsorption} of {Proteins} on {Solid} {Surfaces}, a {Common} but {Very} {Complicated} {Phenomenon}},
	language = {en},
	author = {Nakanishi, Kazuhiro and Sakiyama, Takaharu and Imamura, Koreyoshi},
	pages = {12},
}

@article{gray_interaction_2004,
	title = {The interaction of proteins with solid surfaces},
	volume = {14},
	issn = {0959-440X},
	url = {http://www.sciencedirect.com/science/article/pii/S0959440X03001866},
	doi = {10.1016/j.sbi.2003.12.001},
	abstract = {The interaction of proteins with solid surfaces is a fundamental phenomenon with implications for nanotechnology, biomaterials and biotechnological processes. Kinetic and thermodynamic studies have long indicated that significant conformational changes may occur as a protein encounters a surface; new techniques are measuring and modeling these changes. Combinatorial and directed evolution techniques have created new peptide sequences that bind specifically to solid surfaces, similar to the natural proteins that regulate crystal growth. Modeling efforts capture kinetics and thermodynamics on the colloidal scale, but detailed treatments of atomic structure are still in development and face the usual challenges of protein modeling. Opportunities abound for fundamental discovery, as well as breakthroughs in biomaterials, biotechnology and nanotechnology.},
	number = {1},
	urldate = {2018-10-21},
	journal = {Current Opinion in Structural Biology},
	author = {Gray, Jeffrey J},
	month = feb,
	year = {2004},
	pages = {110--115},
}

@article{gessner_nanoparticles_2000,
	title = {Nanoparticles with decreasing surface hydrophobicities: influence on plasma protein adsorption},
	volume = {196},
	issn = {03785173},
	shorttitle = {Nanoparticles with decreasing surface hydrophobicities},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378517399004329},
	doi = {10.1016/S0378-5173(99)00432-9},
	abstract = {The rapid uptake of i.v. injected nanoparticles by cells of the mononuclear phagocytic system (MPS) is a major obstacle for a long blood circulation time and a drug targeting to sites other than the MPS. The adsorption of proteins on the particles surface after i.v. administration depends on their surface characteristics and is regarded as key factor for the in vivo organ distribution. The objective of this study is to investigate changes in the plasma protein adsorption patterns in the course of surface hydrophobicity variation. Latex particles with decreasing surface hydrophobicity were synthesized as model colloidal carriers. Physicochemical characterization had been performed and considerable differences in the protein adsorption patterns on the particles could be detected by using two-dimensional polyacrylamide gel electrophoresis (2-D PAGE). Correlations between physicochemical characteristics and the protein adsorption patterns have been found and are discussed. © 2000 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {2},
	urldate = {2018-10-21},
	journal = {International Journal of Pharmaceutics},
	author = {Gessner, A and Waicz, R and Lieske, A and Paulke, B.-R and Mäder, K and Müller, R.H},
	month = mar,
	year = {2000},
	pages = {245--249},
}

@article{lynch_detecting_2006,
	title = {Detecting {Cryptic} {Epitopes} {Created} by {Nanoparticles}},
	volume = {2006},
	issn = {1945-0877, 1937-9145},
	url = {http://stke.sciencemag.org/cgi/doi/10.1126/stke.3272006pe14},
	doi = {10.1126/stke.3272006pe14},
	language = {en},
	number = {327},
	urldate = {2018-10-21},
	journal = {Science Signaling},
	author = {Lynch, I. and Dawson, K. A. and Linse, S.},
	month = mar,
	year = {2006},
	pages = {pe14--pe14},
}

@article{aggarwal_nanoparticle_2009,
	series = {Identifying and {Assessing} {Biomaterial} {Nanotoxicity} in {Translational} {Research} for {Preclinical} {Drug} {Development}},
	title = {Nanoparticle interaction with plasma proteins as it relates to particle biodistribution, biocompatibility and therapeutic efficacy},
	volume = {61},
	issn = {0169-409X},
	url = {http://www.sciencedirect.com/science/article/pii/S0169409X0900101X},
	doi = {10.1016/j.addr.2009.03.009},
	abstract = {Proteins bind the surfaces of nanoparticles, and biological materials in general, immediately upon introduction of the materials into a physiological environment. The further biological response of the body is influenced by the nanoparticle–protein complex. The nanoparticle's composition and surface chemistry dictate the extent and specificity of protein binding. Protein binding is one of the key elements that affects biodistribution of the nanoparticles throughout the body. Here we review recent research on nanoparticle physicochemical properties important for protein binding, techniques for isolation and identification of nanoparticle-bound proteins, and how these proteins can influence particle biodistribution and biocompatibility. Understanding the nanoparticle–protein complex is necessary for control and manipulation of protein binding, and allows for improved engineering of nanoparticles with favorable bioavailability and biodistribution.},
	number = {6},
	urldate = {2018-10-20},
	journal = {Advanced Drug Delivery Reviews},
	author = {Aggarwal, Parag and Hall, Jennifer B. and McLeland, Christopher B. and Dobrovolskaia, Marina A. and McNeil, Scott E.},
	month = jun,
	year = {2009},
	keywords = {Biocompatibility, Biodistribution, Immunology, Nanoparticles, Protein binding},
	pages = {428--437},
}

@article{arai_behavior_1990,
	title = {The behavior of some model proteins at solid-liquid interfaces 1. {Adsorption} from single protein solutions},
	volume = {51},
	issn = {0166-6622},
	url = {http://www.sciencedirect.com/science/article/pii/016666229080127P},
	doi = {10.1016/0166-6622(90)80127-P},
	abstract = {The adsorption of proteins of similar molecular size and shape on various well-defined surfaces is discussed. The hydrophobicity and the electrical charge density of both the protein molecule and the sorbent surface as well as the structure stability of the protein molecule were taken as the experimental variables. The adsorption process was studied by determining adsorption isotherms and by measuring electrophoretic mobilities and heats of adsorption at varying degrees of coverage of the sorbent surface by the protein. It appeared that proteins of which the structure is stabilized by a large Gibbs energy behave like hard particles: they adsorb on hydrophobic interfaces under all conditions of charge interaction and on hydrophilic surfaces only if electrostatically attracted. Soft proteins, i.e., proteins characterized by a lower structure stability, adsorb on hydrophobic and hydrophilic surfaces under attractive and repulsive electrostatic conditions. These proteins contain an extra driving force for adsorption, related to structure rearrangements in the molecule, that outweighs the unfavorable contributions form hydrophilic dehydration and electrostatic repulsion.},
	urldate = {2018-10-20},
	journal = {Colloids and Surfaces},
	author = {Arai, Takaaki and Norde, Willem},
	month = jan,
	year = {1990},
	pages = {1--15},
}

@article{lindman_systematic_2007,
	title = {Systematic {Investigation} of the {Thermodynamics} of {HSA} {Adsorption} to {N}-iso-{Propylacrylamide}/{N}-tert-{Butylacrylamide} {Copolymer} {Nanoparticles}. {Effects} of {Particle} {Size} and {Hydrophobicity}},
	volume = {7},
	issn = {1530-6984},
	url = {https://doi.org/10.1021/nl062743+},
	doi = {10.1021/nl062743+},
	abstract = {Nanoparticles in biological fluids almost invariably become coated with proteins that may confer nanomedical and nanotoxicological effects. Understanding these effects requires quantitative measurements using simple systems. Adsorption of HSA to copolymer nanoparticles of varying hydrophobicity and curvature was studied using ITC, yielding stoichiometry, affinity, and enthalpy changes upon binding. The hydrophobicity was controlled via the co-monomer ratio, N-iso-propylacrylamide/N-tert-butylacrylamide. The most hydrophobic particles become fully covered with a single layer of protein, except at high curvature.},
	number = {4},
	urldate = {2018-10-20},
	journal = {Nano Letters},
	author = {Lindman, Stina and Lynch, Iseult and Thulin, Eva and Nilsson, Hanna and Dawson, Kenneth A. and Linse, Sara},
	month = apr,
	year = {2007},
	pages = {914--920},
}

@article{lynch_protein-nanoparticle_2008,
	title = {Protein-nanoparticle interactions},
	volume = {3},
	url = {https://ac-els-cdn-com.ep.fjernadgang.kb.dk/S1748013208700148/1-s2.0-S1748013208700148-main.pdf?_tid=e1f79e98-6bce-4307-be6d-771a6505878b&acdnat=1540013103_b1e335b7cb074575e4612ca3fde809fd},
	number = {1-2},
	urldate = {2018-10-20},
	journal = {Nano Today},
	author = {Lynch, Iseult and Dawson, Kenneth A.},
	month = apr,
	year = {2008},
	pages = {40--47},
}

@article{cedervall_detailed_2007,
	title = {Detailed {Identification} of {Plasma} {Proteins} {Adsorbed} on {Copolymer} {Nanoparticles}},
	volume = {46},
	issn = {14337851, 15213773},
	url = {http://doi.wiley.com/10.1002/anie.200700465},
	doi = {10.1002/anie.200700465},
	language = {en},
	number = {30},
	urldate = {2018-10-20},
	journal = {Angewandte Chemie International Edition},
	author = {Cedervall, Tommy and Lynch, Iseult and Foy, Martina and Berggård, Tord and Donnelly, Seamas C. and Cagney, Gerard and Linse, Sara and Dawson, Kenneth A.},
	month = jul,
	year = {2007},
	pages = {5754--5756},
}

@article{clemments_effect_2014,
	title = {Effect of surface properties in protein corona development on mesoporous silica nanoparticles},
	volume = {4},
	issn = {2046-2069},
	url = {http://xlink.rsc.org/?DOI=C4RA03277B},
	doi = {10.1039/C4RA03277B},
	language = {en},
	number = {55},
	urldate = {2018-10-20},
	journal = {RSC Adv.},
	author = {Clemments, Alden M. and Muniesa, Carlos and Landry, Christopher C. and Botella, Pablo},
	year = {2014},
	pages = {29134--29138},
}

@article{kelly_mapping_2015,
	title = {Mapping protein binding sites on the biomolecular corona of nanoparticles},
	volume = {10},
	issn = {1748-3387, 1748-3395},
	url = {http://www.nature.com/articles/nnano.2015.47},
	doi = {10.1038/nnano.2015.47},
	language = {en},
	number = {5},
	urldate = {2018-10-20},
	journal = {Nature Nanotechnology},
	author = {Kelly, Philip M. and Åberg, Christoffer and Polo, Ester and O'Connell, Ann and Cookman, Jennifer and Fallon, Jonathan and Krpetić, Željka and Dawson, Kenneth A.},
	month = may,
	year = {2015},
	pages = {472--479},
}

@article{rocker_quantitative_2009,
	title = {A quantitative fluorescence study of protein monolayer formation on colloidal nanoparticles},
	volume = {4},
	issn = {1748-3387, 1748-3395},
	url = {http://www.nature.com/articles/nnano.2009.195},
	doi = {10.1038/nnano.2009.195},
	language = {en},
	number = {9},
	urldate = {2018-10-20},
	journal = {Nature Nanotechnology},
	author = {Röcker, Carlheinz and Pötzl, Matthias and Zhang, Feng and Parak, Wolfgang J. and Nienhaus, G. Ulrich},
	month = sep,
	year = {2009},
	pages = {577--580},
}

@article{clemments_protein_2015,
	title = {Protein {Adsorption} {From} {Biofluids} on {Silica} {Nanoparticles}: {Corona} {Analysis} as a {Function} of {Particle} {Diameter} and {Porosity}},
	volume = {7},
	issn = {1944-8244},
	shorttitle = {Protein {Adsorption} {From} {Biofluids} on {Silica} {Nanoparticles}},
	url = {https://doi.org/10.1021/acsami.5b07631},
	doi = {10.1021/acsami.5b07631},
	abstract = {A study on the adsorption of proteins from fetal bovine serum (FBS) on spherical dense and mesoporous silica nanoparticles with a wide range of diameters, from 70 to 900 nm, is presented. Monodisperse populations of particles with a range of diameters were obtained through modifications of the Stöber method. Extensive characterization of the particles was then performed using N2 physisorption, TEM, DLS, and ζ-potential. Following serum exposure, proteomic evaluation in concert with thermogravimetric analysis revealed the associated concentrations of each protein identified in the hard corona. Small particles adsorbed the largest amount of protein, due to their larger external surface area. Proteins with low molecular weights ({\textless}50 kDa) constituted the majority of the protein corona, totaling between 60 and 80\% of the total mass of adsorbed protein. Here, the higher surface curvature of small particles favors the enrichment of smaller proteins. Porosity does not promote protein adsorption but improves deposition of the low molecular weight protein fraction due to the size-exclusion effect related to pore diameter. These results have important implications for the use of dense and porous silica nanoparticles in biomedical applications.},
	number = {39},
	urldate = {2018-10-20},
	journal = {ACS Applied Materials \& Interfaces},
	author = {Clemments, Alden M. and Botella, Pablo and Landry, Christopher C.},
	month = oct,
	year = {2015},
	pages = {21682--21689},
}

@article{clemments_spatial_2017,
	title = {Spatial {Mapping} of {Protein} {Adsorption} on {Mesoporous} {Silica} {Nanoparticles} by {Stochastic} {Optical} {Reconstruction} {Microscopy}},
	volume = {139},
	issn = {0002-7863, 1520-5126},
	url = {http://pubs.acs.org/doi/10.1021/jacs.7b01118},
	doi = {10.1021/jacs.7b01118},
	abstract = {Exposure to biological ﬂuid envelops a nanoparticle in layers of proteins and biomolecules, which has a profound impact on the nanoparticle’s biological fate. Although the identities and amounts of the proteins in this “corona” have been thoroughly examined, the spatial arrangement of the proteins is unclear, a problem that is compounded on porous nanoparticles due to penetration of proteins within the porous network. To address this problem, we have developed a procedure based on information derived from stochastic optical reconstruction microscopy. We employed a mathematical model to reveal the penetration depth of several proteins within porous nanoparticles. Understanding protein penetration depth provides an explanation for the composition of the protein corona, aiding in the development of safe and eﬀective particle-based therapies.},
	language = {en},
	number = {11},
	urldate = {2018-10-19},
	journal = {Journal of the American Chemical Society},
	author = {Clemments, Alden M. and Botella, Pablo and Landry, Christopher C.},
	month = mar,
	year = {2017},
	pages = {3978--3981},
}

@article{borgmann_single_2016,
	title = {Single {Molecule} {Fluorescence} {Microscopy} and {Machine} {Learning} for {Rhesus} {D} {Antigen} {Classification}},
	volume = {6},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep32317},
	doi = {10.1038/srep32317},
	language = {en},
	number = {1},
	urldate = {2018-10-11},
	journal = {Scientific Reports},
	author = {Borgmann, Daniela M. and Mayr, Sandra and Polin, Helene and Schaller, Susanne and Dorfer, Viktoria and Obritzberger, Lisa and Endmayr, Tanja and Gabriel, Christian and Winkler, Stephan M. and Jacak, Jaroslaw},
	month = oct,
	year = {2016},
}

@article{albrecht_deep_2017,
	title = {Deep learning for single-molecule science},
	volume = {28},
	issn = {0957-4484, 1361-6528},
	url = {http://stacks.iop.org/0957-4484/28/i=42/a=423001?key=crossref.0e0fca14c36d6e7a75048226db7c4506},
	doi = {10.1088/1361-6528/aa8334},
	abstract = {Exploring and making predictions based on single-molecule data can be challenging, not only due to the sheer size of the datasets, but also because a priori knowledge about the signal characteristics is typically limited and poor signal-to-noise ratio. For example, hypothesisdriven data exploration, informed by an expectation of the signal characteristics, can lead to interpretation bias or loss of information. Equally, even when the different data categories are known, e.g., the four bases in DNA sequencing, it is often difﬁcult to know how to make best use of the available information content. The latest developments in machine learning (ML), so-called deep learning (DL) offer interesting, new avenues to address such challenges. In some applications, such as speech and image recognition, DL has been able to outperform conventional ML strategies and even human performance. However, to date DL has not been applied much in single-molecule science, presumably in part because relatively little is known about the ‘internal workings’ of such DL tools within single-molecule science as a ﬁeld. In this Tutorial, we make an attempt to illustrate in a step-by-step guide how one of those, a convolutional neural network (CNN), may be used for base calling in DNA sequencing applications. We compare it with a SVM as a more conventional ML method, and discuss some of the strengths and weaknesses of the approach. In particular, a ‘deep’ neural network has many features of a ‘black box’, which has important implications on how we look at and interpret data.},
	language = {en},
	number = {42},
	urldate = {2018-10-11},
	journal = {Nanotechnology},
	author = {Albrecht, Tim and Slabaugh, Gregory and Alonso, Eduardo and Al-Arif, SM Masudur R},
	month = oct,
	year = {2017},
	pages = {423001},
}

@article{pham_polymorphism_2014,
	title = {Polymorphism of {Oligomers} of a {Peptide} from β-{Amyloid}},
	volume = {136},
	issn = {0002-7863, 1520-5126},
	url = {http://pubs.acs.org/doi/10.1021/ja500996d},
	doi = {10.1021/ja500996d},
	abstract = {This contribution reports solution-phase structural studies of oligomers of a family of peptides derived from the β-amyloid peptide (Aβ). We had previously reported the X-ray crystallographic structures of the oligomers and oligomer assemblies formed in the solid state by a macrocyclic β-sheet peptide containing the Aβ15−23 nonapeptide. In the current study, we set out to determine its assembly in aqueous solution. In the solid state, macrocyclic β-sheet peptide 1 assembles to form hydrogen-bonded dimers that further assemble in a sandwich-like fashion to form tetramers through hydrophobic interactions between the faces bearing V18 and F20. In aqueous solution, macrocyclic β-sheet peptide 1 and homologue 2a form hydrogen-bonded dimers that assemble to form tetramers through hydrophobic interactions between the faces bearing L17, F19, and A21. In the solid state, the hydrogen-bonded dimers are antiparallel, and the β-strands are fully aligned, with residues 17−23 of one of the macrocycles aligned with residues 23−17 of the other. In solution, residues 17−23 of the hydrogen-bonded dimers are shifted out of alignment by two residues toward the C-termini. The two hydrogen-bonded dimers are nearly orthogonal in the solid state, while in solution the dimers are only slightly rotated. The diﬀering morphology of the solution-state and solid-state tetramers is signiﬁcant, because it may provide a glimpse into some of the structural bases for polymorphism among Aβ oligomers in Alzheimer’s disease.},
	language = {en},
	number = {14},
	urldate = {2018-10-11},
	journal = {Journal of the American Chemical Society},
	author = {Pham, Johnny D. and Demeler, Borries and Nowick, James S.},
	month = apr,
	year = {2014},
	pages = {5432--5442},
}

@article{masters_amyloid_1985,
	title = {Amyloid plaque core protein in {Alzheimer} disease and {Down} syndrome.},
	volume = {82},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC397973/},
	abstract = {We have purified and characterized the cerebral amyloid protein that forms the plaque core in Alzheimer disease and in aged individuals with Down syndrome. The protein consists of multimeric aggregates of a polypeptide of about 40 residues (4 kDa). The amino acid composition, molecular mass, and NH2-terminal sequence of this amyloid protein are almost identical to those described for the amyloid deposited in the congophilic angiopathy of Alzheimer disease and Down syndrome, but the plaque core proteins have ragged NH2 termini. The shared 4-kDa subunit indicates a common origin for the amyloids of the plaque core and of the congophilic angiopathy. There are superficial resemblances between the solubility characteristics of the plaque core and some of the properties of scrapie infectivity, but there are no similarities in amino acid sequences between the plaque core and scrapie polypeptides.},
	number = {12},
	urldate = {2018-10-11},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Masters, C L and Simms, G and Weinman, N A and Multhaup, G and McDonald, B L and Beyreuther, K},
	month = jun,
	year = {1985},
	pmid = {3159021},
	pmcid = {PMC397973},
	pages = {4245--4249},
}

@article{kaminski_schierle_situ_2011,
	title = {In {Situ} {Measurements} of the {Formation} and {Morphology} of {Intracellular} β-{Amyloid} {Fibrils} by {Super}-{Resolution} {Fluorescence} {Imaging}},
	volume = {133},
	issn = {0002-7863},
	url = {https://doi.org/10.1021/ja201651w},
	doi = {10.1021/ja201651w},
	abstract = {Misfolding and aggregation of peptides and proteins is a characteristic of many neurodegenerative disorders, including Alzheimer’s disease (AD). In AD the β-amyloid peptide (Aβ) aggregates to form characteristic fibrillar structures, which are the deposits found as plaques in the brains of patients. We have used direct stochastic optical reconstruction microscopy, dSTORM, to probe the process of in situ Aβ aggregation and the morphology of the ensuing aggregates with a resolution better than 20 nm. We are able to distinguish different types of structures, including oligomeric assemblies and mature fibrils, and observe a number of morphological differences between the species formed in vitro and in vivo, which may be significant in the context of disease. Our data support the recent view that intracellular Aβ could be associated with Aβ pathogenicity in AD, although the major deposits are extracellular, and suggest that this approach will be widely applicable to studies of the molecular mechanisms of protein deposition diseases.},
	number = {33},
	urldate = {2018-10-08},
	journal = {Journal of the American Chemical Society},
	author = {Kaminski Schierle, Gabriele S. and van de Linde, Sebastian and Erdelyi, Miklos and Esbjörner, Elin K. and Klein, Teresa and Rees, Eric and Bertoncini, Carlos W. and Dobson, Christopher M. and Sauer, Markus and Kaminski, Clemens F.},
	month = aug,
	year = {2011},
	pages = {12902--12905},
}

@article{braak_staging_2003,
	title = {Staging of brain pathology related to sporadic {Parkinson}’s disease},
	volume = {24},
	issn = {0197-4580},
	url = {http://www.sciencedirect.com/science/article/pii/S0197458002000659},
	doi = {10.1016/S0197-4580(02)00065-9},
	abstract = {Sporadic Parkinson’s disease involves multiple neuronal systems and results from changes developing in a few susceptible types of nerve cells. Essential for neuropathological diagnosis are α-synuclein-immunopositive Lewy neurites and Lewy bodies. The pathological process targets specific induction sites: lesions initially occur in the dorsal motor nucleus of the glossopharyngeal and vagal nerves and anterior olfactory nucleus. Thereafter, less vulnerable nuclear grays and cortical areas gradually become affected. The disease process in the brain stem pursues an ascending course with little interindividual variation. The pathology in the anterior olfactory nucleus makes fewer incursions into related areas than that developing in the brain stem. Cortical involvement ensues, beginning with the anteromedial temporal mesocortex. From there, the neocortex succumbs, commencing with high order sensory association and prefrontal areas. First order sensory association/premotor areas and primary sensory/motor fields then follow suit. This study traces the course of the pathology in incidental and symptomatic Parkinson cases proposing a staging procedure based upon the readily recognizable topographical extent of the lesions.},
	number = {2},
	urldate = {2018-10-11},
	journal = {Neurobiology of Aging},
	author = {Braak, Heiko and Tredici, Kelly Del and Rüb, Udo and de Vos, Rob A. I and Jansen Steur, Ernst N. H and Braak, Eva},
	month = mar,
	year = {2003},
	keywords = {Lewy bodies, Lewy neurites, Limbic system, Motor system, Parkinson’s disease, Staging procedure, α-synuclein},
	pages = {197--211},
}

@article{bates_multicolor_2007,
	title = {Multicolor {Super}-{Resolution} {Imaging} with {Photo}-{Switchable} {Fluorescent} {Probes}},
	volume = {317},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1146598},
	doi = {10.1126/science.1146598},
	language = {en},
	number = {5845},
	urldate = {2018-10-11},
	journal = {Science},
	author = {Bates, M. and Huang, B. and Dempsey, G. T. and Zhuang, X.},
	month = sep,
	year = {2007},
	pages = {1749--1753},
}

@article{mounet_novel_2018,
	title = {Novel two-dimensional materials from high-throughput computational exfoliation of experimentally known compounds},
	volume = {13},
	issn = {1748-3387, 1748-3395},
	url = {http://arxiv.org/abs/1611.05234},
	doi = {10.1038/s41565-017-0035-5},
	abstract = {We search for novel two-dimensional materials that can be easily exfoliated from their parent compounds. Starting from 108423 unique, experimentally known three-dimensional compounds we identify a subset of 5619 that appear layered according to robust geometric and bonding criteria. High-throughput calculations using van-der-Waals density-functional theory, validated against experimental structural data and calculated random-phase-approximation binding energies, allow to identify 1844 compounds that are either easily or potentially exfoliable, including all that are commonly exfoliated experimentally. In particular, the subset of 1053 easily exfoliable cases \$-\$layered materials held together mostly by dispersion interactions and with binding energies in the range of few tens of meV\${\textbackslash}cdot\${\textbackslash}AA\${\textasciicircum}\{-2\}-\$ contains several hundreds of entries with few atoms per primitive cell (273 with less than 6 atoms, 606 with less than 12), revealing a wealth of new structural prototypes, simple ternary compounds, and a large portfolio to search for optimal electronic, optical, magnetic, topological, or chemical properties.},
	number = {3},
	urldate = {2018-10-11},
	journal = {Nature Nanotechnology},
	author = {Mounet, Nicolas and Gibertini, Marco and Schwaller, Philippe and Merkys, Andrius and Castelli, Ivano E. and Cepellotti, Andrea and Pizzi, Giovanni and Marzari, Nicola},
	month = mar,
	year = {2018},
	note = {arXiv: 1611.05234},
	keywords = {Condensed Matter - Materials Science},
	pages = {246--252},
}

@article{min_imaging_2009,
	title = {Imaging chromophores with undetectable fluorescence by stimulated emission microscopy},
	volume = {461},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature08438},
	doi = {10.1038/nature08438},
	language = {en},
	number = {7267},
	urldate = {2018-10-09},
	journal = {Nature},
	author = {Min, Wei and Lu, Sijia and Chong, Shasha and Roy, Rahul and Holtom, Gary R. and Xie, X. Sunney},
	month = oct,
	year = {2009},
	pages = {1105--1109},
}

@article{cedervall_understanding_2007,
	title = {Understanding the nanoparticle-protein corona using methods to quantify exchange rates and affinities of proteins for nanoparticles},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0608582104},
	doi = {10.1073/pnas.0608582104},
	language = {en},
	number = {7},
	urldate = {2018-10-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cedervall, T. and Lynch, I. and Lindman, S. and Berggard, T. and Thulin, E. and Nilsson, H. and Dawson, K. A. and Linse, S.},
	month = feb,
	year = {2007},
	pages = {2050--2055},
}

@article{lundqvist_evolution_2011,
	title = {The {Evolution} of the {Protein} {Corona} around {Nanoparticles}: {A} {Test} {Study}},
	volume = {5},
	issn = {1936-0851, 1936-086X},
	shorttitle = {The {Evolution} of the {Protein} {Corona} around {Nanoparticles}},
	url = {http://pubs.acs.org/doi/10.1021/nn202458g},
	doi = {10.1021/nn202458g},
	abstract = {The importance of the protein corona formed around nanoparticles upon entering a biological ﬂuid has recently been highlighted. This corona is, when suﬃciently long-lived, thought to govern the particles' biological fate. However, even this long-lived “hard” corona evolves and reequilibrates as particles pass from one biological ﬂuid to another, and may be an important feature for long-term fate. Here we show the evolution of the protein corona as a result of transfer of nanoparticles from one biological ﬂuid (plasma) into another (cytosolic ﬂuid), a simple illustrative model for the uptake of nanoparticles into cells. While no direct comparison can be made to what would happen in, for example, the uptake pathway, the results conﬁrm that signiﬁcant evolution of the corona occurs in the second biological solution, but that the ﬁnal corona contains a “ﬁngerprint” of its history. This could be evolved to map the transport pathways utilized by nanoparticles, and eventually to predict nanoparticle fate and behavior.},
	language = {en},
	number = {9},
	urldate = {2018-10-09},
	journal = {ACS Nano},
	author = {Lundqvist, Martin and Stigler, Johannes and Cedervall, Tommy and Berggård, Tord and Flanagan, Michelle B. and Lynch, Iseult and Elia, Giuliano and Dawson, Kenneth},
	month = sep,
	year = {2011},
	pages = {7503--7509},
}

@article{muls_direct_2005,
	title = {Direct {Measurement} of the {End}-to-{End} {Distance} of {Individual} {Polyfluorene} {Polymer} {Chains}},
	volume = {6},
	issn = {1439-4235, 1439-7641},
	url = {http://doi.wiley.com/10.1002/cphc.200500235},
	doi = {10.1002/cphc.200500235},
	language = {en},
	number = {11},
	urldate = {2018-10-09},
	journal = {ChemPhysChem},
	author = {Muls, Benoît and Uji-i, Hiroshi and Melnikov, Sergey and Moussa, Alain and Verheijen, Wendy and Soumillion, Jean-Philippe and Josemon, Jacob and Müllen, Klaus and Hofkens, Johan},
	month = nov,
	year = {2005},
	pages = {2286--2294},
}

@article{vosch_strongly_2007,
	title = {Strongly emissive individual {DNA}-encapsulated {Ag} nanoclusters as single-molecule fluorophores},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0610677104},
	doi = {10.1073/pnas.0610677104},
	language = {en},
	number = {31},
	urldate = {2018-10-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Vosch, T. and Antoku, Y. and Hsiang, J.-C. and Richards, C. I. and Gonzalez, J. I. and Dickson, R. M.},
	month = jul,
	year = {2007},
	pages = {12616--12621},
}

@article{habuchi_reversible_nodate,
	title = {Reversible single-molecule photoswitching in the {GFP}-like fluorescent protein {Dronpa}},
	language = {en},
	author = {Habuchi, Satoshi and Ando, Ryoko and Dedecker, Peter and Verheijen, Wendy and Mizuno, Hideaki and Miyawaki, Atsushi and Hofkens, Johan},
	pages = {14},
}

@article{gaufres_giant_2014,
	title = {Giant {Raman} scattering from {J}-aggregated dyes inside carbon nanotubes for multispectral imaging},
	volume = {8},
	issn = {1749-4885, 1749-4893},
	url = {http://www.nature.com/articles/nphoton.2013.309},
	doi = {10.1038/nphoton.2013.309},
	language = {en},
	number = {1},
	urldate = {2018-10-09},
	journal = {Nature Photonics},
	author = {Gaufrès, E. and Tang, N. Y.-Wa and Lapointe, F. and Cabana, J. and Nadon, M.-A. and Cottenye, N. and Raymond, F. and Szkopek, T. and Martel, R.},
	month = jan,
	year = {2014},
	pages = {72--78},
}

@article{gross_bond-order_2012,
	title = {Bond-{Order} {Discrimination} by {Atomic} {Force} {Microscopy}},
	volume = {337},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1225621},
	doi = {10.1126/science.1225621},
	language = {en},
	number = {6100},
	urldate = {2018-10-09},
	journal = {Science},
	author = {Gross, L. and Mohn, F. and Moll, N. and Schuler, B. and Criado, A. and Guitian, E. and Pena, D. and Gourdon, A. and Meyer, G.},
	month = sep,
	year = {2012},
	pages = {1326--1329},
}

@article{gross_high-resolution_2011,
	title = {High-{Resolution} {Molecular} {Orbital} {Imaging} {Using} a p -{Wave} {STM} {Tip}},
	volume = {107},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.107.086101},
	doi = {10.1103/PhysRevLett.107.086101},
	language = {en},
	number = {8},
	urldate = {2018-10-09},
	journal = {Physical Review Letters},
	author = {Gross, Leo and Moll, Nikolaj and Mohn, Fabian and Curioni, Alessandro and Meyer, Gerhard and Hanke, Felix and Persson, Mats},
	month = aug,
	year = {2011},
}

@article{millo_imaging_2001,
	title = {Imaging and {Spectroscopy} of {Artificial}-{Atom} {States} in {Core}/{Shell} {Nanocrystal} {Quantum} {Dots}},
	volume = {86},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.86.5751},
	doi = {10.1103/PhysRevLett.86.5751},
	language = {en},
	number = {25},
	urldate = {2018-10-09},
	journal = {Physical Review Letters},
	author = {Millo, Oded and Katz, David and Cao, YunWei and Banin, Uri},
	month = jun,
	year = {2001},
	pages = {5751--5754},
}

@article{andersen_self-assembly_2009,
	title = {Self-assembly of a nanoscale {DNA} box with a controllable lid},
	volume = {459},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature07971},
	doi = {10.1038/nature07971},
	language = {en},
	number = {7243},
	urldate = {2018-10-09},
	journal = {Nature},
	author = {Andersen, Ebbe S. and Dong, Mingdong and Nielsen, Morten M. and Jahn, Kasper and Subramani, Ramesh and Mamdouh, Wael and Golas, Monika M. and Sander, Bjoern and Stark, Holger and Oliveira, Cristiano L. P. and Pedersen, Jan Skov and Birkedal, Victoria and Besenbacher, Flemming and Gothelf, Kurt V. and Kjems, Jørgen},
	month = may,
	year = {2009},
	pages = {73--76},
}

@article{nam_graphene_2014,
	title = {Graphene {Nanopore} with a {Self}-{Integrated} {Optical} {Antenna}},
	volume = {14},
	issn = {1530-6984, 1530-6992},
	url = {http://pubs.acs.org/doi/10.1021/nl503159d},
	doi = {10.1021/nl503159d},
	abstract = {We report graphene nanopores with integrated optical antennae. We demonstrate that a nanometer-sized heated spot created by photon-to-heat conversion of a gold nanorod resting on a graphene membrane forms a nanoscale pore with a self-integrated optical antenna in a single step. The distinct plasmonic traits of metal nanoparticles, which have a unique capability to concentrate light into nanoscale regions, yield the signiﬁcant advantage of parallel nanopore fabrication compared to the conventional sequential process using an electron beam. Tunability of both the nanopore dimensions and the optical characteristics of plasmonic nanoantennae are further achieved. Finally, the key optical function of our self-integrated optical antenna on the vicinity of graphene nanopore is manifested by multifold ﬂuorescent signal enhancement during DNA translocation.},
	language = {en},
	number = {10},
	urldate = {2018-10-09},
	journal = {Nano Letters},
	author = {Nam, SungWoo and Choi, Inhee and Fu, Chi-cheng and Kim, Kwanpyo and Hong, SoonGweon and Choi, Yeonho and Zettl, Alex and Lee, Luke P.},
	month = oct,
	year = {2014},
	pages = {5584--5589},
}

@article{voiry_high-quality_2016,
	title = {High-quality graphene via microwave reduction of solution-exfoliated graphene oxide},
	volume = {353},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aah3398},
	doi = {10.1126/science.aah3398},
	language = {en},
	number = {6306},
	urldate = {2018-10-09},
	journal = {Science},
	author = {Voiry, D. and Yang, J. and Kupferberg, J. and Fullon, R. and Lee, C. and Jeong, H. Y. and Shin, H. S. and Chhowalla, M.},
	month = sep,
	year = {2016},
	pages = {1413--1416},
}

@article{chen_molecular_2017,
	title = {Molecular diodes with rectification ratios exceeding 105 driven by electrostatic interactions},
	volume = {12},
	issn = {1748-3387, 1748-3395},
	url = {http://www.nature.com/doifinder/10.1038/nnano.2017.110},
	doi = {10.1038/nnano.2017.110},
	language = {en},
	number = {8},
	urldate = {2018-10-09},
	journal = {Nature Nanotechnology},
	author = {Chen, Xiaoping and Roemer, Max and Yuan, Li and Du, Wei and Thompson, Damien and del Barco, Enrique and Nijhuis, Christian A.},
	month = jul,
	year = {2017},
	pages = {797--803},
}

@article{garner_comprehensive_2018,
	title = {Comprehensive suppression of single-molecule conductance using destructive σ-interference},
	volume = {558},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-018-0197-9},
	doi = {10.1038/s41586-018-0197-9},
	language = {en},
	number = {7710},
	urldate = {2018-10-09},
	journal = {Nature},
	author = {Garner, Marc H. and Li, Haixing and Chen, Yan and Su, Timothy A. and Shangguan, Zhichun and Paley, Daniel W. and Liu, Taifeng and Ng, Fay and Li, Hexing and Xiao, Shengxiong and Nuckolls, Colin and Venkataraman, Latha and Solomon, Gemma C.},
	month = jun,
	year = {2018},
	pages = {415--419},
}

@article{wang_super-temporal-resolved_2018,
	title = {Super-{Temporal}-{Resolved} {Microscopy} {Reveals} {Multistep} {Desorption} {Kinetics} of α-{Lactalbumin} from {Nylon}},
	volume = {34},
	issn = {0743-7463},
	url = {https://doi.org/10.1021/acs.langmuir.8b00686},
	doi = {10.1021/acs.langmuir.8b00686},
	abstract = {Insight into the mechanisms driving protein–polymer interactions is constantly improving due to advances in experimental and computational methods. In this study, we used super-temporal-resolved microscopy (STReM) to study the interfacial kinetics of a globular protein, α-lactalbumin (α-LA), adsorbing at the water–nylon 6,6 interface. The improved temporal resolution of STReM revealed that residence time distributions involve an additional step in the desorption process. Increasing the ionic strength in the bulk solution accelerated the desorption rate of α-LA, attributed to adsorption-induced conformational changes. Ensemble circular dichroism measurements were used to support a consecutive reaction mechanism. Without the improved temporal resolution of STReM, the desorption intermediate was not resolvable, highlighting both STReM’s potential to uncover new kinetic mechanisms and the continuing need to push for better time and space resolution.},
	number = {23},
	urldate = {2018-10-08},
	journal = {Langmuir},
	author = {Wang, Wenxiao and Shen, Hao and Moringo, Nicholas A. and Carrejo, Nicole C. and Ye, Fan and Robinson, Jacob T. and Landes, Christy F.},
	month = jun,
	year = {2018},
	pages = {6697--6702},
}

@article{ries_superresolution_2013,
	title = {Superresolution {Imaging} of {Amyloid} {Fibrils} with {Binding}-{Activated} {Probes}},
	volume = {4},
	url = {https://doi.org/10.1021/cn400091m},
	doi = {10.1021/cn400091m},
	abstract = {Protein misfolding into amyloid-like aggregates underlies many neurodegenerative diseases. Thus, insights into the structure and function of these amyloids will provide valuable information on the pathological mechanisms involved and aid in the design of improved drugs for treating amyloid-based disorders. However, determining the structure of endogenous amyloids at high resolution has been difficult. Here we employ binding-activated localization microscopy (BALM) to acquire superresolution images of α-synuclein amyloid fibrils with unprecedented optical resolution. We propose that BALM imaging can be extended to study the structure of other amyloids, for differential diagnosis of amyloid-related diseases and for discovery of drugs that perturb amyloid structure for therapy.},
	number = {7},
	urldate = {2018-10-08},
	journal = {ACS Chemical Neuroscience},
	author = {Ries, Jonas and Udayar, Vinod and Soragni, Alice and Hornemann, Simone and Nilsson, K. Peter R. and Riek, Roland and Hock, Christoph and Ewers, Helge and Aguzzi, Adriano A. and Rajendran, Lawrence},
	month = jul,
	year = {2013},
	pages = {1057--1061},
}

@incollection{mukhopadhyay_nanoscale_2014,
	title = {Nanoscale {Optical} {Imaging} of {Protein} {Amyloids}},
	isbn = {978-0-12-394431-3},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123944313000377},
	language = {en},
	urldate = {2018-10-08},
	booktitle = {Bio-nanoimaging},
	publisher = {Elsevier},
	author = {Mukhopadhyay, Samrat and Dalal, Vijit and Arya, Shruti},
	year = {2014},
	doi = {10.1016/B978-0-12-394431-3.00037-7},
	pages = {409--428},
}

@article{alquraishi_end--end_2018,
	title = {End-to-end differentiable learning of protein structure},
	url = {http://biorxiv.org/lookup/doi/10.1101/265231},
	doi = {10.1101/265231},
	abstract = {Accurate prediction of protein structure is one of the central challenges of biochemistry. Despite significant progress made by co-evolution methods to predict protein structure from signatures of residue-residue coupling found in the evolutionary record, a direct and explicit mapping between protein sequence and structure remains elusive, with no substantial recent progress. Meanwhile, rapid developments in deep learning, which have found remarkable success in computer vision, natural language processing, and quantum chemistry raise the question of whether a deep learning based approach to protein structure could yield similar advancements. A key ingredient of the success of deep learning is the reformulation of complex, human-designed, multi-stage pipelines with differentiable models that can be jointly optimized end-to-end. We report the development of such a model, which reformulates the entire structure prediction pipeline using differentiable primitives. Achieving this required combining four technical ideas: (1) the adoption of a recurrent neural architecture to encode the internal representation of protein sequence, (2) the parameterization of (local) protein structure by torsional angles, which provides a way to reason over protein conformations without violating the covalent chemistry of protein chains, (3) the coupling of local protein structure to its global representation via recurrent geometric units, and (4) the use of a differentiable loss function to capture deviations between predicted and experimental structures. To our knowledge this is the first end-to-end differentiable model for learning of protein structure. We test the effectiveness of this approach using two challenging tasks: the prediction of novel protein folds without the use of co-evolutionary information, and the prediction of known protein folds without the use of structural templates. On the first task the model achieves state-of-the-art performance, even when compared to methods that rely on co-evolutionary data. On the second task the model is competitive with methods that use experimental protein structures as templates, achieving 3-7Å accuracy despite being template-free. Beyond protein structure prediction, end-to-end differentiable models of proteins represent a new paradigm for learning and modeling protein structure, with potential applications in docking, molecular dynamics, and protein design.},
	language = {en},
	urldate = {2018-09-25},
	author = {AlQuraishi, Mohammed},
	month = aug,
	year = {2018},
}

@article{cohen_trapping_nodate,
	title = {{TRAPPING} {AND} {MANIPULATING} {SINGLE} {MOLECULES} {IN} {SOLUTION}},
	language = {en},
	author = {Cohen, Adam E},
	pages = {189},
}

@article{li_current_2018,
	title = {Current {Conjugation} {Methods} for {Immunosensors}},
	volume = {8},
	issn = {2079-4991},
	url = {http://www.mdpi.com/2079-4991/8/5/278},
	doi = {10.3390/nano8050278},
	abstract = {Recent advances in the development of immunosensors using polymeric nanomaterials and nanoparticles have enabled a wide range of new functions and applications in diagnostic and prognostic research. One fundamental challenge that all immunosensors must overcome is to provide the speciﬁcity of target molecular recognition by immobilizing antibodies, antibody fragments, and/or other peptides or oligonucleotide molecules that are capable of antigen recognition on a compact device surface. This review presents progress in the application of immobilization strategies including the classical adsorption process, afﬁnity attachment, random cross-linking and speciﬁc covalent linking. The choice of immobilization methods and its impact on biosensor performance in terms of capture molecule loading, orientation, stability and capture efﬁciency are also discussed in this review.},
	language = {en},
	number = {5},
	urldate = {2018-09-16},
	journal = {Nanomaterials},
	author = {Li, Zeyang and Chen, Guan-Yu},
	month = apr,
	year = {2018},
	keywords = {affinity, binding, biosensor, electrochemical   immunosensors, graphene oxide nanosheets, immobilization methods, immunosensors, monoclonal-antibody, optical immunosensors, orientation, polymeric nanomaterials, proteins, single-domain antibodies},
	pages = {278},
}

@article{hatzakis_single_2014,
	title = {Single molecule insights on conformational selection and induced fit mechanism},
	volume = {186},
	issn = {03014622},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0301462213001956},
	doi = {10.1016/j.bpc.2013.11.003},
	abstract = {Biomolecular interactions regulate a plethora of vital cellular processes, including signal transduction, metabolism, catalysis and gene regulation. Regulation is encoded in the molecular properties of the constituent proteins; distinct conformations correspond to different functional outcomes. To describe the molecular basis of this behavior, two main mechanisms have been advanced: ‘induced ﬁt’ and ‘conformational selection’. Our understanding of these models relies primarily on NMR, computational studies and kinetic measurements. These techniques report the average behavior of a large ensemble of unsynchronized molecules, often masking intrinsic dynamic behavior of proteins and biologically signiﬁcant transient intermediates. Single molecule measurements are emerging as a powerful tool for characterizing protein function. They offer the direct observation and quantiﬁcation of the activity, abundance and lifetime of multiple states and transient intermediates in the energy landscape, that are typically averaged out in non-synchronized ensemble measurements. Here we survey new insights from single molecule studies that advance our understanding of the molecular mechanisms underlying biomolecular recognition.},
	language = {en},
	urldate = {2018-09-16},
	journal = {Biophysical Chemistry},
	author = {Hatzakis, Nikos S.},
	month = feb,
	year = {2014},
	pages = {46--54},
}

@article{brown_robert_brief_1828,
	series = {New {Series}},
	title = {A brief {Account} of {Microscopical} {Observations} made in the {Months} of {June}, {July}, and {August}, 1827, on the {Particles} contained in the {Pollen} of {Plants}; and on the general {Existence} of active {Molecules} in {Organic} and {Inorganic} {Bodies}},
	volume = {4},
	language = {English},
	number = {21},
	journal = {Annals of Philosophy},
	author = {Brown, Robert},
	month = sep,
	year = {1828},
	pages = {161 -- 173},
}

@book{lindsay_introduction_2010,
	address = {Oxford},
	title = {Introduction to nanoscience},
	isbn = {978-0-19-954420-2 978-0-19-954421-9},
	publisher = {Oxford University Press},
	author = {Lindsay, S. M.},
	year = {2010},
	note = {OCLC: ocn465660440},
	keywords = {Nanoscience},
}

@book{noauthor_notitle_nodate,
}

@article{einstein_motion_1905,
	title = {On the {Motion} of {Small} {Particles} {Suspended} in {Liquids} at {Rest} {Required} by the {Molecular}-{Kinetic} {Theory} of {Heat}},
	abstract = {IN THIS PAPER it will be shown that, according to the molecular-kinetic theory of heat,
bodies of a microscopically visible size suspended in liquids must, as a result of thermal molecular
motions, perform motions of such magnitude that they can be easily observed with a microscope.
It is possible that the motions to be discussed here are identical with so-called Brownian molecular
motion; however, the data available to me on the latter are so imprecise that I could not form a
judgment on the question. If the motion to be discussed here can actually be observed, together
with the laws it is expected to obey, then classical thermodynamics can no longer be viewed as
applying to regions that can be distinguished even with a microscope, and an exact determination
of actual atomic sizes becomes possible. On the other hand, if the prediction of this motion were
to be proved wrong, this fact would provide a far-reaching argument against the molecular-kinetic
conception of heat.},
	language = {en},
	number = {17},
	journal = {Annalen der Physik},
	author = {Einstein, Albert},
	year = {1905},
	pages = {549--560},
}

@article{yang_resolving_2018,
	title = {Resolving {Mixtures} in {Solution} by {Single}-{Molecule} {Rotational} {Diffusivity}},
	volume = {18},
	issn = {1530-6984, 1530-6992},
	url = {http://pubs.acs.org/doi/10.1021/acs.nanolett.8b02280},
	doi = {10.1021/acs.nanolett.8b02280},
	abstract = {Sensing the size of individual molecules in an ensemble has proven to be a powerful tool to investigate biomolecular interactions and association−dissociation processes. In biologically relevant solution environments, molecular size is often sensed by translational or rotational diﬀusivity. The rotational diﬀusivity is more sensitive to the size and conformation of the molecules as it is inversely proportional to the cube of the hydrodynamic radius, as opposed to the inverse linear dependence of the translational diﬀusion coeﬃcient. Single-molecule rotational diﬀusivity has been measured with time-resolved ﬂuorescence anisotropy decay, but the ability to sense diﬀerent sizes has been restricted by the limited number of photons available or has required surface attachment to observe each molecule longer, and the attachment may be perturbative. To address these limitations, we show how to measure and monitor single-molecule rotational diﬀusivity by combining the solution-phase AntiBrownian ELectrokinetic (ABEL) trap and maximum likelihood analysis of time-resolved ﬂuorescence anisotropy based on the information inherent in each detected photon. We demonstrate this approach by resolving a mixture of single- and doublestranded ﬂuorescently labeled DNA molecules at equilibrium, freely rotating in a native solution environment. The rotational diﬀusivity, ﬂuorescence brightness and lifetime, and initial and steady-state anisotropy are simultaneously determined for each trapped single DNA molecule. The time resolution and precision of this method are analyzed using statistical signal analysis and simulations. We present key parameters that deﬁne the usefulness of a particular ﬂuorescent label for extracting molecular size information from single-molecule rotational diﬀusivity measurements.},
	language = {en},
	number = {8},
	urldate = {2018-09-16},
	journal = {Nano Letters},
	author = {Yang, Hsiang-Yu and Moerner, W. E.},
	month = aug,
	year = {2018},
	pages = {5279--5287},
}

@article{cohen_controlling_2008,
	title = {Controlling {Brownian} motion of single protein molecules and single fluorophores in aqueous buffer},
	volume = {16},
	issn = {1094-4087},
	doi = {10.1364/OE.16.006941},
	abstract = {We present an Anti-Brownian Electrokinetic trap (ABEL trap) capable of trapping individual fluorescently labeled protein molecules in aqueous buffer. The ABEL trap operates by tracking the Brownian motion of a single fluorescent particle in solution, and applying a time-dependent electric field designed to induce an electrokinetic drift that cancels the Brownian motion. The trapping strength of the ABEL trap is limited by the latency of the feedback loop. In previous versions of the trap, this latency was set by the finite frame rate of the camera used for video-tracking. In the present system, the motion of the particle is tracked entirely in hardware (without a camera or image-processing software) using a rapidly rotating laser focus and lock-in detection. The feedback latency is set by the finite rate of arrival of photons. We demonstrate trapping of individual molecules of the protein GroEL in buffer, and we show confinement of single fluorophores of the dye Cy3 in water. (C) 2008 Optical Society of America.},
	language = {English},
	number = {10},
	journal = {Optics Express},
	author = {Cohen, Adam E. and Moerner, W. E.},
	month = may,
	year = {2008},
	note = {WOS:000256469800020},
	keywords = {fluctuations, particle tracking},
	pages = {6941--6956},
}

@incollection{jiang_hardware-based_2008,
	address = {Bellingham},
	title = {Hardware-based anti-{Brownian} electrokinetic trap ({ABEL} trap) for single molecules: {Control} loop simulations and application to {ATP} binding stoichiometry in multi-subunit enzymes},
	volume = {7038},
	isbn = {978-0-8194-7258-8},
	shorttitle = {Hardware-based anti-{Brownian} electrokinetic trap ({ABEL} trap) for single molecules},
	abstract = {The hardware-based Anti-Brownian ELectrokinetic trap (ABEL trap) features a feedback latency as short as 25 mu s, suitable for trapping single protein molecules in aqueous solution. The performance of the feedback control loop is analyzed to extract estimates of the position variance for various controller designs. Preliminary data are presented in which the trap is applied to the problem of determining the distribution of numbers of ATP bound for single chaperonin multi-subunit enzymes.},
	language = {English},
	booktitle = {Optical {Trapping} and {Optical} {Micromanipulation} {V}},
	publisher = {Spie-Int Soc Optical Engineering},
	author = {Jiang, Yan and Wang, Quan and Cohen, Adam E. and Douglas, Nick and Frydman, Judith and Moerner, W. E.},
	editor = {Dholakia, K. and Spalding, G. C.},
	year = {2008},
	note = {WOS:000262711000004},
	keywords = {Brownian motion, chaperonin, electrokinetic trap, fluorophore counting, motion, multi-subunit enzymes, multiple particles, particle tracking, single molecule, system},
	pages = {703807},
}

@incollection{cohen_anti-brownian_2005,
	address = {Bellingham},
	title = {The {Anti}-{Brownian} {ELectrophoretic} trap ({ABEL} trap): {Fabrication} and software},
	volume = {5699},
	isbn = {978-0-8194-5673-1},
	shorttitle = {The {Anti}-{Brownian} {ELectrophoretic} trap ({ABEL} trap)},
	abstract = {The Anti-Brownian ELectrophoretic trap (ABEL trap) is a new device that allows a user to trap and manipulate fluorescent objects as small as 20 nm freely diffusing in solution. We describe in detail how to build an ABEL trap.},
	language = {English},
	booktitle = {Imaging, {Manipulation}, and {Analysis} of {Biomolecules} and {Cells}:   {Fundamentals} and {Applications} {III}},
	publisher = {Spie-Int Soc Optical Engineering},
	author = {Cohen, A. E.},
	editor = {Nicolau, D. V. and Enderlein, J. and Leif, R. C. and Farkas, D. L. and Raghavachari, R.},
	year = {2005},
	note = {WOS:000229039000036},
	keywords = {dynamics, molecule, protein, tracking},
	pages = {296--305},
}

@article{cohen_method_2005,
	title = {Method for trapping and manipulating nanoscale objects in solution},
	volume = {86},
	issn = {0003-6951, 1077-3118},
	url = {http://aip.scitation.org/doi/10.1063/1.1872220},
	doi = {10.1063/1.1872220},
	language = {en},
	number = {9},
	urldate = {2018-09-13},
	journal = {Applied Physics Letters},
	author = {Cohen, Adam E. and Moerner, W. E.},
	month = feb,
	year = {2005},
	pages = {093109},
}

@article{restrepo-perez_paving_2018,
	title = {Paving the way to single-molecule protein sequencing},
	volume = {13},
	copyright = {2018 The Author(s)},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/s41565-018-0236-6},
	doi = {10.1038/s41565-018-0236-6},
	abstract = {Dekker et al. review protein sequencing at the single-molecule level, an entirely new technique for which various approaches including sequencing using fluorescence, nanopores and tunnelling currents are discussed.},
	language = {en},
	number = {9},
	urldate = {2018-09-11},
	journal = {Nature Nanotechnology},
	author = {Restrepo-Pérez, Laura and Joo, Chirlmin and Dekker, Cees},
	month = sep,
	year = {2018},
	pages = {786--796},
}

@article{fatermans_single_2018,
	title = {Single {Atom} {Detection} from {Low} {Contrast}-to-{Noise} {Ratio} {Electron} {Microscopy} {Images}},
	volume = {121},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.121.056101},
	doi = {10.1103/PhysRevLett.121.056101},
	language = {en},
	number = {5},
	urldate = {2018-09-11},
	journal = {Physical Review Letters},
	author = {Fatermans, J. and den Dekker, A. J. and Müller-Caspary, K. and Lobato, I. and O’Leary, C. M. and Nellist, P. D. and Van Aert, S.},
	month = jul,
	year = {2018},
}

@article{faria_minimum_2018,
	title = {Minimum information reporting in bio–nano experimental literature},
	volume = {13},
	copyright = {2018 The Author(s)},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/s41565-018-0246-4},
	doi = {10.1038/s41565-018-0246-4},
	abstract = {A proposed list of characterization items aims at improving reproducibility and consistency in experiments reporting the use of nanoengineered materials in biological applications.},
	language = {en},
	number = {9},
	urldate = {2018-09-11},
	journal = {Nature Nanotechnology},
	author = {Faria, Matthew and Björnmalm, Mattias and Thurecht, Kristofer J. and Kent, Stephen J. and Parton, Robert G. and Kavallaris, Maria and Johnston, Angus P. R. and Gooding, J. Justin and Corrie, Simon R. and Boyd, Ben J. and Thordarson, Pall and Whittaker, Andrew K. and Stevens, Molly M. and Prestidge, Clive A. and Porter, Christopher J. H. and Parak, Wolfgang J. and Davis, Thomas P. and Crampin, Edmund J. and Caruso, Frank},
	month = sep,
	year = {2018},
	pages = {777--785},
}

@incollection{pujals_unveiling_2018,
	title = {Unveiling complex structure and dynamics in supramolecular biomaterials using super-resolution microscopy},
	isbn = {978-0-08-102015-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780081020159000137},
	language = {en},
	urldate = {2018-09-03},
	booktitle = {Self-assembling {Biomaterials}},
	publisher = {Elsevier},
	author = {Pujals, Sílvia and Feiner-Gracia, Natalia and Albertazzi, Lorenzo},
	year = {2018},
	doi = {10.1016/B978-0-08-102015-9.00013-7},
	pages = {251--274},
}

@article{bortolini_rapid_2018,
	title = {Rapid {Growth} of {Acetylated} {Aβ}(16–20) into {Macroscopic} {Crystals}},
	volume = {12},
	issn = {1936-0851, 1936-086X},
	url = {http://pubs.acs.org/doi/10.1021/acsnano.8b00448},
	doi = {10.1021/acsnano.8b00448},
	abstract = {Aberrant assembly of the amyloid-β (Aβ) is responsible for the development of Alzheimer’s disease, but can also be exploited to obtain highly functional biomaterials. The short Aβ fragment, KLVFF (Aβ16−20), is crucial for Aβ assembly and considered to be an Aβ aggregation inhibitor. Here, we show that acetylation of KLVFF turns it into an extremely fast selfassembling molecule, reaching macroscopic (i.e., mm) size in seconds. We show that KLVFF is metastable and that the self-assembly can be directed toward a crystalline or ﬁbrillar phase simply through chemical modiﬁcation, via acetylation or amidation of the peptide. Amidated KLVFF can form amyloid ﬁbrils; we observed folding events of such ﬁbrils occurring in as little as 60 ms. The ability of single KLVFF molecules to rapidly assemble as highly ordered macroscopic structures makes it a promising candidate for applications as a rapid-forming templating material.},
	language = {en},
	number = {6},
	urldate = {2018-09-03},
	journal = {ACS Nano},
	author = {Bortolini, Christian and Klausen, Lasse Hyldgaard and Hoffmann, Søren Vrønning and Jones, Nykola C. and Saadeh, Daniela and Wang, Zegao and Knowles, Tuomas P. J. and Dong, Mingdong},
	month = jun,
	year = {2018},
	pages = {5408--5416},
}

@article{yu_how_2011,
	title = {How {Liposomes} {Diffuse} in {Concentrated} {Liposome} {Suspensions}},
	volume = {115},
	issn = {1520-6106},
	url = {https://doi.org/10.1021/jp109146s},
	doi = {10.1021/jp109146s},
	abstract = {Building upon the observation that liposomes of zwitterionic lipids can be stabilized against fusion by the adsorption of cationic nanoparticles (Yu, Y.; Anthony, S.; Zhang, L.; Bae, S. C.; Granick, S. J. Phys. Chem. C2007, 111, 8233), we study, using single-particle fluorescence tracking, mobility in this distinctively deformable colloid system, in the volume fraction range of φ = 0.01 to 0.7. Liposome motion is diffusive and homogeneous at low volume fractions, but separable fast and slow populations emerge as the volume fraction increases beyond φ ≈ 0.45, the same volume fraction at which hard colloids with sufficiently strong attraction are known to experience gelation. This is reflected not only in scaling of the mean square displacement, but also in the step size distribution (van Hove function) measured by fluorescence imaging. The fast liposomes are observed to follow Brownian motion, and the slow ones follow anomalous diffusion characterized by a 1/3 time scaling of their mean square displacement.},
	number = {12},
	urldate = {2018-06-18},
	journal = {The Journal of Physical Chemistry B},
	author = {Yu, Yan and Anthony, Stephen M. and Bae, Sung Chul and Granick, Steve},
	month = mar,
	year = {2011},
	pages = {2748--2753},
}

@article{arya_quantum_2005,
	title = {Quantum dots in bio-imaging: {Revolution} by the small},
	volume = {329},
	issn = {0006-291X},
	shorttitle = {Quantum dots in bio-imaging},
	url = {http://www.sciencedirect.com/science/article/pii/S0006291X05002664},
	doi = {10.1016/j.bbrc.2005.02.043},
	abstract = {Visual analysis of biomolecules is an integral avenue of basic and applied biological research. It has been widely carried out by tagging of nucleotides and proteins with traditional fluorophores that are limited in their application by features such as photobleaching, spectral overlaps, and operational difficulties. Quantum dots (QDs) are emerging as a superior alternative and are poised to change the world of bio-imaging and further its applications in basic and applied biology. The interdisciplinary field of nanobiotechnology is experiencing a revolution and QDs as an enabling technology have become a harbinger of this hybrid field. Within a decade, research on QDs has evolved from being a pure science subject to the one with high-end commercial applications.},
	number = {4},
	urldate = {2018-06-18},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Arya, Harinder and Kaul, Zeenia and Wadhwa, Renu and Taira, Kazunari and Hirano, Takashi and Kaul, Sunil C.},
	month = apr,
	year = {2005},
	keywords = {Bio-imaging, Biomolecules, Quantum dots, Sensitive, Stable},
	pages = {1173--1177},
}

@article{beller_discovery_2018,
	title = {Discovery of enzymes for toluene synthesis from anoxic microbial communities},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1552-4469},
	url = {http://www-nature-com/articles/s41589-018-0017-4},
	doi = {10.1038/s41589-018-0017-4},
	abstract = {The source of biological toluene production in diverse\&nbsp;anoxic microbial communities is a glycyl radical enzyme that catalyzes phenylacetate decarboxylation (PhdB), and its cognate activating radical S-adenosylmethionine enzyme (PhdA).},
	language = {en},
	number = {5},
	urldate = {2018-06-16},
	journal = {Nature Chemical Biology},
	author = {Beller, Harry R. and Rodrigues, Andria V. and Zargar, Kamrun and Wu, Yu-Wei and Saini, Avneesh K. and Saville, Renee M. and Pereira, Jose H. and Adams, Paul D. and Tringe, Susannah G. and Petzold, Christopher J. and Keasling, Jay D.},
	month = may,
	year = {2018},
	pages = {451--457},
}

@article{pavlidis_using_2004,
	title = {Using the {Gene} {Ontology} for {Microarray} {Data} {Mining}: {A} {Comparison} of {Methods} and {Application} to {Age} {Effects} in {Human} {Prefrontal} {Cortex}},
	volume = {29},
	issn = {0364-3190, 1573-6903},
	shorttitle = {Using the {Gene} {Ontology} for {Microarray} {Data} {Mining}},
	url = {https://link.springer.com/article/10.1023/B:NERE.0000023608.29741.45},
	doi = {10.1023/B:NERE.0000023608.29741.45},
	abstract = {One of the challenges in the analysis of gene expression data is placing the results in the context of other data available about genes and their relationships to each other. Here, we approach this problem in the study of gene expression changes associated with age in two areas of the human prefrontal cortex, comparing two computational methods. The first method, “overrepresentation analysis” (ORA), is based on statistically evaluating the fraction of genes in a particular gene ontology class found among the set of genes showing age-related changes in expression. The second method, “functional class scoring” (FCS), examines the statistical distribution of individual gene scores among all genes in the gene ontology class and does not involve an initial gene selection step. We find that FCS yields more consistent results than ORA, and the results of ORA depended strongly on the gene selection threshold. Our findings highlight the utility of functional class scoring for the analysis of complex expression data sets and emphasize the advantage of considering all available genomic information rather than sets of genes that pass a predetermined “threshold of significance.”},
	language = {en},
	number = {6},
	urldate = {2018-06-11},
	journal = {Neurochemical Research},
	author = {Pavlidis, Paul and Qin, Jie and Arango, Victoria and Mann, John J. and Sibille, Etienne},
	month = jun,
	year = {2004},
	pages = {1213--1222},
}

@article{jackson_generalizability_2018,
	title = {On the {Generalizability} of {Linear} and {Non}-{Linear} {Region} of {Interest}-{Based} {Multivariate} {Regression} {Models} for {fMRI} {Data}},
	url = {http://arxiv.org/abs/1802.02423},
	abstract = {In contrast to conventional, univariate analysis, various types of multivariate analysis have been applied to functional magnetic resonance imaging (fMRI) data. In this paper, we compare two contemporary approaches for multivariate regression on task-based fMRI data: linear regression with ridge regularization and non-linear symbolic regression using genetic programming. The data for this project is representative of a contemporary fMRI experimental design for visual stimuli. Linear and non-linear models were generated for 10 subjects, with another 4 withheld for validation. Model quality is evaluated by comparing R scores (Pearson product-moment correlation) in various contexts, including single run self-ﬁt, within-subject generalization, and between-subject generalization. Propensity for modelling strategies to overﬁt is estimated using a separate resting state scan. Results suggest that neither method is objectively or inherently better than the other.},
	language = {en},
	urldate = {2018-06-07},
	journal = {arXiv:1802.02423 [cs, stat]},
	author = {Jackson, Ethan C. and Hughes, James Alexander and Daley, Mark},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.02423},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Applications},
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2018-06-02},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	year = {1936},
	pages = {179--188},
}

@article{wei_super-multiplex_2017,
	title = {Super-multiplex vibrational imaging},
	volume = {544},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature22051},
	doi = {10.1038/nature22051},
	abstract = {The ability to visualize directly a large number of distinct molecular species inside cells is increasingly essential for understanding complex systems and processes. Even though existing methods have successfully been used to explore structure–function relationships in nervous systems, to profile RNA in situ, to reveal the heterogeneity of tumour microenvironments and to study dynamic macromolecular assembly1,2,3,4, it remains challenging to image many species with high selectivity and sensitivity under biological conditions. For instance, fluorescence microscopy faces a ‘colour barrier’, owing to the intrinsically broad (about 1,500 inverse centimetres) and featureless nature of fluorescence spectra5 that limits the number of resolvable colours to two to five (or seven to nine if using complicated instrumentation and analysis)6,7,8. Spontaneous Raman microscopy probes vibrational transitions with much narrower resonances (peak width of about 10 inverse centimetres) and so does not suffer from this problem, but weak signals make many bio-imaging applications impossible. Although surface-enhanced Raman scattering offers high sensitivity and multiplicity, it cannot be readily used to image specific molecular targets quantitatively inside live cells9. Here we use stimulated Raman scattering under electronic pre-resonance conditions to image target molecules inside living cells with very high vibrational selectivity and sensitivity (down to 250 nanomolar with a time constant of 1 millisecond). We create a palette of triple-bond-conjugated near-infrared dyes that each displays a single peak in the cell-silent Raman spectral window; when combined with available fluorescent probes, this palette provides 24 resolvable colours, with the potential for further expansion. Proof-of-principle experiments on neuronal co-cultures and brain tissues reveal cell-type-dependent heterogeneities in DNA and protein metabolism under physiological and pathological conditions, underscoring the potential of this 24-colour (super-multiplex) optical imaging approach for elucidating intricate interactions in complex biological systems.},
	language = {en},
	number = {7651},
	urldate = {2018-06-04},
	journal = {Nature},
	author = {Wei, Lu and Chen, Zhixing and Shi, Lixue and Long, Rong and Anzalone, Andrew V. and Zhang, Luyuan and Hu, Fanghao and Yuste, Rafael and Cornish, Virginia W. and Min, Wei},
	month = apr,
	year = {2017},
	pages = {465--470},
}

@article{allen_liposomal_2013,
	series = {Advanced {Drug} {Delivery}: {Perspectives} and {Prospects}},
	title = {Liposomal drug delivery systems: {From} concept to clinical applications},
	volume = {65},
	issn = {0169-409X},
	shorttitle = {Liposomal drug delivery systems},
	url = {http://www.sciencedirect.com/science/article/pii/S0169409X12002980},
	doi = {10.1016/j.addr.2012.09.037},
	abstract = {The first closed bilayer phospholipid systems, called liposomes, were described in 1965 and soon were proposed as drug delivery systems. The pioneering work of countless liposome researchers over almost 5 decades led to the development of important technical advances such as remote drug loading, extrusion for homogeneous size, long-circulating (PEGylated) liposomes, triggered release liposomes, liposomes containing nucleic acid polymers, ligand-targeted liposomes and liposomes containing combinations of drugs. These advances have led to numerous clinical trials in such diverse areas as the delivery of anti-cancer, anti-fungal and antibiotic drugs, the delivery of gene medicines, and the delivery of anesthetics and anti-inflammatory drugs. A number of liposomes (lipidic nanoparticles) are on the market, and many more are in the pipeline. Lipidic nanoparticles are the first nanomedicine delivery system to make the transition from concept to clinical application, and they are now an established technology platform with considerable clinical acceptance. We can look forward to many more clinical products in the future.},
	number = {1},
	urldate = {2018-06-03},
	journal = {Advanced Drug Delivery Reviews},
	author = {Allen, Theresa M. and Cullis, Pieter R.},
	month = jan,
	year = {2013},
	keywords = {Anti-cancer drugs, Biodistribution, Ligand-targeted, Lipidic nanoparticle, Liposome, Pharmacokinetics, Polyethylene glycol, siRNA},
	pages = {36--48},
}

@article{zylberberg_pharmaceutical_2016,
	title = {Pharmaceutical liposomal drug delivery: a review of new delivery systems and a look at the regulatory landscape},
	volume = {23},
	issn = {1071-7544},
	shorttitle = {Pharmaceutical liposomal drug delivery},
	url = {https://doi.org/10.1080/10717544.2016.1177136},
	doi = {10.1080/10717544.2016.1177136},
	abstract = {Liposomes were the first nanoscale drug to be approved for clinical use in 1995. Since then, the technology has grown considerably, and pioneering recent work in liposome-based delivery systems has brought about remarkable developments with significant clinical implications. This includes long-circulating liposomes, stimuli-responsive liposomes, nebulized liposomes, elastic liposomes for topical, oral and transdermal delivery and covalent lipid-drug complexes for improved drug plasma membrane crossing and targeting to specific organelles. While the regulatory bodies’ opinion on liposomes is well-documented, current guidance that address new delivery systems are not. This review describes, in depth, the current state-of-the-art of these new liposomal delivery systems and provides a critical overview of the current regulatory landscape surrounding commercialization efforts of higher-level complexity systems, the expected requirements and the hurdles faced by companies seeking to bring novel liposome-based systems for clinical use to market.},
	number = {9},
	urldate = {2018-06-03},
	journal = {Drug Delivery},
	author = {Zylberberg, Claudia and Matosevic, Sandro},
	month = nov,
	year = {2016},
	pmid = {27145899},
	keywords = {Bioengineering, drug delivery, liposomes, regulatory landscape, synthetic biology},
	pages = {3319--3329},
}

@article{pollheimer_reversible_2013,
	title = {Reversible biofunctionalization of surfaces with a switchable mutant of avidin},
	volume = {24},
	issn = {1520-4812},
	doi = {10.1021/bc400087e},
	abstract = {Label-free biosensors detect binding of prey molecules (″analytes″) to immobile bait molecules on the sensing surface. Numerous methods are available for immobilization of bait molecules. A convenient option is binding of biotinylated bait molecules to streptavidin-functionalized surfaces, or to biotinylated surfaces via biotin-avidin-biotin bridges. The goal of this study was to find a rapid method for reversible immobilization of biotinylated bait molecules on biotinylated sensor chips. The task was to establish a biotin-avidin-biotin bridge which was easily cleaved when desired, yet perfectly stable under a wide range of measurement conditions. The problem was solved with the avidin mutant M96H which contains extra histidine residues at the subunit-subunit interfaces. This mutant was bound to a mixed self-assembled monolayer (SAM) containing biotin residues on 20\% of the oligo(ethylene glycol)-terminated SAM components. Various biotinylated bait molecules were bound on top of the immobilized avidin mutant. The biotin-avidin-biotin bridge was stable at pH ≥3, and it was insensitive to sodium dodecyl sulfate (SDS) at neutral pH. Only the combination of citric acid (2.5\%, pH 2) and SDS (0.25\%) caused instantaneous cleavage of the biotin-avidin-biotin bridge. As a consequence, the biotinylated bait molecules could be immobilized and removed as often as desired, the only limit being the time span for reproducible chip function when kept in buffer (2-3 weeks at 25 °C). As expected, the high isolectric pH (pI) of the avidin mutant caused nonspecific adsorption of proteins. This problem was solved by acetylation of avidin (to pI {\textless} 5), or by optimization of SAM formation and passivation with biotin-BSA and BSA.},
	language = {eng},
	number = {10},
	journal = {Bioconjugate Chemistry},
	author = {Pollheimer, Philipp and Taskinen, Barbara and Scherfler, Andreas and Gusenkov, Sergey and Creus, Marc and Wiesauer, Philipp and Zauner, Dominik and Schöfberger, Wolfgang and Schwarzinger, Clemens and Ebner, Andreas and Tampé, Robert and Stutz, Hanno and Hytönen, Vesa P. and Gruber, Hermann J.},
	month = oct,
	year = {2013},
	pmid = {23978112},
	keywords = {Animals, Avidin, Biosensing Techniques, Biotin, Biotinylation, Immobilized Proteins, Point Mutation, Surface Properties},
	pages = {1656--1668},
}

@misc{noauthor_spectra_nodate,
	title = {Spectra at {UA}},
	url = {http://www.spectra.arizona.edu/},
	urldate = {2018-05-31},
}

@article{cheezum_quantitative_2001,
	title = {Quantitative {Comparison} of {Algorithms} for {Tracking} {Single} {Fluorescent} {Particles}},
	volume = {81},
	issn = {0006-3495},
	url = {http://www.sciencedirect.com/science/article/pii/S0006349501758845},
	doi = {10.1016/S0006-3495(01)75884-5},
	abstract = {Single particle tracking has seen numerous applications in biophysics, ranging from the diffusion of proteins in cell membranes to the movement of molecular motors. A plethora of computer algorithms have been developed to monitor the sub-pixel displacement of fluorescent objects between successive video frames, and some have been claimed to have “nanometer” resolution. To date, there has been no rigorous comparison of these algorithms under realistic conditions. In this paper, we quantitatively compare specific implementations of four commonly used tracking algorithms: cross-correlation, sum-absolute difference, centroid, and direct Gaussian fit. Images of fluorescent objects ranging in size from point sources to 5μm were computer generated with known sub-pixel displacements. Realistic noise was added and the above four algorithms were compared for accuracy and precision. We found that cross-correlation is the most accurate algorithm for large particles. However, for point sources, direct Gaussian fit to the intensity distribution is the superior algorithm in terms of both accuracy and precision, and is the most robust at low signal-to-noise. Most significantly, all four algorithms fail as the signal-to-noise ratio approaches 4. We judge direct Gaussian fit to be the best algorithm when tracking single fluorophores, where the signal-to-noise is frequently near 4.},
	number = {4},
	urldate = {2018-05-30},
	journal = {Biophysical Journal},
	author = {Cheezum, Michael K. and Walker, William F. and Guilford, William H.},
	month = oct,
	year = {2001},
	pages = {2378--2388},
}

@inproceedings{zuoju_study_2014,
	title = {A {Study} of {Threshold} {De}-noising {Method} and {Its} {Application} in {Signal} {Processing}},
	doi = {10.1109/ISDEA.2014.27},
	abstract = {In this article, the characteristics, theories and classification of the threshold de-noising method are introduced in detail. According to the study, when wavelet threshold de-noising method is compared with the wavelet packet threshold de-noising method, the first method is suitable for dealing with the low frequency information, but the second method is available to deal with the information with high frequency. On the other hand, through the study of this thesis, we found the soft-threshold processing method is better than the hard-threshold processing method in most cases.},
	booktitle = {2014 {Fifth} {International} {Conference} on {Intelligent} {Systems} {Design} and {Engineering} {Applications}},
	author = {Zuoju, W. and Junwei, B. and Zhijia, W.},
	month = jun,
	year = {2014},
	keywords = {Noise, Noise reduction, Wavelet analysis, Wavelet coefficients, Wavelet packets, hard-threshold processing method, low frequency information, signal denoising, signal processing, soft-threshold processing method, threshold de-noising method, wavelet packet threshold denoising method, wavelet packet transform, wavelet threshold denoising method, wavelet transform, wavelet transforms},
	pages = {88--91},
}

@inproceedings{othman_image_2010,
	title = {Image thresholding using neural network},
	doi = {10.1109/ISDA.2010.5687030},
	abstract = {Image thresholding is a very important phase in the image analysis process. However, different images have different characteristics making the traditional process of thresholding by one algorithm a very challenging task. That is because any thresholding method may be perform well for some images but for sure it will not be suitable for all images. In this paper, intelligent thresholding by training a neural network is proposed. The neural network is trained using a set of features extracted from medical images randomly selected form a sample set and then tested using the remaining medical images. This process is repeated multiple times to verify the generalization ability of the network. The average of segmentation accuracy is calculated by comparing every segmented image with its gold standard image.},
	booktitle = {2010 10th {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	author = {Othman, A. A. and Tizhoosh, H. R.},
	month = nov,
	year = {2010},
	keywords = {Accuracy, Artificial neural networks, Biomedical imaging, Feature extraction, Image segmentation, Pixel, Training, feature extraction, image analysis, image segmentation, image thresholding, intelligent thresholding, medical image processing, medical images, neural nets, neural network},
	pages = {1159--1164},
}

@article{jaqaman_robust_2008,
	title = {Robust single-particle tracking in live-cell time-lapse sequences},
	volume = {5},
	copyright = {2008 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1237},
	doi = {10.1038/nmeth.1237},
	abstract = {Single-particle tracking (SPT) is often the rate-limiting step in live-cell imaging studies of subcellular dynamics. Here we present a tracking algorithm that addresses the principal challenges of SPT, namely high particle density, particle motion heterogeneity, temporary particle disappearance, and particle merging and splitting. The algorithm first links particles between consecutive frames and then links the resulting track segments into complete trajectories. Both steps are formulated as global combinatorial optimization problems whose solution identifies the overall most likely set of particle trajectories throughout a movie. Using this approach, we show that the GTPase dynamin differentially affects the kinetics of long- and short-lived endocytic structures and that the motion of CD36 receptors along cytoskeleton-mediated linear tracks increases their aggregation probability. Both applications indicate the requirement for robust and complete tracking of dense particle fields to dissect the mechanisms of receptor organization at the level of the plasma membrane.},
	language = {en},
	number = {8},
	urldate = {2018-05-28},
	journal = {Nature Methods},
	author = {Jaqaman, Khuloud and Loerke, Dinah and Mettlen, Marcel and Kuwata, Hirotaka and Grinstein, Sergio and Schmid, Sandra L. and Danuser, Gaudenz},
	month = aug,
	year = {2008},
	pages = {695--702},
}

@article{serge_dynamic_2008,
	title = {Dynamic multiple-target tracing to probe spatiotemporal cartography of cell membranes},
	volume = {5},
	copyright = {2008 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1233},
	doi = {10.1038/nmeth.1233},
	abstract = {Although the highly dynamic and mosaic organization of the plasma membrane is well-recognized, depicting a resolved, global view of this organization remains challenging. We present an analytical single-particle tracking (SPT) method and tool, multiple-target tracing (MTT), that takes advantage of the high spatial resolution provided by single-fluorophore sensitivity. MTT can be used to generate dynamic maps at high densities of tracked particles, thereby providing global representation of molecular dynamics in cell membranes. Deflation by subtracting detected peaks allows detection of lower-intensity peaks. We exhaustively detected particles using MTT, with performance reaching theoretical limits, and then reconnected trajectories integrating the statistical information from past trajectories. We demonstrate the potential of this method by applying it to the epidermal growth factor receptor (EGFR) labeled with quantum dots (Qdots), in the plasma membrane of live cells. We anticipate the use of MTT to explore molecular dynamics and interactions at the cell membrane.},
	language = {en},
	number = {8},
	urldate = {2018-05-28},
	journal = {Nature Methods},
	author = {Sergé, Arnauld and Bertaux, Nicolas and Rigneault, Hervé and Marguet, Didier},
	month = aug,
	year = {2008},
	pages = {687--694},
}

@article{hajj_whole-cell_2014,
	title = {Whole-cell, multicolor superresolution imaging using volumetric multifocus microscopy},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1412396111},
	doi = {10.1073/pnas.1412396111},
	language = {en},
	number = {49},
	urldate = {2018-05-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hajj, Bassam and Wisniewski, Jan and El Beheiry, Mohamed and Chen, Jiji and Revyakin, Andrey and Wu, Carl and Dahan, Maxime},
	month = dec,
	year = {2014},
	pages = {17480--17485},
}

@article{winterhalter_liposome_1993,
	title = {Liposome stability and formation: {Experimental} parameters and theories on the size distribution},
	volume = {64},
	issn = {0009-3084},
	shorttitle = {Liposome stability and formation},
	url = {http://www.sciencedirect.com/science/article/pii/0009308493900569},
	doi = {10.1016/0009-3084(93)90056-9},
	abstract = {The most important characteristics of liposomes, in addition to chemical composition and surface properties, are size distribution and lamellarity. Liposomes can be formed by many different preparation techniques, which according to the literature yield rather well defined vesicle preparations. In contrast to abundant information on the experimental procedures and preparation protocols the theoretical understanding of these processes is lacking. Only geometrical models of structural changes exist for few preparation procedures and size of the liposomes prepared by sonication and detergent depletion method were estimated using simple models. In this paper we first outline different theories on the stability of liposomes and their influence on the size distribution. In the experimental section we shall briefly present the importance of size distribution in experimental work and influence of various experimental parameters on the size distributions obtained.},
	number = {1},
	urldate = {2018-05-28},
	journal = {Chemistry and Physics of Lipids},
	author = {Winterhalter, M. and Lasic, D. D.},
	month = sep,
	year = {1993},
	keywords = {formation, size distribution, stability, thermodynamics, vesicles},
	pages = {35--43},
}

@article{nilsson_lipid-mediated_2016,
	title = {Lipid-mediated {Protein}-protein {Interactions} {Modulate} {Respiration}-driven {ATP} {Synthesis}},
	volume = {6},
	copyright = {2016 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep24113},
	doi = {10.1038/srep24113},
	abstract = {Energy conversion in biological systems is underpinned by membrane-bound proton transporters that generate and maintain a proton electrochemical gradient across the membrane which used, e.g. for generation of ATP by the ATP synthase. Here, we have co-reconstituted the proton pump cytochrome bo3 (ubiquinol oxidase) together with ATP synthase in liposomes and studied the effect of changing the lipid composition on the ATP synthesis activity driven by proton pumping. We found that for 100 nm liposomes, containing 5 of each proteins, the ATP synthesis rates decreased significantly with increasing fractions of DOPA, DOPE, DOPG or cardiolipin added to liposomes made of DOPC; with e.g. 5\% DOPG, we observed an almost 50\% decrease in the ATP synthesis rate. However, upon increasing the average distance between the proton pumps and ATP synthases, the ATP synthesis rate dropped and the lipid dependence of this activity vanished. The data indicate that protons are transferred along the membrane, between cytochrome bo3 and the ATP synthase, but only at sufficiently high protein densities. We also argue that the local protein density may be modulated by lipid-dependent changes in interactions between the two proteins complexes, which points to a mechanism by which the cell may regulate the overall activity of the respiratory chain.},
	language = {en},
	urldate = {2018-05-22},
	journal = {Scientific Reports},
	author = {Nilsson, Tobias and Lundin, Camilla Rydström and Nordlund, Gustav and Ädelroth, Pia and Ballmoos, Christoph von and Brzezinski, Peter},
	month = apr,
	year = {2016},
	pages = {24113},
}

@article{shibata_polyethylene_2012,
	title = {Polyethylene glycol prevents in vitro aggregation of slightly negatively-charged liposomes induced by heparin in the presence of bivalent ions},
	volume = {35},
	issn = {1347-5215},
	abstract = {Liposomes are of great interest as drug delivery vehicles, and studies have focused on understanding how the physical and chemical characteristics of liposomes can be modified to improve their in vivo behavior. In a previous study, we found that the slightly negatively-charged liposomes aggregate only in the culture medium of human umbilical vein endothelial cells, whereas the liposomes modified with polyethylene glycol (PEG) (PEGylated) did not aggregate. In the present study, we investigated the underlying mechanism of this phenomenon. Firstly, it was found that heparin in the culture medium is one of the factors that cause aggregation of the non-PEGylated liposomes. Since the addition of ethylenediaminetetraacetic acid (EDTA) prevented the aggregation, metal ions, such as Ca(2+) and Mg(2+), in the culture medium could also be important in driving the aggregation. In the presence of heparin, higher concentrations of Ca(2+) or Mg(2+) increased the particle size of the non-PEGylated liposomes, although no change in the particle size of PEGylated liposomes was observed. Under conditions in which aggregation occurred, we measured the binding and uptake of liposomes by macrophages in vitro. The binding and uptake of non-PEGylated liposomes were significantly increased with increasing Ca(2+) concentrations, whereas those of PEGylated liposomes were unchanged. While the formation of aggregations of cationic or anionic liposomes has been reported previously, there are few reports addressing the aggregation of slightly negatively-charged or neutral liposomes. Thus, our data provide useful insights on the effect of PEGylation on liposomal aggregation and in vivo behavior.},
	language = {eng},
	number = {11},
	journal = {Biological \& Pharmaceutical Bulletin},
	author = {Shibata, Hiroko and Yomota, Chikako and Kawanishi, Toru and Okuda, Haruhiro},
	year = {2012},
	pmid = {23123478},
	keywords = {Animals, Anticoagulants, Calcium, Cell Line, Chelating Agents, Doxorubicin, Edetic Acid, Heparin, Liposomes, Macrophages, Magnesium, Mice, Polyethylene Glycols},
	pages = {2081--2087},
}

@article{kunding_fluorescence-based_2008,
	title = {A {Fluorescence}-{Based} {Technique} to {Construct} {Size} {Distributions} from {Single}-{Object} {Measurements}: {Application} to the {Extrusion} of {Lipid} {Vesicles}},
	volume = {95},
	issn = {0006-3495},
	shorttitle = {A {Fluorescence}-{Based} {Technique} to {Construct} {Size} {Distributions} from {Single}-{Object} {Measurements}},
	url = {https://www.cell.com/biophysj/abstract/S0006-3495(08)70188-7},
	doi = {10.1529/biophysj.108.128819},
	language = {English},
	number = {3},
	urldate = {2018-05-22},
	journal = {Biophysical Journal},
	author = {Kunding, Andreas H. and Mortensen, Michael W. and Christensen, Sune M. and Stamou, Dimitrios},
	month = aug,
	year = {2008},
	pmid = {18424503},
	pages = {1176--1188},
}

@article{christensen_single_2013,
	title = {Single vesicle biochips for ultra-miniaturized nanoscale fluidics and single molecule bioscience},
	volume = {13},
	issn = {1473-0197, 1473-0189},
	url = {http://xlink.rsc.org/?DOI=c3lc50492a},
	doi = {10.1039/c3lc50492a},
	language = {en},
	number = {18},
	urldate = {2018-05-22},
	journal = {Lab on a Chip},
	author = {Christensen, Andreas L. and Lohr, Christina and Christensen, Sune M. and Stamou, Dimitrios},
	year = {2013},
	pages = {3613},
}

@article{hughes_choose_2014,
	title = {Choose {Your} {Label} {Wisely}: {Water}-{Soluble} {Fluorophores} {Often} {Interact} with {Lipid} {Bilayers}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Choose {Your} {Label} {Wisely}},
	url = {http://dx.plos.org/10.1371/journal.pone.0087649},
	doi = {10.1371/journal.pone.0087649},
	language = {en},
	number = {2},
	urldate = {2018-05-22},
	journal = {PLoS ONE},
	author = {Hughes, Laura D. and Rawle, Robert J. and Boxer, Steven G.},
	editor = {Gruenberg, Jean},
	month = feb,
	year = {2014},
	pages = {e87649},
}

@book{prasad_introduction_2003,
	address = {Hoboken, NJ},
	title = {Introduction to biophotonics},
	isbn = {978-0-471-28770-4},
	publisher = {Wiley-Interscience},
	author = {Prasad, Paras N.},
	year = {2003},
	keywords = {Biosensors, Nanotechnology, Photobiology, Photonics},
}

@article{kennedy_reading_2016,
	title = {Reading the primary structure of a protein with 0.07 nm$^{\textrm{3}}$ resolution using a subnanometre-diameter pore},
	volume = {11},
	copyright = {2016 Nature Publishing Group},
	issn = {1748-3395},
	url = {https://www.nature.com/articles/nnano.2016.120},
	doi = {10.1038/nnano.2016.120},
	abstract = {The primary structure of a protein consists of a sequence of amino acids and is a key factor in determining how a protein folds and functions. However, conventional methods for sequencing proteins, such as mass spectrometry and Edman degradation, suffer from short reads and lack sensitivity, so alternative approaches are sought. Here, we show that a subnanometre-diameter pore, sputtered through a thin silicon nitride membrane, can be used to detect the primary structure of a denatured protein molecule. When a denatured protein immersed in electrolyte is driven through the pore by an electric field, measurements of a blockade in the current reveal nearly regular fluctuations, the number of which coincides with the number of residues in the protein. Furthermore, the amplitudes of the fluctuations are highly correlated with the volumes that are occluded by quadromers (four residues) in the primary structure. Each fluctuation, therefore, represents a read of a quadromer. Scrutiny of the fluctuations reveals that the subnanometre pore is sensitive enough to read the occluded volume that is related to post-translational modifications of a single residue, measuring volume differences of ∼0.07 nm3, but it is not sensitive enough to discriminate between the volumes of all twenty amino acids.},
	language = {en},
	number = {11},
	urldate = {2018-05-15},
	journal = {Nature Nanotechnology},
	author = {Kennedy, Eamonn and Dong, Zhuxin and Tennant, Clare and Timp, Gregory},
	month = nov,
	year = {2016},
	pages = {968--976},
}

@misc{eisenstein_nanobiotechnology:_2016,
	type = {Research {Highlights}},
	title = {Nanobiotechnology: {Teaching} nanopores to speak protein},
	copyright = {2016 Nature Publishing Group},
	shorttitle = {Nanobiotechnology},
	url = {https://www.nature.com/articles/nmeth.3988},
	abstract = {A proof-of-concept platform demonstrates the feasibility of nanopore-based sequencing of polypeptide chains.},
	language = {en},
	urldate = {2018-05-15},
	journal = {Nature Methods},
	author = {Eisenstein, Michael},
	month = aug,
	year = {2016},
	doi = {10.1038/nmeth.3988},
}

@article{larsen_observation_2011,
	title = {Observation of {Inhomogeneity} in the {Lipid} {Composition} of {Individual} {Nanoscale} {Liposomes}},
	volume = {133},
	issn = {0002-7863},
	url = {https://doi.org/10.1021/ja203984j},
	doi = {10.1021/ja203984j},
	abstract = {Liposomes, or vesicles, have been studied extensively both as models of biological membranes and as drug delivery vehicles. Typically it is assumed that all liposomes within the same preparation are identical. Here by employing pairs of fluorescently labeled lipids we demonstrated an up to 10-fold variation in the relative lipid composition of individual liposomes with diameters between 50 nm and 15 μm. Since the physicochemical properties of liposomes are directly linked to their composition, a direct consequence of compositional inhomogeneities is a polydispersity in the properties of the individual liposomes in an ensemble.},
	number = {28},
	urldate = {2018-05-15},
	journal = {Journal of the American Chemical Society},
	author = {Larsen, Jannik and Hatzakis, Nikos S. and Stamou, Dimitrios},
	month = jul,
	year = {2011},
	pages = {10685--10687},
}

@article{larsen_membrane_2015,
	title = {Membrane curvature enables {N}-{Ras} lipid anchor sorting to liquid-ordered membrane phases},
	volume = {11},
	issn = {1552-4450, 1552-4469},
	url = {http://www.nature.com/articles/nchembio.1733},
	doi = {10.1038/nchembio.1733},
	language = {en},
	number = {3},
	urldate = {2018-05-15},
	journal = {Nature Chemical Biology},
	author = {Larsen, Jannik Bruun and Jensen, Martin Borch and Bhatia, Vikram K and Pedersen, Søren L and Bjørnholm, Thomas and Iversen, Lars and Uline, Mark and Szleifer, Igal and Jensen, Knud J and Hatzakis, Nikos S and Stamou, Dimitrios},
	month = mar,
	year = {2015},
	pages = {192--194},
}

@article{hatzakis_how_2009,
	title = {How curved membranes recruit amphipathic helices and protein anchoring motifs},
	volume = {5},
	issn = {1552-4450, 1552-4469},
	url = {http://www.nature.com/articles/nchembio.213},
	doi = {10.1038/nchembio.213},
	language = {en},
	number = {11},
	urldate = {2018-05-15},
	journal = {Nature Chemical Biology},
	author = {Hatzakis, Nikos S and Bhatia, Vikram K and Larsen, Jannik and Madsen, Kenneth L and Bolinger, Pierre-Yves and Kunding, Andreas H and Castillo, John and Gether, Ulrik and Hedegård, Per and Stamou, Dimitrios},
	month = nov,
	year = {2009},
	pages = {835--841},
}

@misc{noauthor_how_nodate,
	title = {How curved membranes recruit amphipathic helices and protein anchoring motifs {\textbar} {Nature} {Chemical} {Biology}},
	url = {https://www.nature.com/articles/nchembio.213?error=cookies_not_supported&code=955eae6b-4d3e-41f1-9a57-47f49840df6e},
	urldate = {2018-05-15},
}

@misc{noauthor_membrane_nodate,
	title = {Membrane curvature enables {N}-{Ras} lipid anchor sorting to liquid-ordered membrane phases {\textbar} {Nature} {Chemical} {Biology}},
	url = {https://www.nature.com/articles/nchembio.1733},
	urldate = {2018-05-15},
}

@article{ma_watching_2018,
	title = {Watching {Three}-{Dimensional} {Movements} of {Single} {Membrane} {Proteins} in {Lipid} {Bilayers}},
	issn = {0006-2960},
	url = {https://doi.org/10.1021/acs.biochem.8b00253},
	doi = {10.1021/acs.biochem.8b00253},
	abstract = {It is challenging to assess protein–membrane interactions because of the lack of appropriate tools to detect position changes of single proteins in the ∼4 nm range of biological membranes. We developed an assay recently, termed surface-induced fluorescence attenuation (SIFA). It is able to track both vertical and lateral dynamic motion of singly labeled membrane proteins in supported lipid bilayers. Similar to the FRET (fluorescence resonance energy transfer) principle, SIFA takes advantage of the energy transfer from a fluorophore to a light-absorbing surface to determine the distance at 2–8 nm away from the surface. By labeling a protein with a proper fluorophore and using graphene oxide as a two-dimensional quencher, we showed that SIFA is capable of monitoring three-dimensional movements of the fluorophore-labeled protein not only inside but also above the lipid bilayer atop the graphene oxide. Our data show that SIFA is a well-suited method to study the interplay between proteins and membranes.},
	urldate = {2018-05-15},
	journal = {Biochemistry},
	author = {Ma, Li and Li, Ying and Ma, Jianbing and Hu, Shuxin and Li, Ming},
	month = apr,
	year = {2018},
}

@article{moremen_vertebrate_2012,
	title = {Vertebrate protein glycosylation: diversity, synthesis and function},
	volume = {13},
	issn = {1471-0072, 1471-0080},
	shorttitle = {Vertebrate protein glycosylation},
	url = {http://www.nature.com/articles/nrm3383},
	doi = {10.1038/nrm3383},
	language = {en},
	number = {7},
	urldate = {2018-05-15},
	journal = {Nature Reviews Molecular Cell Biology},
	author = {Moremen, Kelley W. and Tiemeyer, Michael and Nairn, Alison V.},
	month = jul,
	year = {2012},
	pages = {448--462},
}

@article{tsunoyama_super-long_2018,
	title = {Super-long single-molecule tracking reveals dynamic-anchorage-induced integrin function},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1552-4469},
	url = {https://www.nature.com/articles/s41589-018-0032-5},
	doi = {10.1038/s41589-018-0032-5},
	abstract = {Dissolved oxygen and a reducing-plus-oxidizing system suppress photobleaching and photoblinking in single-molecule tracking experiments, allowing long recordings of CD47 and integrin that showed temporary immobilization within focal adhesions.},
	language = {en},
	number = {5},
	urldate = {2018-05-01},
	journal = {Nature Chemical Biology},
	author = {Tsunoyama, Taka A. and Watanabe, Yusuke and Goto, Junri and Naito, Kazuma and Kasai, Rinshi S. and Suzuki, Kenichi G. N. and Fujiwara, Takahiro K. and Kusumi, Akihiro},
	month = may,
	year = {2018},
	pages = {497--506},
}

@article{woodbury_reducing_2006,
	title = {Reducing {Liposome} {Size} with {Ultrasound}: {Bimodal} {Size} {Distributions}},
	volume = {16},
	issn = {0898-2104, 1532-2394},
	shorttitle = {Reducing {Liposome} {Size} with {Ultrasound}},
	url = {http://www.tandfonline.com/doi/full/10.1080/08982100500528842},
	doi = {10.1080/08982100500528842},
	language = {en},
	number = {1},
	urldate = {2018-03-21},
	journal = {Journal of Liposome Research},
	author = {Woodbury, Dixon J. and Richardson, Eric S. and Grigg, Aaron W. and Welling, Rodney D. and Knudson, Brian H.},
	month = jan,
	year = {2006},
	pages = {57--80},
}

@book{coleman_effect_2005,
	title = {The effect of anharmonicity on diatomic vibration: {A} spreadsheet simulation},
	shorttitle = {The effect of anharmonicity on diatomic vibration},
	publisher = {ACS Publications},
	author = {Coleman, William F. and Lim, Kieran F.},
	year = {2005},
}

@article{gunnarsson_liposome-based_2010,
	title = {Liposome-{Based} {Chemical} {Barcodes} for {Single} {Molecule} {DNA} {Detection} {Using} {Imaging} {Mass} {Spectrometry}},
	volume = {10},
	issn = {1530-6984},
	doi = {10.1021/nl904208y},
	abstract = {We report on a mass-spectrometry (time-of-flight secondary ion mass spectrometry, TOF-SIMS) based method For multiplexed DNA detection utilizing a random array, where the lipid composition of small unilamellar liposomes act as chemical barcodes to identify unique DNA target sequences down to the single molecule level. In a sandwich format, suspended target-DNA to be detected mediates the binding of capture-DNA modified liposomes to surface-immobilized probe-DNA. With the lipid composition of each liposome encoding a unique target-DNA sequence, TOF-SIMS analysis was used to determine the chemical fingerprint of the bound liposomes. Using high-resolution TOF-SIMS imaging, providing sub-200 nm spatial resolution, single DNA targets could be detected and identified via the chemical fingerprint of individual liposomes. The results also demonstrate the capability of TOF-SIMS to provide multiplexed detection of DNA targets on substrate areas in the micrometer range. Together with a high multiplexing capacity, this makes the concept an interesting alternative to existing barcode concepts based on fluorescence, Raman, or graphical codes for small-scale bioanalysis.},
	language = {English},
	number = {2},
	journal = {Nano Letters},
	author = {Gunnarsson, Anders and Sjovall, Peter and Hook, Fredrik},
	month = feb,
	year = {2010},
	note = {WOS:000274338800063},
	keywords = {arrays, barcode, dna, expression, high-density, hybridization, liposome, maldi-tof, nanoparticle probes, sims, single molecule detection, targets, tof-sims},
	pages = {732--737},
}

@article{veshaguri_direct_2016,
	title = {Direct observation of proton pumping by a eukaryotic {P}-type {ATPase}},
	volume = {351},
	copyright = {Copyright © 2016, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/351/6280/1469},
	doi = {10.1126/science.aad6429},
	abstract = {A proton pump in action
P-type adenosine triphosphatases (ATPases) use the energy from ATP hydrolysis to pump cations across biological membranes. The electrochemical gradients that are generated control many essential cellular processes. Veshaguri et al. incorporated a plant proton pump into vesicles and monitored the dynamics of single pumps. Pumping was stochastically interrupted by long-lived inactive or leaky states. The work reveals how these proton pumps are regulated by a protein domain and by pH gradients.
Science, this issue p. 1469
In eukaryotes, P-type adenosine triphosphatases (ATPases) generate the plasma membrane potential and drive secondary transport systems; however, despite their importance, their regulation remains poorly understood. We monitored at the single-molecule level the activity of the prototypic proton-pumping P-type ATPase Arabidopsis thaliana isoform 2 (AHA2). Our measurements, combined with a physical nonequilibrium model of vesicle acidification, revealed that pumping is stochastically interrupted by long-lived ({\textasciitilde}100 seconds) inactive or leaky states. Allosteric regulation by pH gradients modulated the switch between these states but not the pumping or leakage rates. The autoinhibitory regulatory domain of AHA2 reduced the intrinsic pumping rates but increased the dwell time in the active pumping state. We anticipate that similar functional dynamics underlie the operation and regulation of many other active transporters.
Single-molecule experiments reveal inactive and leaky states that define the activity and the regulation of a proton pump.
Single-molecule experiments reveal inactive and leaky states that define the activity and the regulation of a proton pump.},
	language = {en},
	number = {6280},
	urldate = {2018-01-17},
	journal = {Science},
	author = {Veshaguri, Salome and Christensen, Sune M. and Kemmer, Gerdi C. and Ghale, Garima and Møller, Mads P. and Lohr, Christina and Christensen, Andreas L. and Justesen, Bo H. and Jørgensen, Ida L. and Schiller, Jürgen and Hatzakis, Nikos S. and Grabe, Michael and Pomorski, Thomas Günther and Stamou, Dimitrios},
	month = mar,
	year = {2016},
	pmid = {27013734},
	pages = {1469--1473},
}

@article{mathiasen_nanoscale_2014,
	title = {Nanoscale high-content analysis using compositional heterogeneities of single proteoliposomes},
	volume = {11},
	copyright = {2014 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3062},
	doi = {10.1038/nmeth.3062},
	abstract = {{\textless}p{\textgreater}The compositional heterogeneity of proteoliposome reconstitution can skew the results of ensemble-average measurements of transmembrane protein structure and function. These compositional heterogeneities can be exploited, however, with a single-proteoliposome, high-content screening method.{\textless}/p{\textgreater}},
	language = {En},
	number = {9},
	urldate = {2018-01-17},
	journal = {Nature Methods},
	author = {Mathiasen, Signe and Christensen, Sune M. and Fung, Juan José and Rasmussen, Søren G. F. and Fay, Jonathan F. and Jorgensen, Sune K. and Veshaguri, Salome and Farrens, David L. and Kiskowski, Maria and Kobilka, Brian and Stamou, Dimitrios},
	month = aug,
	year = {2014},
	pages = {931},
}

@article{diamandis_biotin-strept_1991,
	title = {The biotin-(strept) avidin system: principles and applications in biotechnology.},
	volume = {37},
	shorttitle = {The biotin-(strept) avidin system},
	number = {5},
	journal = {Clinical chemistry},
	author = {Diamandis, Eleftherios P. and Christopoulos, Theodore K.},
	year = {1991},
	pages = {625--636},
}

@article{busse_sensitivity_2002,
	title = {Sensitivity studies for specific binding reactions using the biotin/streptavidin system by evanescent optical methods},
	volume = {17},
	issn = {0956-5663},
	url = {http://www.sciencedirect.com/science/article/pii/S0956566302000271},
	doi = {10.1016/S0956-5663(02)00027-1},
	abstract = {The combination of various evanescent optical methods such as surface plasmon spectroscopy, waveguide mode spectroscopy and an integrated optical Mach–Zehnder-interferometer are used to characterize biotinylated self-assembled monolayers as well as the binding of streptavidin to these labels. The aim of designing a highly specific and sensitive, re-usable affinity sensor for antigens on the basis of an integrated optical Mach–Zehnder interferometer is based on a proper understanding of the characteristics of the entire binding matrix architecture. Therefore, a variety of biotin-derivatives immobilized in a monolayer are investigated with respect to their affinity to streptavidin and the possibility to remove the steptavidin layer specifically. The density of the streptavidin layer as well as the optical constants of the involved molecules are measured. Finally the integrated optical Mach–Zehnder interferometer is tested with respect to the sensitivity to an antigen–antibody binding reaction. An attempt to further increase the sensitivity by simultaneous detection of a fluorescence signal failed due to bleaching effects.},
	number = {8},
	urldate = {2018-01-17},
	journal = {Biosensors and Bioelectronics},
	author = {Busse, Stefan and Scheumann, Volker and Menges, Bernhard and Mittler, Silvia},
	month = aug,
	year = {2002},
	keywords = {AFM, Antigen–antibody binding, Biotin–streptavidin binding, Evanescent wave optics, Mach–Zehnder interferometer, Refractive index, Surface plasmon spectroscopy, Thickness of swollen streptavidin, Waveguide mode spectroscopy},
	pages = {704--710},
}

@article{candeias_catalysed_1998,
	title = {The catalysed {NADH} reduction of resazurin to resorufin},
	number = {11},
	journal = {Journal of the Chemical Society, Perkin Transactions 2},
	author = {Candeias, LuisáP and MacFarlane, DonaldáP S. and McWhinnie, SeanáL W. and Maidwell, NicolaáL and Roeschlaub, CarláA and Sammes, PeteráG},
	year = {1998},
	pages = {2333--2334},
}

@article{moses_links_2016,
	title = {Links of {Conformational} {Sampling} to {Functional} {Plasticity} and {Clinical} {Phenotypes} by {Single} {Molecule} {Studies}},
	volume = {110},
	issn = {0006-3495},
	url = {http://www.sciencedirect.com/science/article/pii/S0006349515033275},
	doi = {10.1016/j.bpj.2015.11.2144},
	number = {3, Supplement 1},
	urldate = {2018-01-17},
	journal = {Biophysical Journal},
	author = {Moses, Matias E. and Thodberg, Sara and Bavishi, Krutika and Eiersholt, Stine and Li, Darui and Stamou, Dimitrios and Moller, Birger L. and Laursen, Tomas and Hatzakis, Nikos S.},
	month = feb,
	year = {2016},
	pages = {397a},
}

@article{bendix_quantification_2009,
	title = {Quantification of nano-scale intermembrane contact areas by using fluorescence resonance energy transfer},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/106/30/12341},
	doi = {10.1073/pnas.0903052106},
	abstract = {Nanometer-scale intermembrane contact areas (CAs) formed between single small unilamellar lipid vesicles (SUVs) and planar supported lipid bilayers are quantified by measuring fluorescence resonance energy transfer (FRET) between a homogenous layer of donor fluorophores labeling the supported bilayer and acceptor fluorophores labeling the SUVs. The smallest CAs detected in our setup between biotinylated SUVs and dense monolayers of streptavidin were ≈20 nm in radius. Deformation of SUVs is revealed by comparing the quenching of the donors to calculations of FRET between a perfectly spherical shell and a flat surface containing complementary fluorophores. These results confirmed the theoretical prediction that the degree of deformation scales with the SUV diameter. The size of the CA can be controlled experimentally by conjugating polyethylene glycol polymers to the SUV or the surface and thereby modulating the interfacial energy of adhesion. In this manner, we could achieve secure immobilization of SUVs under conditions of minimal deformation. Finally, we demonstrate that kinetic measurements of CA, at constant adhesion, can be used to record in real-time quantitative changes in the bilayer tension of a nano-scale lipid membrane system.},
	language = {en},
	number = {30},
	urldate = {2018-01-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bendix, Poul Martin and Pedersen, Mette S. and Stamou, Dimitrios},
	month = jul,
	year = {2009},
	pmid = {19597158},
	keywords = {adhesion, membrane deformation, membrane tension, small unilamellar lipid vesicles},
	pages = {12341--12346},
}

@article{hatzakis_single_2012,
	title = {Single {Enzyme} {Studies} {Reveal} the {Existence} of {Discrete} {Functional} {States} for {Monomeric} {Enzymes} and {How} {They} {Are} “{Selected}” upon {Allosteric} {Regulation}},
	volume = {134},
	issn = {0002-7863},
	url = {http://dx.doi.org/10.1021/ja3011429},
	doi = {10.1021/ja3011429},
	abstract = {Allosteric regulation of enzymatic activity forms the basis for controlling a plethora of vital cellular processes. While the mechanism underlying regulation of multimeric enzymes is generally well understood and proposed to primarily operate via conformational selection, the mechanism underlying allosteric regulation of monomeric enzymes is poorly understood. Here we monitored for the first time allosteric regulation of enzymatic activity at the single molecule level. We measured single stochastic catalytic turnovers of a monomeric metabolic enzyme (Thermomyces lanuginosus Lipase) while titrating its proximity to a lipid membrane that acts as an allosteric effector. The single molecule measurements revealed the existence of discrete binary functional states that could not be identified in macroscopic measurements due to ensemble averaging. The discrete functional states correlate with the enzyme’s major conformational states and are redistributed in the presence of the regulatory effector. Thus, our data support allosteric regulation of monomeric enzymes to operate via selection of preexisting functional states and not via induction of new ones.},
	number = {22},
	urldate = {2018-01-17},
	journal = {Journal of the American Chemical Society},
	author = {Hatzakis, Nikos S. and Wei, Li and Jorgensen, Sune K. and Kunding, Andreas H. and Bolinger, Pierre-Yves and Ehrlich, Nicky and Makarov, Ivan and Skjot, Michael and Svendsen, Allan and Hedegård, Per and Stamou, Dimitrios},
	month = jun,
	year = {2012},
	pages = {9296--9302},
}

@article{meshcheryakov_structural_2012,
	title = {Structural flexibility of the cytoplasmic domain of flagellar type {III} secretion protein {FlhB} is important for the function of the protein},
	volume = {102},
	number = {3},
	journal = {Biophysical Journal},
	author = {Meshcheryakov, Vladimir A. and Barker, Clive S. and Meshcheryakova, Irina V. and Kostyukova, Alla S. and Samatey, Fadel A.},
	year = {2012},
	pages = {626a},
}

@incollection{moses_quantification_2016,
	title = {Quantification of {Functional} {Dynamics} of {Membrane} {Proteins} {Reconstituted} in {Nanodiscs} {Membranes} by {Single} {Turnover} {Functional} {Readout}},
	volume = {581},
	isbn = {978-0-12-809267-5},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0076687916302737},
	language = {en},
	urldate = {2018-01-17},
	booktitle = {Methods in {Enzymology}},
	publisher = {Elsevier},
	author = {Moses, M.E. and Hedegård, P. and Hatzakis, N.S.},
	year = {2016},
	doi = {10.1016/bs.mie.2016.08.026},
	pages = {227--256},
}

@article{jorgensen_creating_2012,
	title = {Creating a {Proteoliposome} {Assay} for {Single} {Photosystem} {I} {Activity} {Assessment}},
	volume = {102},
	issn = {0006-3495},
	doi = {10.1016/j.bpj.2011.11.3412},
	language = {English},
	number = {3},
	journal = {Biophysical Journal},
	author = {Jorgensen, Sune K. and Lassen, Laerke M. M. and Kemmer, Gerdi and Gunther-Pomorski, Thomas and Stamou, Dimitrios and Bjornholm, Thomas and Jensen, Poul E. and Hatzakis, Nikos S.},
	month = jan,
	year = {2012},
	note = {WOS:000321561204342},
	keywords = {forskningspraktik},
	pages = {626A--627A},
}
